{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResumeScannerBeta.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rwfchan/ECS-193B-Testing-Repo/blob/main/ResumeScannerBeta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB2cNrD1fmRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd85e48-35d9-4426-f7ee-5aa68d6f62de"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gin_W7SFYl4W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "5ae2700f-0955-4ecf-99b4-0a8bf12834ef"
      },
      "source": [
        "pip install --upgrade google-cloud-vision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google-cloud-vision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/51/e6321162877a2903ba3158737b944cf582a62b7f045e22864ab56b764adc/google_cloud_vision-2.3.1-py2.py3-none-any.whl (461kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-vision) (1.26.3)\n",
            "Collecting proto-plus>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/8a/61c5a9b9b6288f9b060b6e3d88374fc083953a29aeac7206616c2d3c9c8e/proto_plus-1.18.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (56.0.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.28.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.53.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (0.4.8)\n",
            "Installing collected packages: proto-plus, google-cloud-vision\n",
            "Successfully installed google-cloud-vision-2.3.1 proto-plus-1.18.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z0exEKNhLdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de4925a3-265a-4795-92df-204f0e58c262"
      },
      "source": [
        "pip install pdf2jpg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pdf2jpg in /usr/local/lib/python3.7/dist-packages (1.0)\n",
            "Requirement already satisfied: img2pdf in /usr/local/lib/python3.7/dist-packages (from pdf2jpg) (0.4.0)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.7/dist-packages (from img2pdf->pdf2jpg) (2.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from img2pdf->pdf2jpg) (7.1.2)\n",
            "Requirement already satisfied: lxml>=4.0 in /usr/local/lib/python3.7/dist-packages (from pikepdf->img2pdf->pdf2jpg) (4.2.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idZg4VKxdeJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b614b010-99ce-4c63-dcc3-d840588c8d69"
      },
      "source": [
        "pip install --upgrade spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/d8/0361bbaf7a1ff56b44dca04dace54c82d63dad7475b7d25ea1baefafafb2/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 291kB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/67/d4002a18e26bf29b17ab563ddb55232b445ab6a02f97bf17d1345ff34d3f/spacy_legacy-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Collecting pathy>=0.3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/87/5991d87be8ed60beb172b4062dbafef18b32fa559635a8e2b633c2974f85/pathy-0.5.2-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.5MB/s \n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/82/a5/b5021c74c04cac35a27d34cbf3146d86eb8e173b4491888bc4908c4c8b3b/catalogue-2.0.3-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Collecting thinc<8.1.0,>=8.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/87/decceba68a0c6ca356ddcb6aea8b2500e71d9bc187f148aae19b747b7d3c/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 43.6MB/s \n",
            "\u001b[?25hCollecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 35.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.0.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 52.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Building wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=9143c4117a773fc0145226cf87181202fef1ed700ad125e9ec40e335b4ed0c0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built smart-open\n",
            "Installing collected packages: spacy-legacy, typer, smart-open, pathy, catalogue, srsly, pydantic, thinc, spacy\n",
            "  Found existing installation: smart-open 5.0.0\n",
            "    Uninstalling smart-open-5.0.0:\n",
            "      Successfully uninstalled smart-open-5.0.0\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.3 pathy-0.5.2 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.6 spacy-legacy-3.0.5 srsly-2.4.1 thinc-8.0.3 typer-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUuughx9dW8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd9e10c7-fcac-4e32-d4b5-c54e70046bbd"
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-29 03:32:11.748159: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-md==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.0.0/en_core_web_md-3.0.0-py3-none-any.whl (47.1MB)\n",
            "\u001b[K     |████████████████████████████████| 47.1MB 96kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (20.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (56.0.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.5.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (8.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.4.1)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zx9CoyRmB3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61da33b0-51b8-49bd-e793-e4acd07ee2d4"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-29 03:32:26.604918: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 254kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.5.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (56.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5qmxmQ-FszC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54945133-2134-4956-f358-ba4cf3498d51"
      },
      "source": [
        "pip install mysql-connector-python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mysql-connector-python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/69/aa9545022bc11029e7b45cb9c60ab8e6ce064c67a6cdff23081ded67d216/mysql_connector_python-8.0.24-cp37-cp37m-manylinux1_x86_64.whl (25.4MB)\n",
            "\u001b[K     |████████████████████████████████| 25.4MB 111kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from mysql-connector-python) (3.12.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.0.0->mysql-connector-python) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.0.0->mysql-connector-python) (56.0.0)\n",
            "Installing collected packages: mysql-connector-python\n",
            "Successfully installed mysql-connector-python-8.0.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF1JDXtKiptZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb272136-e4cf-40cb-aa5f-ff9bb8e88a85"
      },
      "source": [
        "# Install stanza; note that the prefix \"!\" is not needed if you are running in a terminal\n",
        "!pip install stanza\n",
        "\n",
        "# Import stanza\n",
        "import stanza\n",
        "corenlp_dir = './corenlp'\n",
        "stanza.install_corenlp(dir=corenlp_dir)\n",
        "\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir\n",
        "from stanza.server import CoreNLPClient"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/ae/a70a58ce6b4e2daad538688806ee0f238dbe601954582a74ea57cde6c532/stanza-1.2-py3-none-any.whl (282kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 14.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 71kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 81kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 92kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 102kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 112kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 122kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 143kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 153kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 163kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 174kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 184kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 194kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 204kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 215kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 225kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 235kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 245kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 256kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 266kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 276kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.8.1+cu101)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (56.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-29 03:33:00 INFO: Installing CoreNLP package into ./corenlp...\n",
            "Downloading http://nlp.stanford.edu/software/stanford-corenlp-latest.zip: 100%|██████████| 505M/505M [01:32<00:00, 5.45MB/s]\n",
            "2021-04-29 03:34:36 WARNING: For customized installation location, please set the `CORENLP_HOME` environment variable to the location of the installation. In Unix, this is done with `export CORENLP_HOME=./corenlp`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUSmzCeKnuB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb5257b2-3176-4cf2-ebd1-e0b9c70608e2"
      },
      "source": [
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "!wget 'https://nlp.stanford.edu/software/stanford-ner-2018-10-16.zip'\n",
        "!unzip stanford-ner-2018-10-16.zip\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "st = StanfordNERTagger('/content/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
        "                       '/content/stanford-ner-2018-10-16/stanford-ner.jar',\n",
        "                       encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-29 03:34:50--  https://nlp.stanford.edu/software/stanford-ner-2018-10-16.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-ner-2018-10-16.zip [following]\n",
            "--2021-04-29 03:34:50--  https://downloads.cs.stanford.edu/nlp/software/stanford-ner-2018-10-16.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180358328 (172M) [application/zip]\n",
            "Saving to: ‘stanford-ner-2018-10-16.zip’\n",
            "\n",
            "stanford-ner-2018-1 100%[===================>] 172.00M  5.06MB/s    in 30s     \n",
            "\n",
            "2021-04-29 03:35:21 (5.70 MB/s) - ‘stanford-ner-2018-10-16.zip’ saved [180358328/180358328]\n",
            "\n",
            "Archive:  stanford-ner-2018-10-16.zip\n",
            "   creating: stanford-ner-2018-10-16/\n",
            "  inflating: stanford-ner-2018-10-16/README.txt  \n",
            "  inflating: stanford-ner-2018-10-16/ner-gui.bat  \n",
            "  inflating: stanford-ner-2018-10-16/build.xml  \n",
            "  inflating: stanford-ner-2018-10-16/stanford-ner.jar  \n",
            "  inflating: stanford-ner-2018-10-16/sample-conll-file.txt  \n",
            "  inflating: stanford-ner-2018-10-16/sample.ner.txt  \n",
            "  inflating: stanford-ner-2018-10-16/stanford-ner-3.9.2-sources.jar  \n",
            "   creating: stanford-ner-2018-10-16/lib/\n",
            "  inflating: stanford-ner-2018-10-16/lib/joda-time.jar  \n",
            "  inflating: stanford-ner-2018-10-16/lib/stanford-ner-resources.jar  \n",
            "  inflating: stanford-ner-2018-10-16/lib/jollyday-0.4.9.jar  \n",
            "  inflating: stanford-ner-2018-10-16/ner-gui.command  \n",
            "  inflating: stanford-ner-2018-10-16/ner.sh  \n",
            "  inflating: stanford-ner-2018-10-16/stanford-ner-3.9.2.jar  \n",
            "  inflating: stanford-ner-2018-10-16/NERDemo.java  \n",
            "  inflating: stanford-ner-2018-10-16/stanford-ner-3.9.2-javadoc.jar  \n",
            "  inflating: stanford-ner-2018-10-16/ner.bat  \n",
            "   creating: stanford-ner-2018-10-16/classifiers/\n",
            "  inflating: stanford-ner-2018-10-16/classifiers/english.conll.4class.distsim.prop  \n",
            "  inflating: stanford-ner-2018-10-16/classifiers/example.serialized.ncc.ncc.ser.gz  \n",
            "  inflating: stanford-ner-2018-10-16/classifiers/english.muc.7class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2018-10-16/classifiers/english.conll.4class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2018-10-16/classifiers/english.muc.7class.distsim.prop  \n",
            "  inflating: stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.prop  \n",
            "  inflating: stanford-ner-2018-10-16/classifiers/example.serialized.ncc.prop  \n",
            "  inflating: stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2018-10-16/sample.txt  \n",
            "  inflating: stanford-ner-2018-10-16/sample-w-time.txt  \n",
            "  inflating: stanford-ner-2018-10-16/ner-gui.sh  \n",
            "  inflating: stanford-ner-2018-10-16/LICENSE.txt  \n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
            "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
            "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
            "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQmV3i01ZZ-8"
      },
      "source": [
        "import spacy\n",
        "from spacy.vocab import Vocab\n",
        "import numpy\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import np_utils\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import GlobalAveragePooling1D\n",
        "from keras.models import load_model\n",
        "import pickle\n",
        "from spacy.matcher import Matcher\n",
        "import pandas as pd\n",
        "import re\n",
        "import mysql.connector\n",
        "from sqlalchemy import create_engine\n",
        "import io\n",
        "import os\n",
        "import shutil\n",
        "from pdf2jpg import pdf2jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9VWqbx1is0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7e9224e-eaa1-49c8-8e60-8c93da20212d"
      },
      "source": [
        "# Setup standford could\n",
        "client = CoreNLPClient(annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner'], \n",
        "                    memory='4G', endpoint='http://localhost:9003', be_quiet=True)\n",
        "import time; time.sleep(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-29 03:35:43 INFO: Writing properties to tmp file: corenlp_server-3e2aa58803d94459.props\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-vuxQzrF4Dk"
      },
      "source": [
        "def sendCSVFile(csv_dataframe):\n",
        "    mydb = mysql.connector.connect(host=\"00.000.00.000\", user=\"root\", passwd=\"EDIT\", database=\"EDIT\")\n",
        "    mycursor = mydb.cursor()\n",
        "\n",
        "    csv_dataframe = csv_dataframe.drop(['Init_index'], axis=1)\n",
        "\n",
        "    sql_create_table = '''\n",
        "                      CREATE TABLE resume_analyze_output\n",
        "                      (Name TEXT(1000),Organization TEXT(1000),Position TEXT(1000),\n",
        "                      Email TEXT(1000),Phone TEXT(1000),Skill TEXT(1000),\n",
        "                      SchoolName TEXT(1000),SchoolDegree TEXT(1000),SchoolDate TEXT(1000),\n",
        "                      Qualification TEXT(1000))\n",
        "                      '''\n",
        "    mycursor.execute(\"DROP TABLE IF EXISTS csv_output.resume_analyze_output\")\n",
        "    mycursor.execute(sql_create_table)\n",
        "    \n",
        "    for row, column in csv_dataframe.iterrows():\n",
        "      mycursor.execute('''\n",
        "                INSERT INTO csv_output.resume_analyze_output \n",
        "                (Name, Organization, Position, Email, Phone, \n",
        "                Skill, SchoolName, SchoolDegree, \n",
        "                SchoolDate, Qualification)\n",
        "                VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
        "                ''',\n",
        "                (column['binary'],\n",
        "                # (column['name'],\n",
        "                 column.organization,\n",
        "                 column.position,\n",
        "                 column.email,\n",
        "                 column.phone,\n",
        "                 column.skill,\n",
        "                 column.SchoolName,\n",
        "                 column.SchoolDegree,\n",
        "                 column.SchoolDate,\n",
        "                 column.Qualifications))\n",
        "    \n",
        "    mydb.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdv6gc0-zKHM"
      },
      "source": [
        "def convertPDF2JEG(directory):\n",
        "  import os, shutil\n",
        "  outputpathpre = \"/content/drive/MyDrive/Resume Scanner Folder/ResumeFirstDir\"\n",
        "  outputpathpost = \"/content/drive/MyDrive/Resume Scanner Folder/testingJPG\"\n",
        "\n",
        "  containPDF = False\n",
        "\n",
        "  dir = outputpathpre\n",
        "  for files in os.listdir(dir):\n",
        "    path = os.path.join(dir, files)\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "    except OSError:\n",
        "        os.remove(path)\n",
        "  \n",
        "  dir = outputpathpost\n",
        "  for files in os.listdir(dir):\n",
        "    path = os.path.join(dir, files)\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "    except OSError:\n",
        "        os.remove(path)\n",
        "\n",
        "  # Convert pdf to jpg and place them into separate directories each\n",
        "  for filename in os.listdir(directory):\n",
        "    if  filename.endswith(\".pdf\"):\n",
        "      containPDF = True\n",
        "      pdf2jpg.convert_pdf2jpg(os.path.join(directory, filename), outputpathpre, dpi=300, pages=\"ALL\")\n",
        "\n",
        "  # Place jpg from subdirectories into one singular directory\n",
        "  for subdirectory in os.listdir(outputpathpre):\n",
        "    for filename in os.listdir(os.path.join(outputpathpre, subdirectory)):\n",
        "      shutil.move(os.path.join(outputpathpre, subdirectory, filename), outputpathpost)\n",
        "  \n",
        "  return containPDF\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FTdN0mdHYEG"
      },
      "source": [
        "def processFileFormat(directory):\n",
        "  if convertPDF2JEG(directory):\n",
        "    directory = \"/content/drive/MyDrive/Resume Scanner Folder/testingJPG\"\n",
        "\n",
        "  return directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb4_fNjyc7cp"
      },
      "source": [
        "# Process the resume into relevant text informations\n",
        "# Input: Directory of resumes\n",
        "# Output: the name of the text files\n",
        "def preprocess_resume(directory, debug=False):\n",
        "    import os\n",
        "    import re, string\n",
        "    import copy\n",
        "\n",
        "    print(\"Processing \" + directory)\n",
        "\n",
        "    outputtxt = \"/content/BulkTxtOutput.txt\"\n",
        "    outputtxt_org = \"/content/BulkTxtOutput_org.txt\"\n",
        "\n",
        "    # Set up file clean up\n",
        "    newline = re.compile(\"[\" + \"\\n\" + \"]\")\n",
        "    pattern = re.compile(\"[\" + re.escape(string.punctuation) + \"]\")\n",
        "    afterPattern = re.compile(\"•\")\n",
        "    text_orgPattern = re.compile(\"\\\"\")\n",
        "\n",
        "    text_file = open(outputtxt, \"w\")\n",
        "    text_file_org = open(outputtxt_org, \"w\")\n",
        "    data = pandas.DataFrame(columns=['Sentence', 'Location'])\n",
        "\n",
        "    # Start by writing the column name into textfile\n",
        "    text_file.write('Sentence' + '\\n')\n",
        "    text_file_org.write('Sentence' + '\\n')\n",
        "\n",
        "    # Counter for the first x resume we want to process\n",
        "    i = 0\n",
        "    x = 10\n",
        "\n",
        "    #save process file name to csv\n",
        "    filenameList = [[]]\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "      if debug==True and i > x:\n",
        "        break\n",
        "\n",
        "      if not filename.endswith(\".pdf\"):\n",
        "        print(\"\\n\" + \"Currently processing: \" + filename)\n",
        "        filenameList += [filename]\n",
        "        texts, sentence_location_perResume, df = detect_text(os.path.join(directory, filename))\n",
        "        df['Cleaned Sentence'] = df['Sentence']\n",
        "\n",
        "        for row in range(len(df['Cleaned Sentence'])):\n",
        "          if df['Cleaned Sentence'][row] != \"--------------------------------------------------\\n\" :\n",
        "            df['Cleaned Sentence'][row] = pattern.sub(' ', df['Cleaned Sentence'][row])\n",
        "        for row in range(len(df['Sentence'])):\n",
        "          df['Sentence'][row] = text_orgPattern.sub(' ', df['Sentence'][row])\n",
        "\n",
        "        clean_sentence = '\\n'.join(df['Cleaned Sentence'].tolist())\n",
        "        text_file.write(clean_sentence)\n",
        "\n",
        "        sentence = '\\n'.join(df['Sentence'].tolist())\n",
        "        text_file_org.write(sentence)\n",
        "\n",
        "        data = data.append(df, ignore_index=True)\n",
        "\n",
        "        print(\"Finished Resume number: \" + str(i) + \"--------------------------------------------------\" + \"\\n\")\n",
        "        i+=1\n",
        "\n",
        "    text_file.close()\n",
        "    text_file_org.close()\n",
        "\n",
        "    data = {'filename':filenameList}\n",
        "    filenameDF = pd.DataFrame(data)\n",
        "    filenameDF.to_csv('/content/fileorder.csv')\n",
        "\n",
        "    return outputtxt, outputtxt_org"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5AEcohXZb0H"
      },
      "source": [
        "# Running Vision API for parsing resume image into text file\n",
        "# Input: path of the resume image\n",
        "# Output: the converted text string\n",
        "def detect_text(path):\n",
        "    \"\"\"Detects text in the file.\"\"\"\n",
        "    from google.cloud import vision\n",
        "    import io\n",
        "    import os\n",
        "\n",
        "    from enum import Enum\n",
        "    from PIL import Image\n",
        "    im = Image.open(path)\n",
        "    width, height = im.size\n",
        "    # print(width)\n",
        "    \n",
        "\n",
        "    \n",
        "    class FeatureType(Enum):\n",
        "        PAGE = 1\n",
        "        BLOCK = 2\n",
        "        PARA = 3\n",
        "        WORD = 4\n",
        "        SYMBOL = 5\n",
        "\n",
        "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/Resume_Analyze-4f918029480e.json\"\n",
        "    \n",
        "    client = vision.ImageAnnotatorClient()\n",
        "\n",
        "    with io.open(path, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    response = client.document_text_detection(image=image)\n",
        "    texts = response.full_text_annotation\n",
        "\n",
        "    first_line_x = 0\n",
        "\n",
        "    for page in texts.pages:\n",
        "        for block in page.blocks:\n",
        "            if block.bounding_box.vertices[0].x < width/2:\n",
        "                first_line_x = block.bounding_box.vertices[0].x\n",
        "                break\n",
        "\n",
        "    i = 0   # Counting number of blocks\n",
        "    x1, x2, y1, y2 = 9999, 0, 9999, 0   # the sentence's coordinates\n",
        "    breaks = vision.TextAnnotation.DetectedBreak.BreakType\n",
        "    bounds = [[[]]]\n",
        "    sentence_location = []\n",
        "\n",
        "    # Initialization\n",
        "    bounds.append([])\n",
        "    bounds[0].append([])\n",
        "\n",
        "    num_sentence = 0\n",
        "\n",
        "    for page in texts.pages:\n",
        "        for block in page.blocks:\n",
        "            s = 0   # Counting number of sentence\n",
        "            if abs(block.bounding_box.vertices[0].x - first_line_x) < width/4:\n",
        "                  for paragraph in block.paragraphs:\n",
        "                      for word in paragraph.words:\n",
        "                          for symbol in word.symbols: # Beginning of word\n",
        "                              pending_x1 = symbol.bounding_box.vertices[0].x\n",
        "                              pending_y1 = symbol.bounding_box.vertices[0].y\n",
        "                              pending_x2 = symbol.bounding_box.vertices[3].x\n",
        "                              pending_y2 = symbol.bounding_box.vertices[3].y\n",
        "\n",
        "                              x1 = pending_x1 if x1 > pending_x1 else x1\n",
        "                              y1 = pending_y1 if y1 > pending_y1 else y1\n",
        "                              x2 = pending_x2 if x2 < pending_x2 else x2\n",
        "                              y2 = pending_y2 if y2 < pending_y2 else y2\n",
        "\n",
        "                              if symbol.text == '•':\n",
        "                                bounds[i][s] = \"\".join(bounds[i][s]) \n",
        "                                if (bounds[i][s] != '.'):\n",
        "                                    sentence_location.append((x1, y1, x2, y2))\n",
        "                                bounds[i].append([])  # append new sentence\n",
        "                                s+=1\n",
        "                                x1, x2, y1, y2 = 9999, 0, 9999, 0   # reset the coordinates\n",
        "                                continue\n",
        "                              else: \n",
        "                                bounds[i][s]+=symbol.text\n",
        "                              \n",
        "                              if symbol.property.detected_break.type_ == breaks.SPACE:\n",
        "                                bounds[i][s]+=' '  # End of a word\n",
        "                              elif symbol.property.detected_break.type_ == breaks.EOL_SURE_SPACE or symbol.property.detected_break.type_ == breaks.LINE_BREAK:\n",
        "                                bounds[i][s] = \"\".join(bounds[i][s]) \n",
        "                                if (bounds[i][s] != '.'):\n",
        "                                    sentence_location.append((x1, y1, x2, y2))\n",
        "                                bounds[i].append([])  # append new sentence\n",
        "                                s+=1\n",
        "                                x1, x2, y1, y2 = 9999, 0, 9999, 0   # reset the coordinates\n",
        "                      bounds[i].append([])  # append new sentence\n",
        "                  i+=1\n",
        "                  bounds.append([])\n",
        "                  bounds[i].append([]) # For each sentence, not each paragraphs\n",
        "    \n",
        "    for page in texts.pages:\n",
        "        for block in page.blocks:\n",
        "            s = 0   # Counting number of sentence\n",
        "            if abs(block.bounding_box.vertices[0].x - first_line_x) >= width/4:\n",
        "                  for paragraph in block.paragraphs:\n",
        "                      for word in paragraph.words:\n",
        "                          for symbol in word.symbols: # Beginning of word\n",
        "                              pending_x1 = symbol.bounding_box.vertices[0].x\n",
        "                              pending_y1 = symbol.bounding_box.vertices[0].y\n",
        "                              pending_x2 = symbol.bounding_box.vertices[3].x\n",
        "                              pending_y2 = symbol.bounding_box.vertices[3].y\n",
        "\n",
        "                              x1 = pending_x1 if x1 > pending_x1 else x1\n",
        "                              y1 = pending_y1 if y1 > pending_y1 else y1\n",
        "                              x2 = pending_x2 if x2 < pending_x2 else x2\n",
        "                              y2 = pending_y2 if y2 < pending_y2 else y2\n",
        "\n",
        "                              if symbol.text == '•':\n",
        "                                bounds[i][s] = \"\".join(bounds[i][s]) \n",
        "                                if (bounds[i][s] != '.'):\n",
        "                                    sentence_location.append((x1, y1, x2, y2))\n",
        "                                bounds[i].append([])  # append new sentence\n",
        "                                s+=1\n",
        "                                x1, x2, y1, y2 = 9999, 0, 9999, 0   # reset the coordinates\n",
        "                                continue\n",
        "                              else: \n",
        "                                bounds[i][s]+=symbol.text\n",
        "                              \n",
        "                              if symbol.property.detected_break.type_ == breaks.SPACE:\n",
        "                                bounds[i][s]+=' '  # End of a word\n",
        "                              elif symbol.property.detected_break.type_ == breaks.EOL_SURE_SPACE or symbol.property.detected_break.type_ == breaks.LINE_BREAK:\n",
        "                                bounds[i][s] = \"\".join(bounds[i][s]) \n",
        "                                if (bounds[i][s] != '.'):\n",
        "                                    sentence_location.append((x1, y1, x2, y2))\n",
        "                                bounds[i].append([])  # append new sentence\n",
        "                                s+=1\n",
        "                                x1, x2, y1, y2 = 9999, 0, 9999, 0   # reset the coordinates\n",
        "                      bounds[i].append([])  # append new sentence\n",
        "                  i+=1\n",
        "                  bounds.append([])\n",
        "                  bounds[i].append([]) # For each sentence, not each paragraphs\n",
        "\n",
        "\n",
        "    data = pandas.DataFrame(columns=['Sentence', 'Location'])\n",
        "\n",
        "    tidy_text = \"\"\n",
        "    s = 0\n",
        "    for bound in bounds:\n",
        "        for sentence in bound:\n",
        "            if not sentence or sentence == '.':\n",
        "              continue\n",
        "            if (isinstance(sentence,list) == True):\n",
        "              tidy_text = tidy_text + \"\".join(sentence) + '\\n'\n",
        "              sentence_location.insert(s, (-1,-1,-1,-1))    # If there are any out of handle line, simply insert invalid\n",
        "              data = data.append({'Sentence' : \"\".join(sentence), 'Location' : (-1,-1,-1,-1)}, ignore_index=True)\n",
        "            else:\n",
        "              tidy_text = tidy_text + sentence + '\\n'\n",
        "              data = data.append({'Sentence' : sentence, 'Location' : sentence_location[s]}, ignore_index=True)\n",
        "            s+=1\n",
        "    sentence_location.append((0,0,0,0))   # Appending for end of resume indicator\n",
        "    data = data.append({'Sentence' : \"--------------------------------------------------\\n\", 'Location' : (0,0,0,0)}, ignore_index=True)\n",
        "    \n",
        "    return tidy_text, sentence_location, data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJm7myQTUlS8"
      },
      "source": [
        "# Reference from https://github.com/shabeelkandi/Handling-Out-of-Vocabulary-Words-in-Natural-Language-Processing-using-Language-Modelling/blob/master/RNN%20LSTM%20Model.ipynb\n",
        "\n",
        "# generate a sequence using a language model\n",
        "# Input: the model being used (rev or forward), tokenizer file, max_length of word, and seed of the text\n",
        "# Output: list of predicted words\n",
        "def generate_seq(model, tokenizer, max_length, seed_text):\n",
        "    if seed_text == \"\":\n",
        "        return \"\"\n",
        "    else:\n",
        "        in_text = seed_text\n",
        "        n_words = 1\n",
        "        n_preds = 5\n",
        "        pred_words = \"\"\n",
        "        # generate a fixed number of words\n",
        "        for _ in range(n_words):\n",
        "            # encode the text as integer\n",
        "            encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "            # pre-pad sequences to a fixed length\n",
        "            encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "            # predict probabilities for each word\n",
        "            proba = model.predict(encoded, verbose=0).flatten()\n",
        "            #take the n_preds highest probability classes \n",
        "            yhat = numpy.argsort(-proba)[:n_preds] \n",
        "            # map predicted words index to word\n",
        "            out_word = ''\n",
        "\n",
        "            for _ in range(n_preds):\n",
        "                for word, index in tokenizer.word_index.items():\n",
        "                    if index == yhat[_]:\n",
        "                        out_word = word\n",
        "                        pred_words += ' ' + out_word\n",
        "                        break\n",
        "\n",
        "        return pred_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fb8wRi8aqFX"
      },
      "source": [
        "# Find and set embeddings for OOV words\n",
        "# Input: the interested nlp string\n",
        "# Output: None -- function set OOV to the global nlp vector\n",
        "def set_embedding_for_oov(doc):\n",
        "    #checking for oov words and adding embedding\n",
        "    #Can improve accuracy performance by using the weight from embedding layer. \n",
        "    # However, still doesn't mean it will update the embedding layer\n",
        "    for token in doc:\n",
        "        if token.is_oov == True:\n",
        "            before_text = doc[:token.i].text\n",
        "            after_text = str(array(doc)[:token.i:-1]).replace('[','').replace(']','')\n",
        "\n",
        "            pred_before = generate_seq(model, tokenizer, max_length-1, before_text).split()\n",
        "            pred_after = generate_seq(rev_model, tokenizer, max_length-1, after_text).split()\n",
        "            \n",
        "            # Change embedding array size\n",
        "            # spacy en_core_web_md uses 300 dimension. \n",
        "            # Even if the sentence max length becomes 40. We believe the max length of \n",
        "            embedding = numpy.zeros((300,))\n",
        "\n",
        "            i=len(before_text)\n",
        "            for word in pred_before:\n",
        "                embedding += i*nlp.vocab.get_vector(word)\n",
        "                i= i*.5\n",
        "            i=len(after_text)\n",
        "            for word in pred_after:\n",
        "                embedding += i*nlp.vocab.get_vector(word)\n",
        "                i= i*.25\n",
        "            nlp.vocab.set_vector(token.text, embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtQPhWGTbnTr"
      },
      "source": [
        "#function to find most similar words\n",
        "def most_similar(word):\n",
        "    by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)\n",
        "    return [w.orth_ for w in by_similarity[:10]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiRFKtqKQnqA"
      },
      "source": [
        "# Convert a line of words into array of vectors\n",
        "# Input: the string of text\n",
        "# Output: vectorized text string (size=300)\n",
        "def line2vec(line, maxlength):\n",
        "  import itertools\n",
        "\n",
        "  separator = ' '\n",
        "  # Make sure the embedding learn all the oov\n",
        "  set_embedding_for_oov(nlp(separator.join(line)))\n",
        "  # convert words into vectors\n",
        "  vectors = [nlp.vocab.get_vector(word) for word in line]\n",
        "  # join all vectors into one vector\n",
        "  big_vector = itertools.chain(vectors)\n",
        "  big_vector = pad_sequences(list(big_vector), maxlen=maxlength, dtype='float32', padding='post')\n",
        "\n",
        "  return big_vector[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izFitQrgMvWF"
      },
      "source": [
        "# Error adjustment\n",
        "# Input: data=pandas dataset of all the sentences; input=X; predictions=yhat\n",
        "# Output: error corrected predictions\n",
        "def HC_error_adjustment(data, input, predictions):\n",
        "  header_list = ['academic', 'experience', 'education', 'awards', 'award', 'courses', 'voluteer', 'qualification', 'qualifications',\n",
        "                 'professional', 'work', 'working', 'employment', 'internship', 'business', 'activities', 'project', 'projects', \n",
        "                 'intern', 'languages', 'skills', 'skill', 'fluent', 'summary', 'objective', 'eductio', 'personal', 'profile',\n",
        "                 'interest', 'personal', 'email', 'phone', 'milestones', 'linkedin', 'twitter', 'expertise', 'contact', \n",
        "                 'service', 'services', 'career', 'hobbies', 'coursework', 'traits', 'address', 'certificates', 'about', 'tel',\n",
        "                 'university', 'organizations', 'organization', 'expert', 'certifications']\n",
        "  header_black_list = ['leadership', 'speak', 'communities', 'enterprisse', 'administration', 'director', 'design', 'manager', \n",
        "                       'teacher', 'marketing', 'ethic', 'screenwriting', 'cooperative', 'excellence', 'kuo', 'writing', 'association',\n",
        "                       'practices']\n",
        "\n",
        "  for row in range(len(input)):\n",
        "    if isinstance(data.at[row, 'Sentence'],float):\n",
        "      continue\n",
        "    \n",
        "    string = data.at[row, 'Sentence'].replace(':', '')\n",
        "    sentence = string.lower().split(' ')\n",
        "    for item in header_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 1\n",
        "        break\n",
        "    \n",
        "    for item in header_black_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 0\n",
        "        break\n",
        "\n",
        "    if sentence[0] != '' and string.split(' ')[0][0].islower():\n",
        "      predictions[row] = 0\n",
        "    \n",
        "    if (len(sentence) > 5):\n",
        "      predictions[row] = 0\n",
        "    if (row == 0 \n",
        "      or data.at[row-1, 'Sentence'] == '--------------------------------------------------'):\n",
        "      predictions[row] = 1\n",
        "\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycIWzCVSjH3w"
      },
      "source": [
        "# Perpare for pulling all headers out and able to train on all papers (on BulkTxtOutput)\n",
        "#   Program can pull all headers out, and put them into another csv\n",
        "# Input: The resume in text file\n",
        "# Output: file path of the csv file with headers marked\n",
        "def get_headers(BulkTxtOutput, model=None):\n",
        "  HC_model = 0\n",
        "  if (model==None):\n",
        "    HC_model = load_model('/content/drive/MyDrive/ECS 193A ResumeData/Model & Rev_Model/HC_model.h5')\n",
        "  else:\n",
        "    HC_model = load_model(model)\n",
        "\n",
        "  csv_file = '/content/placeholding.csv'\n",
        "  max_length = 300*5\n",
        "  output_file = '/content/HC_output.csv'\n",
        "\n",
        "  print('Loading BulkTxtFile...')\n",
        "  # First, load text file to csv\n",
        "  #read_file = pandas.read_csv (BulkTxtOutput)\n",
        "  read_file = pandas.read_csv (BulkTxtOutput, sep=\"\\n\")\n",
        "  read_file.to_csv (csv_file)\n",
        "\n",
        "  print('Loading csv into dataframe...')\n",
        "  # Now, load csv to pandas (because this is the way I know how to do this)\n",
        "  data = pandas.read_csv(csv_file)\n",
        "  data = data.drop(data.columns[0], axis=1)\n",
        "  X = data.copy()\n",
        "\n",
        "  print('Converting words into vectors...')\n",
        "  for row in range(len(X['Sentence'])):\n",
        "    if not isinstance(X['Sentence'][row],float):\n",
        "      line = X['Sentence'][row].split()\n",
        "    if len(line) > 5:\n",
        "      line = line[:5]\n",
        "    big_vector = line2vec(line, max_length)\n",
        "    X['Sentence'][row] = numpy.asarray(big_vector).astype('float32')\n",
        "\n",
        "  print('Done!')\n",
        "\n",
        "  print('Perparing inputs for prediction model...')\n",
        "  input = X['Sentence'].to_numpy().tolist()\n",
        "  for row in range (len(input)):\n",
        "    input[row] = input[row].tolist()\n",
        "  \n",
        "  print('Predicting...')\n",
        "  predictions = (HC_model.predict_classes(input)).T[0]\n",
        "\n",
        "  # Pick up missed traits\n",
        "  predictions = HC_error_adjustment(data, input, predictions)\n",
        "  \n",
        "  print('Placing predictions into csv')\n",
        "  output = pandas.DataFrame(columns=['Sentence'])\n",
        "  # For every predictions[i] = 1, place the sentence onto the \n",
        "  for i in range (len(predictions)):\n",
        "    if (predictions[i]):\n",
        "      output.at[i, 'Sentence'] = data.at[i, 'Sentence']\n",
        "  \n",
        "  output.to_csv(output_file, index=True)\n",
        "  print(output)\n",
        "\n",
        "  return output_file\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tGITT2iPs13"
      },
      "source": [
        "# One_hot encoding\n",
        "# Input: the classified output needed to be encode\n",
        "# Output: One hot encoded output in numpy array\n",
        "def one_hot_encoding(y):\n",
        "  one_hot_y = np_utils.to_categorical(y)\n",
        "  return one_hot_y.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm9ObPS9VX2x"
      },
      "source": [
        "def isPhoneRow(text):\n",
        "  import re, string\n",
        "  pattern = re.compile(\"[\" + re.escape(string.punctuation) + ' ' + \"]\")\n",
        "\n",
        "  if isinstance(text, float) or '/' in text:\n",
        "    return False\n",
        "\n",
        "  sentence = pattern.sub('', text)\n",
        "\n",
        "  if len(sentence) > 9 and sentence.isdecimal():\n",
        "    return True\n",
        "\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZCQUPv4VYBq"
      },
      "source": [
        "def isWebsiteRow(text):\n",
        "  if isinstance(text, float):\n",
        "    return False\n",
        "\n",
        "  email_list = ['com', 'gmail']\n",
        "  \n",
        "  sentence = text.lower().split(' ')\n",
        "  if len(sentence) > 8:\n",
        "    return False\n",
        "\n",
        "  for item in email_list:\n",
        "    if item in sentence:\n",
        "      return True\n",
        "        \n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yRHse40VYIN"
      },
      "source": [
        "def isNameRow(text):\n",
        "  if extract_name_new(text) != None:\n",
        "    return True\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VYrxIws0o1M"
      },
      "source": [
        "# error adjustment\n",
        "# Input: data=pandas dataset of all the sentences; input=X; predictions=yhat\n",
        "# Output: error corrected predictions\n",
        "def LC_error_adjustment(data, input, predictions):\n",
        "  label1_list = ['academic', 'eductio', 'award', 'awards', 'education', 'coursework', 'courseworks', \n",
        "                 'qualification', 'qualifications', 'courses', 'course', 'university', 'certification']\n",
        "  label2_list = ['professional', 'work', 'working', 'employment', 'internship', 'business', 'activities', 'project', 'intern', 'projects', \n",
        "                 'milestones', 'volunteer', 'service', 'services', 'experience', 'career', 'summary', 'objective', 'me', 'organization'\n",
        "                 , 'organizations']\n",
        "  label3_list = ['languages', 'skills', 'skill', 'fluent', 'interest', 'expertise', 'hobbies', 'traits', 'proficiency', 'expert']\n",
        "  label4_list = ['profile', 'personal', 'email', 'phone', 'linkedin', 'twitter', 'contact', 'address', 'tel']\n",
        "  label5_list = ['reference']\n",
        "\n",
        "  for row in range(len(input)):\n",
        "    if (row == 0 \n",
        "      or data.at[row-1, 'Sentence'] == '--------------------------------------------------'):\n",
        "      predictions[row] = 4\n",
        "      continue\n",
        "\n",
        "    string = data.at[row, 'Sentence'].replace(':', '')\n",
        "    sentence = string.lower().split(' ')\n",
        "    for item in label2_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 2\n",
        "        break\n",
        "    \n",
        "    for item in label3_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 3\n",
        "        break\n",
        "    \n",
        "    for item in label1_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 1\n",
        "        break\n",
        "    \n",
        "    for item in label4_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 4\n",
        "        break\n",
        "    \n",
        "    #if isWebsiteRow(string) or isPhoneRow(string):\n",
        "    #  print('@ LC_error_adjustment website, or phone detected at line:', row)\n",
        "    #  predictions[row] = 4\n",
        "\n",
        "    #if isNameRow(string) or isWebsiteRow(string) or isPhoneRow(string):\n",
        "    #  print('@ LC_error_adjustment name, website, or phone detected at line:', row)\n",
        "    #  predictions[row] = 4\n",
        "\n",
        "    for item in label5_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 5\n",
        "        break\n",
        "  \n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt6e1D3tnhfM"
      },
      "source": [
        "# Note: labels with 5 could be ignore when labeling whole resumes\n",
        "# Input: csv_file of the with Sentence only is ok\n",
        "def get_label(csv_file, model=None, get_accuracy=False):\n",
        "  LC_model = 0\n",
        "  if (model==None):\n",
        "    LC_model = load_model('/content/drive/MyDrive/ECS 193A ResumeData/Model & Rev_Model/LC_model.h5')\n",
        "  else:\n",
        "    LC_model = load_model(model)\n",
        "  \n",
        "  max_length = 300*5\n",
        "  outputfile = '/content/LC_output.csv'\n",
        "\n",
        "  print('Loading csv into dataframe...')\n",
        "  # Load csv to pandas\n",
        "  data = pandas.read_csv(csv_file)\n",
        "  data.rename(columns={data.columns[0]: \"Init_index\"}, inplace=True)\n",
        "  X = data.copy()\n",
        "\n",
        "  print('Converting words into vectors...')\n",
        "  for row in range(len(X['Sentence'])):\n",
        "    line = X['Sentence'][row].split()\n",
        "    if len(line) > 5:\n",
        "      line = line[:5]\n",
        "    big_vector = line2vec(line, max_length)\n",
        "    X['Sentence'][row] = numpy.asarray(big_vector).astype('float32')\n",
        "\n",
        "  print('Done!')\n",
        "\n",
        "  print('Perparing inputs for prediction model...')\n",
        "  input = X['Sentence'].to_numpy().tolist()\n",
        "  for row in range (len(input)):\n",
        "    input[row] = input[row].tolist()\n",
        "  input = numpy.asarray(input)\n",
        "  \n",
        "  print('Predicting...')\n",
        "  predictions = (LC_model.predict_classes(input))\n",
        "\n",
        "  # Pick up missed traits\n",
        "  predictions = LC_error_adjustment(data, input, predictions)\n",
        "\n",
        "  # Prepare output as copy of data\n",
        "  output = data.copy()\n",
        "\n",
        "  # Append predictions onto output\n",
        "  print('Placing predictions into csv')\n",
        "  output['pred_Label'] = predictions\n",
        "  \n",
        "  # Check accuracy of this prediction\n",
        "  if (get_accuracy == True):\n",
        "    accuracy = 0\n",
        "    correction_check = []\n",
        "    for i in range (len(predictions)):\n",
        "      if (predictions[i] == output.at[i, 'Label']):\n",
        "        accuracy+=1\n",
        "        correction_check.append(1)\n",
        "      else:\n",
        "        correction_check.append(0)\n",
        "    \n",
        "    output['Correction_check'] = correction_check\n",
        "    print('Prediction accuracy: ', str(accuracy/len(predictions)*100))\n",
        "    \n",
        "  output.to_csv(outputfile, index=True)\n",
        "\n",
        "  return outputfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lABOSLDiKIo"
      },
      "source": [
        "# Input: string\n",
        "# Output: boolean\n",
        "\n",
        "def isQualification(words):\n",
        "  #words = sentence.lower().split(' ')\n",
        "  qualifications = ['level', 'levels', 'chartered', 'certified']\n",
        "  if len(words) >= 2:\n",
        "    for item in qualifications:\n",
        "      if item in words:\n",
        "        return True\n",
        "  \n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU0Rdvye4tpu"
      },
      "source": [
        "# Input: string\n",
        "# Output: boolean\n",
        "\n",
        "def isSchoolName(words):\n",
        "  #words = sentence.lower().split(' ')\n",
        "  schoolNames = ['university', 'college', 'school' , 'institute', 'polytechnic', 'cpa', \n",
        "                 'universiti', 'program', 'université']\n",
        "  namePhrase = [['chartered', 'financial', 'analyst']]\n",
        "  schoolNamesBlackList = ['award', 'awarded', 'awards', 'president', 'leader', 'leaders', 'prize', \n",
        "                          'manager', 'middle', 'high', 'name']\n",
        "  if len(words) >= 2:\n",
        "    for item in schoolNamesBlackList:\n",
        "      if item in words:\n",
        "        return False\n",
        "\n",
        "    for item in schoolNames:\n",
        "      if item in words:\n",
        "        return True\n",
        "\n",
        "    # Check if each namePhrase is contained in sentence by\n",
        "    #   ensure all strings in phrase also in array words\n",
        "    #   if all strings are in array words, return True\n",
        "    #   else, check the next phrase\n",
        "    #   if exchange all phrases, return false\n",
        "    for item in namePhrase:\n",
        "      flag = True\n",
        "      for subitem in item:\n",
        "        if subitem not in words:\n",
        "          flag = False\n",
        "          break\n",
        "      if flag:\n",
        "        return flag\n",
        "  \n",
        "  return False\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIdEszBd5IH8"
      },
      "source": [
        "# Input: string\n",
        "# Output: boolean\n",
        "\n",
        "def isSchoolDegree(words):\n",
        "  schoolDegrees = ['bachelor', 'master', 'masters', 'minor', 'major', 'institute', \n",
        "                   'ba', 'ms', 'bs', 'bsc', 'b', 'msc', 'bba', 'hd', 'mba', 'ma', 'm',\n",
        "                   'gce', 'level', 'levels', \n",
        "                   'diploma', 'business', 'degree', 'programme', 'chartered', \n",
        "                   'discipline', 'accounting', 'finance', 'course', 'coursework', 'studies', \n",
        "                   'science', 'computer', 'engineering', 'marketing', 'art', 'design', 'english']\n",
        "  degreesphrase = [['foundations', 'program'], ['exchange', 'student']]\n",
        "  schoolDegreesBlackList = ['award', 'awarded', 'awards', 'member', 'academic', 'academics', \n",
        "                            'prize', 'ranked', 'developed', 'prepared', 'technical', 'it', 'office',\n",
        "                            'manager', 'relevant', 'gpa', 'i', 'name', 'hours']\n",
        "  if len(words) >= 2:\n",
        "    for item in schoolDegreesBlackList:\n",
        "      if item in words:\n",
        "        return False\n",
        "\n",
        "    for item in schoolDegrees:\n",
        "      if item in words:\n",
        "        return True\n",
        "\n",
        "    # Check if each degreesphrase is contained in sentence by\n",
        "    #   ensure all strings in phrase also in array words\n",
        "    #   if all strings are in array words, return True\n",
        "    #   else, check the next phrase\n",
        "    #   if exchange all phrases, return false\n",
        "    for item in degreesphrase:\n",
        "      flag = True\n",
        "      for subitem in item:\n",
        "        if subitem not in words:\n",
        "          flag = False\n",
        "          break\n",
        "      if flag:\n",
        "        return flag\n",
        "  \n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkjWQYLm5ITs"
      },
      "source": [
        "# Input: string\n",
        "# Output: boolean\n",
        "\n",
        "def isSchoolDate(words):\n",
        "    schoolDateBlackList = ['gpa', 'scholarship', 'award', 'awarded', 'awards', 'out', 'dean', 'manager']\n",
        "\n",
        "    if len(words) <= 4:\n",
        "      for item in schoolDateBlackList:\n",
        "        if item in words:\n",
        "          return False\n",
        "\n",
        "      for w in words:\n",
        "        if w.isdigit():\n",
        "          if len(w) == 1:\n",
        "            return False\n",
        "          else:\n",
        "            return True\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUvftDtH0X5O"
      },
      "source": [
        "# Input: Path of label 1 csv with lineNum and Sentence\n",
        "# Output: None -- csv file with data picked out and grouped up\n",
        "\n",
        "def extract_education(csv_file_path):\n",
        "  import pandas as pd\n",
        "\n",
        "  i = 0 # --> indexing for storage array\n",
        "\n",
        "  startLine = []\n",
        "  qualification, schoolName, schoolDegree, schoolDate = [[]], [[]], [[]], [[]]\n",
        "\n",
        "  # Load csv Label1 into pandas\n",
        "  data = pd.read_csv(csv_file_path)\n",
        "\n",
        "  # Initalize beginning\n",
        "  startLine.append(0)\n",
        "  qualification.append([])\n",
        "  schoolName.append([])\n",
        "  schoolDegree.append([])\n",
        "  schoolDate.append([])\n",
        "\n",
        "  # For each rows in 'Sentence', place them into classified bins\n",
        "  for row in range(len(data['Sentence'])):\n",
        "    strings = data['Sentence'][row].split(' ')\n",
        "    if data['Sentence'][row].split().count() >= 10 :\n",
        "      continue\n",
        "    # Parse sentence into subsentences\n",
        "    subsentence = []\n",
        "    subsentence.append([])\n",
        "    j = 0\n",
        "    for word in strings:\n",
        "      if word == '':\n",
        "        subsentence.append([])\n",
        "        j+=1\n",
        "      else:\n",
        "        subsentence[j].append(word)\n",
        "    for sentence in subsentence:\n",
        "      # Skip empty cells\n",
        "      if not sentence:\n",
        "        continue\n",
        "      if sentence[0] == \"--------------------------------------------------\":\n",
        "        startLine.append(data[data.columns[0]][row])\n",
        "        if row+1 != len(data['Sentence']):\n",
        "          qualification.append([])\n",
        "          schoolName.append([])\n",
        "          schoolDegree.append([])\n",
        "          schoolDate.append([])\n",
        "        i+=1\n",
        "      lower_sentence = [x.lower() for x in sentence]\n",
        "      if isQualification(lower_sentence):\n",
        "        qualification[i].append(' '.join(lower_sentence))\n",
        "      elif isSchoolName(lower_sentence):\n",
        "        schoolName[i].append(' '.join(lower_sentence))\n",
        "      elif isSchoolDegree(lower_sentence):\n",
        "        schoolDegree[i].append(' '.join(lower_sentence))\n",
        "        print('School Degree Length:', data['Sentence'][row].split().count())\n",
        "      elif isSchoolDate(lower_sentence):\n",
        "        schoolDate[i].append(' '.join(lower_sentence))\n",
        "\n",
        "  # Make data into directionary\n",
        "  data = {'Init_index':startLine, \n",
        "          'SchoolName':schoolName,\n",
        "          'SchoolDegree':schoolDegree,\n",
        "          'SchoolDate':schoolDate,\n",
        "          'Qualifications':qualification}\n",
        "  \n",
        "  # Convert data into dataFrame\n",
        "  EducationList = pd.DataFrame(data)\n",
        "\n",
        "  print(EducationList)\n",
        "\n",
        "  # Put dataFrame into csv\n",
        "  EducationList.to_csv(\"/content/EducationList.csv\", index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCWgsMdv96x3"
      },
      "source": [
        "# Load all the existing models\n",
        "\n",
        "\n",
        "print('Loading word embedders')\n",
        "# Models for word embedding\n",
        "# If adding another path to load model, please add it to None, and comment out the existing load_model line\n",
        "model = None\n",
        "model = load_model('/content/drive/MyDrive/Resume Scanner Folder/WE_model.h5')\n",
        "rev_model = None\n",
        "rev_model = load_model('/content/drive/MyDrive/Resume Scanner Folder/WE_rev_model.h5')\n",
        "\n",
        "# Saved tokenizer and maximum length of word\n",
        "tokenizer_filepath = '/content/drive/MyDrive/Resume Scanner Folder/tokenizer.pkl'\n",
        "with open(tokenizer_filepath, 'rb') as f:\n",
        "    tokenizer = pickle.load(f)  \n",
        "max_length_filepath = '/content/drive/MyDrive/Resume Scanner Folder/max_length.pkl'\n",
        "with open(max_length_filepath, 'rb') as f:\n",
        "    max_length = pickle.load(f)\n",
        "\n",
        "#load spacy GloVe Model\n",
        "#Can load en_core_web_lg for more unique vectors\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "print('Complete')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LYsKZ6AGW5K"
      },
      "source": [
        "# Input: file path of clean text and unclean text\n",
        "# Output: clean_unclean_text dataframe\n",
        "\n",
        "def combine_text(outputtext,outputtext_org):\n",
        "  clean = pandas.read_csv (outputtext, skip_blank_lines=False)\n",
        "  unclean = pandas.read_csv (outputtext_org, sep=\"\\n\")\n",
        "\n",
        "  clean_sentences = clean['Sentence']\n",
        "\n",
        "  unclean.insert(1,\"clean_Sentence\",clean_sentences, True)\n",
        "  \n",
        "  return unclean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEF1Us-KKZDE"
      },
      "source": [
        "# Input: file path of LC_output, clean_unclean_text dataframe\n",
        "# Output: labeled_text df\n",
        "# Labelize the combined text and save it as labeled_text.csv\n",
        "\n",
        "def labelizingfulltext(lc_filepath, placeholding):\n",
        "  #read LC_output.csv and placeholding.csv as datafram format\n",
        "  lc_output = pandas.read_csv(lc_filepath)\n",
        "\n",
        "  lcdf = lc_output.iloc[:,[1,3]]\n",
        "  count_row = lcdf.shape[0]\n",
        "\n",
        "  #replace Label 5 with the previous label in lc_output\n",
        "  for i in range (1,count_row):\n",
        "    if lcdf.iloc[i,1] == 5:\n",
        "      lcdf.iloc[i,1] = lcdf.iloc[i-1,1]\n",
        "\n",
        "  #get each line's label and save them into label list\n",
        "  label = []\n",
        "  for i in range(1,count_row):\n",
        "    for j in range(0,lcdf.iloc[i,0]-lcdf.iloc[i-1,0]):   # lcdf.iloc[i-1,0] + j = currect row\n",
        "      label = label + [lcdf.iloc[i-1,1]]\n",
        "\n",
        "  label = label + [0]\n",
        "\n",
        "  #Add label list to placeholding dataframe as a column\n",
        "  placeholding.insert(2,\"Label\",label,True)\n",
        "  placeholding = placeholding.rename({'Sentence': 'unclean_Sentence'}, axis=1)\n",
        "\n",
        "  #Change all the lines that are website link or phone number to 4\n",
        "  for i in range(1, placeholding.shape[0]):\n",
        "    webstring = placeholding.at[i, 'clean_Sentence']\n",
        "    phonestring = placeholding.at[i, 'unclean_Sentence']\n",
        "    if isPhoneRow(phonestring) or isWebsiteRow(webstring):\n",
        "      placeholding.at[i, \"Label\"] = 4\n",
        "\n",
        "  #Save dataframe to Placeholding.csv\n",
        "  placeholding.to_csv('labeled_text.csv')\n",
        "  placeholding.insert(0,\"index\",numpy.arange(len(placeholding)))\n",
        "\n",
        "\n",
        "  return placeholding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2Tm4R-vi2Wi"
      },
      "source": [
        "def extract_title_corenlp(text):\n",
        "  document = client.annotate(text)\n",
        "  title_set = []\n",
        "  for sent in document.sentence:\n",
        "          for m in sent.mentions:\n",
        "            if m.entityType == \"TITLE\" and text.count(\" \") < 9:\n",
        "              #title_set.append(m.entityMentionText)\n",
        "              title_set.append(m.entityMentionText.lower())\n",
        "  return title_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ9ys89pjELe"
      },
      "source": [
        "def extract_organization_corenlp(text):\n",
        "  document = client.annotate(text)\n",
        "  org_set = []\n",
        "  for sent in document.sentence:\n",
        "          for m in sent.mentions:\n",
        "            if m.entityType == \"ORGANIZATION\" and text.count(\" \") < 9:\n",
        "              #org_set.append(m.entityMentionText)\n",
        "              org_set.append(m.entityMentionText.lower())\n",
        "  return org_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2-gV69Rn4Ns"
      },
      "source": [
        "def extract_email(text):\n",
        "    '''\n",
        "    Helper function to extract email id from text\n",
        "    :param text: plain text extracted from resume file\n",
        "    '''\n",
        "    email = re.findall(r\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", text)\n",
        "    if email:\n",
        "        try:\n",
        "            return email[0].split()[0].strip(';')\n",
        "        except IndexError:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHZcw2puppGG"
      },
      "source": [
        "def extract_mobile_number(text):\n",
        "\n",
        "    mob_num_regex = r'''(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\n",
        "                        [-\\.\\s]*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})'''\n",
        "    phone = re.findall(re.compile(mob_num_regex), text)\n",
        "    \n",
        "    if phone:\n",
        "        number = ''.join(phone[0])\n",
        "        return number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvEqXY2eprmD"
      },
      "source": [
        "def extract_skills(nlp_text, noun_chunks, skills_file=None):\n",
        "    '''\n",
        "    Helper function to extract skills from spacy nlp text\n",
        "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
        "    :param noun_chunks: noun chunks extracted from nlp text\n",
        "    :return: list of skills extracted\n",
        "    '''\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    if not skills_file:\n",
        "        data = pd.read_csv(\n",
        "            '/content/drive/MyDrive/Resume Scanner Folder/skills.csv'\n",
        "        )\n",
        "    else:\n",
        "        data = pd.read_csv(skills_file)\n",
        "    skills = list(data.columns.values)\n",
        "    skillset = []\n",
        "    # check for one-grams\n",
        "    for token in tokens:\n",
        "        if token.lower() in skills:\n",
        "            #skillset.append(token)\n",
        "            skillset.append(token.lower())\n",
        "\n",
        "    # check for bi-grams and tri-grams\n",
        "    for token in noun_chunks:\n",
        "        token = token.text.lower().strip()\n",
        "        if token in skills:\n",
        "            skillset.append(token)\n",
        "    #return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
        "    return [i.lower() for i in skillset]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf5CbNDEnFAv"
      },
      "source": [
        "def extract_name_new(text):\n",
        "  classified_text = st.tag(text.split())\n",
        "  named_entities = get_continuous_chunks(classified_text)\n",
        "  named_entities_str_tag = [(\" \".join([token for token, tag in ne]), ne[0][1]) for ne in named_entities]\n",
        " \n",
        "  for x in named_entities_str_tag:\n",
        "    if x[1] == \"PERSON\":\n",
        "      return x[0]\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2reoTtLEp6_K"
      },
      "source": [
        "def get_continuous_chunks(tagged_sent):\n",
        "    continuous_chunk = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for token, tag in tagged_sent:\n",
        "        if tag != \"O\":\n",
        "            current_chunk.append((token, tag))\n",
        "        else:\n",
        "            if current_chunk: # if the current chunk is not empty\n",
        "                continuous_chunk.append(current_chunk)\n",
        "                current_chunk = []\n",
        "    # Flush the final current_chunk into the continuous_chunk, if any.\n",
        "    if current_chunk:\n",
        "        continuous_chunk.append(current_chunk)\n",
        "    return continuous_chunk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni46-J_NS7Gy"
      },
      "source": [
        "#Input: Path of laved clean text and unclean text file\n",
        "#Output: None \n",
        "\n",
        "def extract_non_context(fulltext):\n",
        "    import pandas as pd\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    i = 0\n",
        "\n",
        "    startLine = []\n",
        "    name, email, skill, phone, org, title = [[]], [[]], [[]], [[]], [[]], [[]]\n",
        "    qualification, schoolName, schoolDegree, schoolDate = [[]], [[]], [[]], [[]]\n",
        "\n",
        "    #get label4 and label1 sentences\n",
        "    unclean_sentences = fulltext['unclean_Sentence']\n",
        "    labels = fulltext['Label']\n",
        "    clean_sentences = fulltext['clean_Sentence']\n",
        "    info_sentences = []\n",
        "    skill_sentences = []\n",
        "    org_sentences = []\n",
        "    education_sentence = []\n",
        "    indx = []\n",
        "\n",
        "    for row in range(len(unclean_sentences)):\n",
        "     if labels[row] == 4 or labels[row] == 0:\n",
        "       info_sentences.append(unclean_sentences[row])\n",
        "       indx.append(fulltext.iloc[row][0])\n",
        "\n",
        "    data = {'Init_index': indx,\n",
        "            'unclean_Sentence': info_sentences}\n",
        "    personalInfo = pd.DataFrame(data)\n",
        "\n",
        "    indx = []\n",
        "    for row in range(len(unclean_sentences)):\n",
        "     if labels[row] == 3 or labels[row] == 0:\n",
        "       skill_sentences.append(unclean_sentences[row])\n",
        "       indx.append(fulltext.iloc[row][0])\n",
        "\n",
        "    data = {'Init_index': indx,\n",
        "            'unclean_Sentence': skill_sentences}\n",
        "    skillInfo = pd.DataFrame(data)\n",
        "\n",
        "    indx = []\n",
        "    for row in range(len(unclean_sentences)):\n",
        "     if labels[row] == 2 or labels[row] == 0:\n",
        "       org_sentences.append(unclean_sentences[row])\n",
        "       indx.append(fulltext.iloc[row][0])\n",
        "\n",
        "    data = {'Init_index': indx,\n",
        "            'unclean_Sentence': org_sentences}\n",
        "    orgInfo = pd.DataFrame(data)\n",
        "\n",
        "    indx = []\n",
        "    for row in range(len(clean_sentences)):\n",
        "     if labels[row] == 1 or labels[row] == 0:\n",
        "       education_sentence.append(clean_sentences[row])\n",
        "       indx.append(fulltext.iloc[row][0])\n",
        "    \n",
        "    data = {'Init_index': indx,\n",
        "            'clean_Sentence': education_sentence}\n",
        "    educationInfo = pd.DataFrame(data)\n",
        "\n",
        "    #init beginning\n",
        "    startLine.append(0)\n",
        "    name.append([])\n",
        "    org.append([])\n",
        "    title.append([])\n",
        "    email.append([])\n",
        "    phone.append([])\n",
        "    skill.append([])\n",
        "    qualification.append([])\n",
        "    schoolName.append([])\n",
        "    schoolDegree.append([])\n",
        "    schoolDate.append([])\n",
        "\n",
        "    #extract org and title\n",
        "    print(len(orgInfo['unclean_Sentence']))\n",
        "    for row in range(len(orgInfo['unclean_Sentence'])):\n",
        "        string1 = orgInfo['unclean_Sentence'][row]\n",
        "        if string1 == \"--------------------------------------------------\":\n",
        "            startLine.append(orgInfo.iloc[row][1])\n",
        "            if row+1 != len(orgInfo['unclean_Sentence']):\n",
        "              org.append([])\n",
        "              title.append([])\n",
        "            i += 1\n",
        "\n",
        "        orgtemp = extract_organization_corenlp(str(string1))\n",
        "        titletemp = extract_title_corenlp(str(string1))\n",
        "\n",
        "        if len(orgtemp) != 0:\n",
        "            for oneorg in orgtemp:\n",
        "                org[i].append(oneorg)\n",
        "        if len(titletemp) != 0:\n",
        "            for onetitle in titletemp:\n",
        "                title[i].append(onetitle)\n",
        "\n",
        "    #extract personal information\n",
        "    i = 0\n",
        "    startLine = []\n",
        "    startLine.append(0)\n",
        "    firstLine = personalInfo['unclean_Sentence'][0]\n",
        "    for row in range(len(personalInfo['unclean_Sentence'])):\n",
        "        string1 = personalInfo['unclean_Sentence'][row]\n",
        "        if string1 == \"--------------------------------------------------\":\n",
        "            if len(name[i]) < 1:\n",
        "              name[i].append(firstLine)\n",
        "            startLine.append(personalInfo.iloc[row][1])\n",
        "            if row+1 != len(personalInfo['unclean_Sentence']):\n",
        "              firstLine = personalInfo['unclean_Sentence'][row + 1]\n",
        "              name.append([])\n",
        "              email.append([])\n",
        "              phone.append([])\n",
        "            i += 1\n",
        "\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        nlp = nlp(string1)\n",
        "        # TODO: add skills_file location here\n",
        "        #skills_file = None\n",
        "        skills_file = None\n",
        "        noun_chunks = list(nlp.noun_chunks)\n",
        "\n",
        "        if(len(name[i]) < 1):\n",
        "          #nametemp = extract_name_new(str(string1))\n",
        "          nametemp = \"test\"\n",
        "        emailtemp = extract_email(str(string1))\n",
        "        phonetemp = extract_mobile_number(str(string1))\n",
        "\n",
        "        if emailtemp != None:\n",
        "            email[i].append(emailtemp)\n",
        "        elif nametemp != None and len(name[i]) < 1:\n",
        "            name[i].append(nametemp)\n",
        "        elif phonetemp != None and len(phone[i]) < 1:\n",
        "            phone[i].append(phonetemp)\n",
        "\n",
        "    #extract skill\n",
        "    i = 0\n",
        "    startLine = []\n",
        "    startLine.append(0)\n",
        "    for row in range(len(skillInfo['unclean_Sentence'])):\n",
        "        string1 = skillInfo['unclean_Sentence'][row]\n",
        "        if string1 == \"--------------------------------------------------\":\n",
        "            startLine.append(skillInfo.iloc[row][1])\n",
        "            if row+1 != len(skillInfo['unclean_Sentence']):\n",
        "              skill.append([])\n",
        "            i += 1\n",
        "\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        nlp = nlp(string1)\n",
        "        skills_file = None\n",
        "        noun_chunks = list(nlp.noun_chunks)\n",
        "\n",
        "        skilltemp = extract_skills(nlp, noun_chunks, skills_file)\n",
        "\n",
        "        if len(skilltemp) != 0:\n",
        "            for oneskill in skilltemp:\n",
        "                skill[i].append(oneskill)\n",
        "\n",
        "    #extract education information\n",
        "    i = 0\n",
        "    startLine = []\n",
        "    startLine.append(0)\n",
        "\n",
        "    for row in range(len(educationInfo['clean_Sentence'])):\n",
        "      strings = educationInfo['clean_Sentence'][row].split(' ')\n",
        "      #if orgInfo['unclean_Sentence'][row].count(\" \") >= 10:\n",
        "      #  continue\n",
        "      if educationInfo['clean_Sentence'][row].split().count() >= 10 :\n",
        "        continue\n",
        "      # Parse sentence into subsentences\n",
        "      subsentence = []\n",
        "      subsentence.append([])\n",
        "      j = 0\n",
        "      for word in strings:\n",
        "        if word == '':\n",
        "          subsentence.append([])\n",
        "          j+=1\n",
        "        else:\n",
        "          subsentence[j].append(word)\n",
        "      for sentence in subsentence:\n",
        "        # Skip empty cells\n",
        "        if not sentence:\n",
        "          continue\n",
        "        if sentence[0] == \"--------------------------------------------------\":\n",
        "          startLine.append(educationInfo[educationInfo.columns[0]][row])\n",
        "          if row+1 != len(educationInfo['clean_Sentence']):\n",
        "            qualification.append([])\n",
        "            schoolName.append([])\n",
        "            schoolDegree.append([])\n",
        "            schoolDate.append([])\n",
        "          i+=1\n",
        "        lower_sentence = [x.lower() for x in sentence]\n",
        "        if isQualification(lower_sentence):\n",
        "          qualification[i].append(' '.join(lower_sentence))\n",
        "        elif isSchoolName(lower_sentence):\n",
        "          schoolName[i].append(' '.join(lower_sentence))\n",
        "        elif isSchoolDegree(lower_sentence):\n",
        "          schoolDegree[i].append(' '.join(lower_sentence))\n",
        "        elif isSchoolDate(lower_sentence):\n",
        "          schoolDate[i].append(' '.join(lower_sentence))\n",
        "\n",
        "    # Make data into directionary\n",
        "    data = {'Init_index': startLine,\n",
        "            'name': name,\n",
        "            'organization': org,\n",
        "            'position': title,\n",
        "            'email': email,\n",
        "            'phone': phone,\n",
        "            'skill': skill,\n",
        "            'SchoolName':schoolName,\n",
        "            'SchoolDegree':schoolDegree,\n",
        "            'SchoolDate':schoolDate,\n",
        "            'Qualifications':qualification}\n",
        "\n",
        "    # Convert data into dataFrame\n",
        "    noncontextualinfo = pd.DataFrame(data)\n",
        "    noncontextualinfo = noncontextualinfo[:-1]\n",
        "\n",
        "    # Put dataFrame into csv\n",
        "    noncontextualinfo.to_csv(\"/content/Noncontutalinfo.csv\", index=True)\n",
        "\n",
        "    return noncontextualinfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjTsWzYiu1I9"
      },
      "source": [
        "# Note: might want to save nlp vectors\n",
        "# Resume scanner (in Alpha)\n",
        "# Input: the Directory with resume of interest (The directory must only contain pdf files or JPG files)\n",
        "# Output: the csv file with labels marked for each resumes\n",
        "def ResumeScanner(directory):\n",
        "  directory = processFileFormat(directory)\n",
        "\n",
        "  print('Converting resumes into text...')\n",
        "  # Convert all resumes in directory to text fileR\n",
        "  outputtext, outputtext_org = presprocess_resume(directory)\n",
        "  print('Finished Converting')\n",
        "\n",
        "  print('Picking out headers...')\n",
        "  # Find the headers and output into csv\n",
        "  #HC_model_path = None\n",
        "  HC_model_path = '/content/drive/MyDrive/Resume Scanner Folder/HC_model.h5'\n",
        "  HC_outputfile = get_headers(outputtext_org, HC_model_path)\n",
        "  print('Finished Picking out headers')\n",
        "\n",
        "  print('Picking out labels...')\n",
        "  # Label each headers and output into csv\n",
        "  #LC_model_path = None\n",
        "  LC_model_path = '/content/drive/MyDrive/Resume Scanner Folder/LC_model.h5'\n",
        "  LC_outputfile = get_label(HC_outputfile,LC_model_path)\n",
        "  print('Finished Picking out labels')\n",
        "\n",
        "  print('Combining text data...')\n",
        "  combine_text_df = combine_text(outputtext, outputtext_org)\n",
        "  print('Finished Combining text data')\n",
        "\n",
        "  print('Lablizing text data...')\n",
        "  labeled_text_df = labelizingfulltext(LC_outputfile,combine_text_df)\n",
        "  print('Finished Lablizing text data...')\n",
        "\n",
        "  print('Extracting information...')\n",
        "  info_df = extract_non_context(labeled_text_df)\n",
        "  info = pandas.read_csv('/content/Noncontutalinfo.csv')\n",
        "  sendCSVFile(info)\n",
        "  print('Finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTk3ZEyc9DCT"
      },
      "source": [
        "ResumeScanner('/content/drive/MyDrive/ECS 193A ResumeData/Resume_Google')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}