{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResumeScanner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rwfchan/ECS-193B-Testing-Repo/blob/main/ResumeScanner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB2cNrD1fmRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf5b4fcf-d00a-45b8-976d-4a6950569c7b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gin_W7SFYl4W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "4536974b-d510-4f81-c2fb-f2a02577d8b4"
      },
      "source": [
        "pip install --upgrade google-cloud-vision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google-cloud-vision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/51/e6321162877a2903ba3158737b944cf582a62b7f045e22864ab56b764adc/google_cloud_vision-2.3.1-py2.py3-none-any.whl (461kB)\n",
            "\r\u001b[K     |▊                               | 10kB 14.3MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 15.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 10.1MB/s eta 0:00:01\r\u001b[K     |██▉                             | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 92kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 102kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 112kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 122kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 133kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 143kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 153kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 163kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 174kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 184kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 194kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 204kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 215kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 225kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 235kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 245kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 256kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 266kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 276kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 286kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 296kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 307kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 317kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 327kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 337kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 348kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 358kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 368kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 378kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 389kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 399kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 409kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 419kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 430kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 440kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 450kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 460kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 471kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-vision) (1.26.3)\n",
            "Collecting proto-plus>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/8a/61c5a9b9b6288f9b060b6e3d88374fc083953a29aeac7206616c2d3c9c8e/proto_plus-1.18.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.53.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.28.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (54.2.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (0.4.8)\n",
            "Installing collected packages: proto-plus, google-cloud-vision\n",
            "Successfully installed google-cloud-vision-2.3.1 proto-plus-1.18.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z0exEKNhLdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf717fc5-3484-4b36-ae80-8472cab94249"
      },
      "source": [
        "pip install pdf2jpg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdf2jpg\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/73/36611a5b4fe56570faf386df294483d6e0f115664f458b4e4979359fcb56/pdf2jpg-1.0.tar.gz (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 5.6MB/s \n",
            "\u001b[?25hCollecting img2pdf\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/ed/5167992abaf268f5a5867e974d9d36a8fa4802800898ec711f4e1942b4f5/img2pdf-0.4.0.tar.gz (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 43.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from img2pdf->pdf2jpg) (7.1.2)\n",
            "Collecting pikepdf\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/91/1f1f68225306e8bc0dc7fda8e3ebc6fbef58ad0d2b45fa6d47372a7dfcdf/pikepdf-2.11.1-cp37-cp37m-manylinux2010_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 41.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.0 in /usr/local/lib/python3.7/dist-packages (from pikepdf->img2pdf->pdf2jpg) (4.2.6)\n",
            "Building wheels for collected packages: pdf2jpg, img2pdf\n",
            "  Building wheel for pdf2jpg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdf2jpg: filename=pdf2jpg-1.0-cp37-none-any.whl size=4287419 sha256=3751826ecf9d6a50befb2443094ef4afc95c08b12e6e6b9e6a22645fbcc7e42e\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/dc/72/17e98df36aef90f761a81a74f84414ef16db83fdaa69739909\n",
            "  Building wheel for img2pdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for img2pdf: filename=img2pdf-0.4.0-cp37-none-any.whl size=40503 sha256=c17dff3fea2ad50f110774337b2300a3d776a8bb1aa34bd25b4da316ee2f094f\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/f5/7f/b88a76bac33669118e7549f4856bbe975a3ad2e59280c59196\n",
            "Successfully built pdf2jpg img2pdf\n",
            "Installing collected packages: pikepdf, img2pdf, pdf2jpg\n",
            "Successfully installed img2pdf-0.4.0 pdf2jpg-1.0 pikepdf-2.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idZg4VKxdeJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2f0888-3ceb-4f16-fc64-7fc9e8f80039"
      },
      "source": [
        "pip install --upgrade spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/70/a0b8bd0cb54d8739ba4d6fb3458785c3b9b812b7fbe93b0f10beb1a53ada/spacy-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 300kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Collecting thinc<8.1.0,>=8.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/08/20e707519bcded1a0caa6fd024b767ac79e4e5d0fb92266bb7dcf735e338/thinc-8.0.2-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 35.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/82/a5/b5021c74c04cac35a27d34cbf3146d86eb8e173b4491888bc4908c4c8b3b/catalogue-2.0.3-py3-none-any.whl\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.1)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 47.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.2.0)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Collecting srsly<3.0.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/53/97dc0197cca9357369b3b71bf300896cf2d3604fa60ffaaf5cbc277de7de/pathy-0.4.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/78/d8/e25bc7f99877de34def57d36769f0cce4e895b374cdc766718efc724f9ac/spacy_legacy-3.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (1.1.1)\n",
            "Building wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=2a275bc5df7cdb303076c1711a9eed16e0a1e5a968f0b0073cbe45756bec2145\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built smart-open\n",
            "Installing collected packages: catalogue, srsly, pydantic, thinc, typer, smart-open, pathy, spacy-legacy, spacy\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: smart-open 5.0.0\n",
            "    Uninstalling smart-open-5.0.0:\n",
            "      Successfully uninstalled smart-open-5.0.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.3 pathy-0.4.0 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.5 spacy-legacy-3.0.2 srsly-2.4.1 thinc-8.0.2 typer-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUuughx9dW8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d60a6b-9538-4c76-c799-15b53031f943"
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-18 12:25:03.073462: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-md==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.0.0/en_core_web_md-3.0.0-py3-none-any.whl (47.1MB)\n",
            "\u001b[K     |████████████████████████████████| 47.1MB 97kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (20.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (54.2.0)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.10.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.0)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zx9CoyRmB3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a51df986-3e1f-4e66-cd0a-de9608b7c590"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-18 12:25:15.331586: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 329kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (54.2.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5qmxmQ-FszC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e882515e-088b-4ad7-de15-bc5ba9df2b55"
      },
      "source": [
        "pip install mysql-connector-python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mysql-connector-python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f7/b783b60a3bd8aea348990c60fb0b3ed2f843c6f40ad29e37a89a5d50d7a3/mysql_connector_python-8.0.23-cp37-cp37m-manylinux1_x86_64.whl (18.0MB)\n",
            "\u001b[K     |████████████████████████████████| 18.1MB 250kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from mysql-connector-python) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.0.0->mysql-connector-python) (54.2.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.0.0->mysql-connector-python) (1.15.0)\n",
            "Installing collected packages: mysql-connector-python\n",
            "Successfully installed mysql-connector-python-8.0.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQmV3i01ZZ-8"
      },
      "source": [
        "# SpecialStar\n",
        "import spacy\n",
        "from spacy.vocab import Vocab\n",
        "import numpy\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import np_utils\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import GlobalAveragePooling1D\n",
        "from keras.models import load_model\n",
        "import pickle\n",
        "from spacy.matcher import Matcher\n",
        "import pandas as pd\n",
        "import re\n",
        "import mysql.connector\n",
        "from sqlalchemy import create_engine\n",
        "import io\n",
        "import os\n",
        "import shutil\n",
        "from pdf2jpg import pdf2jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUSmzCeKnuB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb091ca6-266d-4914-85df-039688c384f0"
      },
      "source": [
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "!wget 'https://nlp.stanford.edu/software/stanford-ner-2018-10-16.zip'\n",
        "!unzip stanford-ner-2018-10-16.zip\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "st = StanfordNERTagger('/content/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
        "                       '/content/stanford-ner-2018-10-16/stanford-ner.jar',\n",
        "                       encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-18 12:25:32--  https://nlp.stanford.edu/software/stanford-ner-2018-10-16.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-ner-2018-10-16.zip [following]\n",
            "--2021-04-18 12:25:33--  https://downloads.cs.stanford.edu/nlp/software/stanford-ner-2018-10-16.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180358328 (172M) [application/zip]\n",
            "Saving to: ‘stanford-ner-2018-10-16.zip’\n",
            "\n",
            "stanford-ner-2018-1 100%[===================>] 172.00M  4.88MB/s    in 33s     \n",
            "\n",
            "2021-04-18 12:26:06 (5.24 MB/s) - ‘stanford-ner-2018-10-16.zip’ saved [180358328/180358328]\n",
            "\n",
            "Archive:  stanford-ner-2018-10-16.zip\n",
            "   creating: stanford-ner-2018-10-16/\n",
            "  inflating: stanford-ner-2018-10-16/README.txt  \n",
            "  inflating: stanford-ner-2018-10-16/ner-gui.bat  \n",
            "  inflating: stanford-ner-2018-10-16/build.xml  \n",
            "  inflating: stanford-ner-2018-10-16/stanford-ner.jar  \n",
            "  inflating: stanford-ner-2018-10-16/sample-conll-file.txt  \n",
            "  inflating: stanford-ner-2018-10-16/sample.ner.txt  \n",
            "  inflating: stanford-ner-2018-10-16/stanford-ner-3.9.2-sources.jar  \n",
            "   creating: stanford-ner-2018-10-16/lib/\n",
            "  inflating: stanford-ner-2018-10-16/lib/joda-time.jar  \n",
            "  inflating: stanford-ner-2018-10-16/lib/stanford-ner-resources.jar  \n",
            "  inflating: stanford-ner-2018-10-16/lib/jollyday-0.4.9.jar  \n",
            "  inflating: stanford-ner-2018-10-16/ner-gui.command  \n",
            "  inflating: stanford-ner-2018-10-16/ner.sh  \n",
            "  inflating: stanford-ner-2018-10-16/stanford-ner-3.9.2.jar  \n",
            "  inflating: stanford-ner-2018-10-16/NERDemo.java  \n",
            "  inflating: stanford-ner-2018-10-16/stanford-ner-3.9.2-javadoc.jar  \n",
            "  inflating: stanford-ner-2018-10-16/ner.bat  \n",
            "   creating: stanford-ner-2018-10-16/classifiers/\n",
            "  inflating: stanford-ner-2018-10-16/classifiers/english.conll.4class.distsim.prop  \n",
            "  inflating: stanford-ner-2018-10-16/classifiers/example.serialized.ncc.ncc.ser.gz  \n",
            "  inflating: stanford-ner-2018-10-16/classifiers/english.muc.7class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2018-10-16/classifiers/english.conll.4class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2018-10-16/classifiers/english.muc.7class.distsim.prop  \n",
            "  inflating: stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.prop  \n",
            "  inflating: stanford-ner-2018-10-16/classifiers/example.serialized.ncc.prop  \n",
            "  inflating: stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2018-10-16/sample.txt  \n",
            "  inflating: stanford-ner-2018-10-16/sample-w-time.txt  \n",
            "  inflating: stanford-ner-2018-10-16/ner-gui.sh  \n",
            "  inflating: stanford-ner-2018-10-16/LICENSE.txt  \n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
            "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
            "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
            "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF1JDXtKiptZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "ea4bfd06-304e-47e4-a019-bb9db6575552"
      },
      "source": [
        "# Install stanza; note that the prefix \"!\" is not needed if you are running in a terminal\n",
        "!pip install stanza\n",
        "\n",
        "# Import stanza\n",
        "import stanza\n",
        "corenlp_dir = './corenlp'\n",
        "stanza.install_corenlp(dir=corenlp_dir)\n",
        "\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir\n",
        "from stanza.server import CoreNLPClient"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/ae/a70a58ce6b4e2daad538688806ee0f238dbe601954582a74ea57cde6c532/stanza-1.2-py3-none-any.whl (282kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 10.7MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 16.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 51kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 102kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 112kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 122kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 143kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 153kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 163kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 174kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 184kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 194kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 204kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 215kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 225kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 235kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 245kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 256kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 266kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 276kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (54.2.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-18 12:26:13 INFO: Installing CoreNLP package into ./corenlp...\n",
            "Downloading http://nlp.stanford.edu/software/stanford-corenlp-latest.zip:  73%|███████▎  | 370M/505M [01:10<00:25, 5.24MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fab4e7f2fb6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcorenlp_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./corenlp'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall_corenlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorenlp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Set the CORENLP_HOME environment variable to point to the installation location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stanza/resources/installation.py\u001b[0m in \u001b[0;36minstall_corenlp\u001b[0;34m(dir, url, logging_level)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Download to destination file: {os.path.join(dir, 'corenlp.zip')}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mrequest_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'stanford-corenlp-latest.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'corenlp.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stanza/resources/common.py\u001b[0m in \u001b[0;36mrequest_file\u001b[0;34m(url, path, md5)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'File exists: {path}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stanza/resources/common.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(url, path)\u001b[0m\n\u001b[1;32m    116\u001b[0m         with tqdm(total=file_size, unit='B', unit_scale=True, \\\n\u001b[1;32m    117\u001b[0m             disable=not verbose, desc=desc) as pbar:\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_chunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9VWqbx1is0D"
      },
      "source": [
        "# Setup standford could\n",
        "client = CoreNLPClient(annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner'], \n",
        "                    memory='4G', endpoint='http://localhost:9003', be_quiet=True)\n",
        "import time; time.sleep(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-vuxQzrF4Dk"
      },
      "source": [
        "def sendCSVFile(csv_dataframe):\n",
        "    mydb = mysql.connector.connect(host=\"35.232.96.245\", user=\"root\", passwd=\"DH0Gks1plMpCG7Ni\", database=\"csv_output\")\n",
        "    mycursor = mydb.cursor()\n",
        "\n",
        "    csv_dataframe = csv_dataframe.drop(['Init_index'], axis=1)\n",
        "\n",
        "    sql_create_table = '''\n",
        "                      CREATE TABLE resume_analyze_output\n",
        "                      (Name TEXT(1000),Organization TEXT(1000),Position TEXT(1000),\n",
        "                      Email TEXT(1000),Phone TEXT(1000),Skill TEXT(1000),\n",
        "                      SchoolName TEXT(1000),SchoolDegree TEXT(1000),SchoolDate TEXT(1000),\n",
        "                      Qualification TEXT(1000))\n",
        "                      '''\n",
        "    mycursor.execute(\"DROP TABLE IF EXISTS csv_output.resume_analyze_output\")\n",
        "    mycursor.execute(sql_create_table)\n",
        "    \n",
        "    for row, column in csv_dataframe.iterrows():\n",
        "      mycursor.execute('''\n",
        "                INSERT INTO csv_output.resume_analyze_output \n",
        "                (Name, Organization, Position, Email, Phone, \n",
        "                Skill, SchoolName, SchoolDegree, \n",
        "                SchoolDate, Qualification)\n",
        "                VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
        "                ''',\n",
        "                (column['name'],\n",
        "                 column.organization,\n",
        "                 column.position,\n",
        "                 column.email,\n",
        "                 column.phone,\n",
        "                 column.skill,\n",
        "                 column.SchoolName,\n",
        "                 column.SchoolDegree,\n",
        "                 column.SchoolDate,\n",
        "                 column.Qualifications))\n",
        "    mydb.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdv6gc0-zKHM"
      },
      "source": [
        "def convertPDF2JEG(directory):\n",
        "  import os, shutil\n",
        "  outputpathpre = \"/content/drive/MyDrive/ResumeFirstDir\"\n",
        "  outputpathpost = \"/content/drive/MyDrive/ECS 193A ResumeData/testingJPG\"\n",
        "\n",
        "  containPDF = False\n",
        "\n",
        "  dir = outputpathpre\n",
        "  for files in os.listdir(dir):\n",
        "    path = os.path.join(dir, files)\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "    except OSError:\n",
        "        os.remove(path)\n",
        "  \n",
        "  dir = outputpathpost\n",
        "  for files in os.listdir(dir):\n",
        "    path = os.path.join(dir, files)\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "    except OSError:\n",
        "        os.remove(path)\n",
        "\n",
        "  # Convert pdf to jpg and place them into separate directories each\n",
        "  for filename in os.listdir(directory):\n",
        "    if  filename.endswith(\".pdf\"):\n",
        "      containPDF = True\n",
        "      pdf2jpg.convert_pdf2jpg(os.path.join(directory, filename), outputpathpre, dpi=300, pages=\"ALL\")\n",
        "\n",
        "  # Place jpg from subdirectories into one singular directory\n",
        "  for subdirectory in os.listdir(outputpathpre):\n",
        "    for filename in os.listdir(os.path.join(outputpathpre, subdirectory)):\n",
        "      shutil.move(os.path.join(outputpathpre, subdirectory, filename), outputpathpost)\n",
        "  \n",
        "  return containPDF\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FTdN0mdHYEG"
      },
      "source": [
        "def processFileFormat(directory):\n",
        "  if convertPDF2JEG(directory):\n",
        "    directory = \"/content/drive/MyDrive/ECS 193A ResumeData/testingJPG\"\n",
        "\n",
        "  return directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb4_fNjyc7cp"
      },
      "source": [
        "# SpecialStar\n",
        "# Process the resume into relevant text informations\n",
        "# Input: Directory of resumes\n",
        "# Output: the name of the text files\n",
        "def preprocess_resume(directory, debug=False):\n",
        "    import os\n",
        "    import re, string\n",
        "    import copy\n",
        "\n",
        "    print(\"Processing \" + directory)\n",
        "\n",
        "    outputtxt = \"/content/BulkTxtOutput.txt\"\n",
        "    outputtxt_org = \"/content/BulkTxtOutput_org.txt\"\n",
        "\n",
        "    # Set up file clean up\n",
        "    newline = re.compile(\"[\" + \"\\n\" + \"]\")\n",
        "    pattern = re.compile(\"[\" + re.escape(string.punctuation) + \"]\")\n",
        "    afterPattern = re.compile(\"•\")\n",
        "    text_orgPattern = re.compile(\"\\\"\")\n",
        "\n",
        "    text_file = open(outputtxt, \"w\")\n",
        "    text_file_org = open(outputtxt_org, \"w\")\n",
        "    data = pandas.DataFrame(columns=['Sentence', 'Location'])\n",
        "\n",
        "    # Start by writing the column name into textfile\n",
        "    text_file.write('Sentence' + '\\n')\n",
        "    text_file_org.write('Sentence' + '\\n')\n",
        "\n",
        "    # Counter for the first x resume we want to process\n",
        "    i = 0\n",
        "    x = 10\n",
        "\n",
        "    #save process file name to csv\n",
        "    filenameList = [[]]\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "      if debug==True and i > x:\n",
        "        break\n",
        "\n",
        "      if not filename.endswith(\".pdf\"):\n",
        "        print(\"\\n\" + \"Currently processing: \" + filename)\n",
        "        filenameList += [filename]\n",
        "        texts, sentence_location_perResume, df = detect_text(os.path.join(directory, filename))\n",
        "        df['Cleaned Sentence'] = df['Sentence']\n",
        "\n",
        "        for row in range(len(df['Cleaned Sentence'])):\n",
        "          if df['Cleaned Sentence'][row] != \"--------------------------------------------------\\n\" :\n",
        "            df['Cleaned Sentence'][row] = pattern.sub(' ', df['Cleaned Sentence'][row])\n",
        "        for row in range(len(df['Sentence'])):\n",
        "          df['Sentence'][row] = text_orgPattern.sub(' ', df['Sentence'][row])\n",
        "\n",
        "        clean_sentence = '\\n'.join(df['Cleaned Sentence'].tolist())\n",
        "        text_file.write(clean_sentence)\n",
        "\n",
        "        sentence = '\\n'.join(df['Sentence'].tolist())\n",
        "        text_file_org.write(sentence)\n",
        "\n",
        "        data = data.append(df, ignore_index=True)\n",
        "\n",
        "        #if (debug == True):\n",
        "        #  print(text)\n",
        "        #  print(type(text))\n",
        "        #  print(text_org)\n",
        "        #  print(type(text_org))\n",
        "        #  print()\n",
        "\n",
        "        print(\"Finished Resume number: \" + str(i) + \"--------------------------------------------------\" + \"\\n\")\n",
        "        i+=1\n",
        "\n",
        "    text_file.close()\n",
        "    text_file_org.close()\n",
        "\n",
        "    data = {'filename':filenameList}\n",
        "    filenameDF = pd.DataFrame(data)\n",
        "    filenameDF.to_csv('/content/fileorder.csv')\n",
        "\n",
        "    return outputtxt, outputtxt_org"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5AEcohXZb0H"
      },
      "source": [
        "# SpecialStar\n",
        "# Running Vision API for parsing resume image into text file\n",
        "# Input: path of the resume image\n",
        "# Output: the converted text string\n",
        "def detect_text(path):\n",
        "    \"\"\"Detects text in the file.\"\"\"\n",
        "    from google.cloud import vision\n",
        "    import io\n",
        "    import os\n",
        "\n",
        "    from enum import Enum\n",
        "    from PIL import Image\n",
        "    im = Image.open(path)\n",
        "    width, height = im.size\n",
        "    # print(width)\n",
        "    \n",
        "\n",
        "    \n",
        "    class FeatureType(Enum):\n",
        "        PAGE = 1\n",
        "        BLOCK = 2\n",
        "        PARA = 3\n",
        "        WORD = 4\n",
        "        SYMBOL = 5\n",
        "\n",
        "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/Resume_Analyze-4f918029480e.json\"\n",
        "    \n",
        "    client = vision.ImageAnnotatorClient()\n",
        "\n",
        "    with io.open(path, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    response = client.document_text_detection(image=image)\n",
        "    texts = response.full_text_annotation\n",
        "\n",
        "    first_line_x = 0\n",
        "\n",
        "    for page in texts.pages:\n",
        "        for block in page.blocks:\n",
        "            if block.bounding_box.vertices[0].x < width/2:\n",
        "                first_line_x = block.bounding_box.vertices[0].x\n",
        "                break\n",
        "\n",
        "    i = 0   # Counting number of blocks\n",
        "    x1, x2, y1, y2 = 9999, 0, 9999, 0   # the sentence's coordinates\n",
        "    breaks = vision.TextAnnotation.DetectedBreak.BreakType\n",
        "    bounds = [[[]]]\n",
        "    sentence_location = []\n",
        "\n",
        "    # Initialization\n",
        "    bounds.append([])\n",
        "    bounds[0].append([])\n",
        "\n",
        "    num_sentence = 0\n",
        "\n",
        "    for page in texts.pages:\n",
        "        for block in page.blocks:\n",
        "            s = 0   # Counting number of sentence\n",
        "            if abs(block.bounding_box.vertices[0].x - first_line_x) < width/4:\n",
        "                  for paragraph in block.paragraphs:\n",
        "                      for word in paragraph.words:\n",
        "                          for symbol in word.symbols: # Beginning of word\n",
        "                              pending_x1 = symbol.bounding_box.vertices[0].x\n",
        "                              pending_y1 = symbol.bounding_box.vertices[0].y\n",
        "                              pending_x2 = symbol.bounding_box.vertices[3].x\n",
        "                              pending_y2 = symbol.bounding_box.vertices[3].y\n",
        "\n",
        "                              x1 = pending_x1 if x1 > pending_x1 else x1\n",
        "                              y1 = pending_y1 if y1 > pending_y1 else y1\n",
        "                              x2 = pending_x2 if x2 < pending_x2 else x2\n",
        "                              y2 = pending_y2 if y2 < pending_y2 else y2\n",
        "\n",
        "                              if symbol.text == '•':\n",
        "                                bounds[i][s] = \"\".join(bounds[i][s]) \n",
        "                                if (bounds[i][s] != '.'):\n",
        "                                    sentence_location.append((x1, y1, x2, y2))\n",
        "                                bounds[i].append([])  # append new sentence\n",
        "                                s+=1\n",
        "                                x1, x2, y1, y2 = 9999, 0, 9999, 0   # reset the coordinates\n",
        "                                continue\n",
        "                              else: \n",
        "                                bounds[i][s]+=symbol.text\n",
        "                              \n",
        "                              if symbol.property.detected_break.type_ == breaks.SPACE:\n",
        "                                bounds[i][s]+=' '  # End of a word\n",
        "                              elif symbol.property.detected_break.type_ == breaks.EOL_SURE_SPACE or symbol.property.detected_break.type_ == breaks.LINE_BREAK:\n",
        "                                bounds[i][s] = \"\".join(bounds[i][s]) \n",
        "                                if (bounds[i][s] != '.'):\n",
        "                                    sentence_location.append((x1, y1, x2, y2))\n",
        "                                bounds[i].append([])  # append new sentence\n",
        "                                s+=1\n",
        "                                x1, x2, y1, y2 = 9999, 0, 9999, 0   # reset the coordinates\n",
        "                      bounds[i].append([])  # append new sentence\n",
        "                  i+=1\n",
        "                  bounds.append([])\n",
        "                  bounds[i].append([]) # For each sentence, not each paragraphs\n",
        "    \n",
        "    for page in texts.pages:\n",
        "        for block in page.blocks:\n",
        "            s = 0   # Counting number of sentence\n",
        "            if abs(block.bounding_box.vertices[0].x - first_line_x) >= width/4:\n",
        "                  for paragraph in block.paragraphs:\n",
        "                      for word in paragraph.words:\n",
        "                          for symbol in word.symbols: # Beginning of word\n",
        "                              pending_x1 = symbol.bounding_box.vertices[0].x\n",
        "                              pending_y1 = symbol.bounding_box.vertices[0].y\n",
        "                              pending_x2 = symbol.bounding_box.vertices[3].x\n",
        "                              pending_y2 = symbol.bounding_box.vertices[3].y\n",
        "\n",
        "                              x1 = pending_x1 if x1 > pending_x1 else x1\n",
        "                              y1 = pending_y1 if y1 > pending_y1 else y1\n",
        "                              x2 = pending_x2 if x2 < pending_x2 else x2\n",
        "                              y2 = pending_y2 if y2 < pending_y2 else y2\n",
        "\n",
        "                              if symbol.text == '•':\n",
        "                                bounds[i][s] = \"\".join(bounds[i][s]) \n",
        "                                if (bounds[i][s] != '.'):\n",
        "                                    sentence_location.append((x1, y1, x2, y2))\n",
        "                                bounds[i].append([])  # append new sentence\n",
        "                                s+=1\n",
        "                                x1, x2, y1, y2 = 9999, 0, 9999, 0   # reset the coordinates\n",
        "                                continue\n",
        "                              else: \n",
        "                                bounds[i][s]+=symbol.text\n",
        "                              \n",
        "                              if symbol.property.detected_break.type_ == breaks.SPACE:\n",
        "                                bounds[i][s]+=' '  # End of a word\n",
        "                              elif symbol.property.detected_break.type_ == breaks.EOL_SURE_SPACE or symbol.property.detected_break.type_ == breaks.LINE_BREAK:\n",
        "                                bounds[i][s] = \"\".join(bounds[i][s]) \n",
        "                                if (bounds[i][s] != '.'):\n",
        "                                    sentence_location.append((x1, y1, x2, y2))\n",
        "                                bounds[i].append([])  # append new sentence\n",
        "                                s+=1\n",
        "                                x1, x2, y1, y2 = 9999, 0, 9999, 0   # reset the coordinates\n",
        "                      bounds[i].append([])  # append new sentence\n",
        "                  i+=1\n",
        "                  bounds.append([])\n",
        "                  bounds[i].append([]) # For each sentence, not each paragraphs\n",
        "\n",
        "\n",
        "    data = pandas.DataFrame(columns=['Sentence', 'Location'])\n",
        "\n",
        "    # print(bounds)\n",
        "\n",
        "    tidy_text = \"\"\n",
        "    s = 0\n",
        "    for bound in bounds:\n",
        "        for sentence in bound:\n",
        "            if not sentence or sentence == '.':\n",
        "              continue\n",
        "            if (isinstance(sentence,list) == True):\n",
        "              tidy_text = tidy_text + \"\".join(sentence) + '\\n'\n",
        "              sentence_location.insert(s, (-1,-1,-1,-1))    # If there are any out of handle line, simply insert invalid\n",
        "              data = data.append({'Sentence' : \"\".join(sentence), 'Location' : (-1,-1,-1,-1)}, ignore_index=True)\n",
        "            else:\n",
        "              tidy_text = tidy_text + sentence + '\\n'\n",
        "              data = data.append({'Sentence' : sentence, 'Location' : sentence_location[s]}, ignore_index=True)\n",
        "            s+=1\n",
        "    sentence_location.append((0,0,0,0))   # Appending for end of resume indicator\n",
        "    data = data.append({'Sentence' : \"--------------------------------------------------\\n\", 'Location' : (0,0,0,0)}, ignore_index=True)\n",
        "    \n",
        "    return tidy_text, sentence_location, data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJm7myQTUlS8"
      },
      "source": [
        "# Reference from https://github.com/shabeelkandi/Handling-Out-of-Vocabulary-Words-in-Natural-Language-Processing-using-Language-Modelling/blob/master/RNN%20LSTM%20Model.ipynb\n",
        "\n",
        "# generate a sequence using a language model\n",
        "# Input: the model being used (rev or forward), tokenizer file, max_length of word, and seed of the text\n",
        "# Output: list of predicted words\n",
        "def generate_seq(model, tokenizer, max_length, seed_text):\n",
        "    if seed_text == \"\":\n",
        "        return \"\"\n",
        "    else:\n",
        "        in_text = seed_text\n",
        "        n_words = 1\n",
        "        n_preds = 5\n",
        "        pred_words = \"\"\n",
        "        # generate a fixed number of words\n",
        "        for _ in range(n_words):\n",
        "            # encode the text as integer\n",
        "            encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "            # pre-pad sequences to a fixed length\n",
        "            encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "            # predict probabilities for each word\n",
        "            proba = model.predict(encoded, verbose=0).flatten()\n",
        "            #take the n_preds highest probability classes \n",
        "            yhat = numpy.argsort(-proba)[:n_preds] \n",
        "            # map predicted words index to word\n",
        "            out_word = ''\n",
        "\n",
        "            for _ in range(n_preds):\n",
        "                for word, index in tokenizer.word_index.items():\n",
        "                    if index == yhat[_]:\n",
        "                        out_word = word\n",
        "                        pred_words += ' ' + out_word\n",
        "                        break\n",
        "\n",
        "        return pred_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPvMO8I_TU-T"
      },
      "source": [
        "# Reference from https://github.com/shabeelkandi/Handling-Out-of-Vocabulary-Words-in-Natural-Language-Processing-using-Language-Modelling/blob/master/RNN%20LSTM%20Model.ipynb\n",
        "\n",
        "# Create the network and reverse network\n",
        "# Input: Lines of resume in text form\n",
        "# Output: The forwarding and rev word embedding model\n",
        "def word_embedding(data):\n",
        "  X,y,rev_X,rev_y,max_length,vocab_size = data_spliting(data)\n",
        "\n",
        "  # define forward sequence model\n",
        "  model = Sequential()\n",
        "  # Can specific the output_dim here for embedding\n",
        "  model.add(Embedding(vocab_size,100, input_length=max_length-1))\n",
        "  model.add(Bidirectional(LSTM(100)))\n",
        "  model.add(Dense(vocab_size, activation='softmax'))\n",
        "  print(model.summary())\n",
        "\n",
        "  # compile, fit, and save forward sequence network\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.fit(X, y,batch_size=100, epochs=100, verbose=2)\n",
        "  model.save('/content/drive/MyDrive/ECS 193A ResumeData/Model & Rev_Model/WE_model.h5')\n",
        "\n",
        "  # define reverse model\n",
        "  rev_model = Sequential()\n",
        "  rev_model.add(Embedding(vocab_size, 100, input_length=max_length-1))\n",
        "  rev_model.add(Bidirectional(LSTM(100)))\n",
        "  rev_model.add(Dense(vocab_size, activation='softmax'))\n",
        "  print(rev_model.summary())\n",
        "\n",
        "  # compile, fit, and save reverse sequence network\n",
        "  rev_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  rev_model.fit(rev_X, rev_y,batch_size=100, epochs=100, verbose=2)\n",
        "  rev_model.save('/content/drive/MyDrive/ECS 193A ResumeData/Model & Rev_Model/WE_rev_model.h5')\n",
        "\n",
        "  return model, rev_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fb8wRi8aqFX"
      },
      "source": [
        "# SpecialStar\n",
        "# Find and set embeddings for OOV words\n",
        "# Input: the interested nlp string\n",
        "# Output: None -- function set OOV to the global nlp vector\n",
        "def set_embedding_for_oov(doc):\n",
        "    #checking for oov words and adding embedding\n",
        "    #Can improve accuracy performance by using the weight from embedding layer. \n",
        "    # However, still doesn't mean it will update the embedding layer\n",
        "    for token in doc:\n",
        "        if token.is_oov == True:\n",
        "            before_text = doc[:token.i].text\n",
        "            after_text = str(array(doc)[:token.i:-1]).replace('[','').replace(']','')\n",
        "\n",
        "            pred_before = generate_seq(model, tokenizer, max_length-1, before_text).split()\n",
        "            pred_after = generate_seq(rev_model, tokenizer, max_length-1, after_text).split()\n",
        "            \n",
        "            # Change embedding array size\n",
        "            # spacy en_core_web_md uses 300 dimension. \n",
        "            # Even if the sentence max length becomes 40. We believe the max length of \n",
        "            embedding = numpy.zeros((300,))\n",
        "\n",
        "            i=len(before_text)\n",
        "            for word in pred_before:\n",
        "                #print(word)\n",
        "                embedding += i*nlp.vocab.get_vector(word)\n",
        "                i= i*.5\n",
        "            i=len(after_text)\n",
        "            for word in pred_after:\n",
        "                embedding += i*nlp.vocab.get_vector(word)\n",
        "                i= i*.25\n",
        "            nlp.vocab.set_vector(token.text, embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtQPhWGTbnTr"
      },
      "source": [
        "#function to find most similar words\n",
        "def most_similar(word):\n",
        "    by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)\n",
        "    return [w.orth_ for w in by_similarity[:10]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiRFKtqKQnqA"
      },
      "source": [
        "# SpecialStar\n",
        "# Convert a line of words into array of vectors\n",
        "# Input: the string of text\n",
        "# Output: vectorized text string (size=300)\n",
        "def line2vec(line, maxlength):\n",
        "  import itertools\n",
        "\n",
        "  separator = ' '\n",
        "  # Make sure the embedding learn all the oov\n",
        "  set_embedding_for_oov(nlp(separator.join(line)))\n",
        "  # convert words into vectors\n",
        "  vectors = [nlp.vocab.get_vector(word) for word in line]\n",
        "  # join all vectors into one vector\n",
        "  big_vector = itertools.chain(vectors)\n",
        "  big_vector = pad_sequences(list(big_vector), maxlen=maxlength, dtype='float32', padding='post')\n",
        "  #big_vector = pad_sequences([[j for i in vectors for j in i]], maxlen=maxlength, dtype='float32', padding='post')\n",
        "  #big_vector = pad_sequences([big_vector], maxlen=maxlength, dtype='float32', padding='post')\n",
        "\n",
        "  return big_vector[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeoAd-fmIZCF"
      },
      "source": [
        "# Using the graded csv, build the binary classifier and train it\n",
        "# Input: the csv with graded 1/0 classifier\n",
        "# Output: the HC_model\n",
        "def header_classifier(data):\n",
        "  print('Spliting given csv...')\n",
        "  X_train, X_test, y_train, y_test, max_length = data_spliting_header_classification(data)\n",
        "  X = X_train.to_numpy().tolist()\n",
        "  for row in range (len(X)):\n",
        "    X[row] = X[row].tolist()\n",
        "  \n",
        "  Y = y_train.to_numpy().tolist()\n",
        "\n",
        "  # Model takes in \n",
        "  model = Sequential()\n",
        "  # Try 'relu' for now. We can also try 'softmax', or all 'sigmoid'\n",
        "  model.add(Dense(int(2 * max_length/3), input_dim=max_length, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  print(model.summary())\n",
        "\n",
        "  # compile, fit, and save forward sequence network\n",
        "  print('Compiling HC_model...')\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  print('Fitting HC_model...')\n",
        "  model.fit(X, Y, batch_size=20, epochs=50, verbose=2)\n",
        "  model.save('/content/drive/MyDrive/ECS 193A ResumeData/Model & Rev_Model/HC_model.h5')\n",
        "  #model = load_model('/content/drive/MyDrive/ECS 193A ResumeData/Model & Rev_Model/HC_model.h5')\n",
        "  _, accuracy = model.evaluate(X, Y)\n",
        "  print('Accuracy: %.2f' % (accuracy*100))\n",
        "\n",
        "  # Testing the model\n",
        "  # Also, remember to concat the prediction of the classifier to the original csv\n",
        "  header_classifier_testing(model, X_test, y_test)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izFitQrgMvWF"
      },
      "source": [
        "# SpecialStar\n",
        "# Error adjustment\n",
        "# Input: data=pandas dataset of all the sentences; input=X; predictions=yhat\n",
        "# Output: error corrected predictions\n",
        "def HC_error_adjustment(data, input, predictions):\n",
        "  header_list = ['academic', 'experience', 'education', 'awards', 'award', 'courses', 'voluteer', 'qualification', 'qualifications',\n",
        "                 'professional', 'work', 'working', 'employment', 'internship', 'business', 'activities', 'project', 'projects', \n",
        "                 'intern', 'languages', 'skills', 'skill', 'fluent', 'summary', 'objective', 'eductio', 'personal', 'profile',\n",
        "                 'interest', 'personal', 'email', 'phone', 'milestones', 'linkedin', 'twitter', 'expertise', 'contact', \n",
        "                 'service', 'services', 'career', 'hobbies', 'coursework', 'traits', 'address', 'certificates', 'about', 'tel',\n",
        "                 'university', 'organizations', 'organization', 'expert', 'certifications']\n",
        "  header_black_list = ['leadership', 'speak', 'communities', 'enterprisse', 'administration', 'director', 'design', 'manager', \n",
        "                       'teacher', 'marketing', 'ethic', 'screenwriting', 'cooperative', 'excellence', 'kuo', 'writing', 'association',\n",
        "                       'practices']\n",
        "\n",
        "  for row in range(len(input)):\n",
        "    if isinstance(data.at[row, 'Sentence'],float):\n",
        "      continue\n",
        "    \n",
        "    string = data.at[row, 'Sentence'].replace(':', '')\n",
        "    sentence = string.lower().split(' ')\n",
        "    for item in header_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 1\n",
        "        break\n",
        "    \n",
        "    for item in header_black_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 0\n",
        "        break\n",
        "\n",
        "    #sentence = data.at[row, 'Sentence'].lower().split(' ')\n",
        "    #for item in header_list:\n",
        "    #  if item == sentence[0]:\n",
        "    #    predictions[row] = 1\n",
        "    #    continue\n",
        "\n",
        "    if sentence[0] != '' and string.split(' ')[0][0].islower():\n",
        "      predictions[row] = 0\n",
        "    \n",
        "    if (len(sentence) > 5):\n",
        "      predictions[row] = 0\n",
        "    if (row == 0 \n",
        "      or data.at[row-1, 'Sentence'] == '--------------------------------------------------'):\n",
        "      predictions[row] = 1\n",
        "\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycIWzCVSjH3w"
      },
      "source": [
        "# SpecialStar\n",
        "# Perpare for pulling all headers out and able to train on all papers (on BulkTxtOutput)\n",
        "#   Program can pull all headers out, and put them into another csv\n",
        "# Input: The resume in text file\n",
        "# Output: file path of the csv file with headers marked\n",
        "def get_headers(BulkTxtOutput, model=None):\n",
        "  HC_model = 0\n",
        "  if (model==None):\n",
        "    HC_model = load_model('/content/drive/MyDrive/ECS 193A ResumeData/Model & Rev_Model/HC_model.h5')\n",
        "  else:\n",
        "    HC_model = load_model(model)\n",
        "\n",
        "  csv_file = '/content/placeholding.csv'\n",
        "  max_length = 300*5\n",
        "  output_file = '/content/HC_output.csv'\n",
        "\n",
        "  print('Loading BulkTxtFile...')\n",
        "  # First, load text file to csv\n",
        "  #read_file = pandas.read_csv (BulkTxtOutput)\n",
        "  read_file = pandas.read_csv (BulkTxtOutput, sep=\"\\n\")\n",
        "  read_file.to_csv (csv_file)\n",
        "\n",
        "  print('Loading csv into dataframe...')\n",
        "  # Now, load csv to pandas (because this is the way I know how to do this)\n",
        "  data = pandas.read_csv(csv_file)\n",
        "  data = data.drop(data.columns[0], axis=1)\n",
        "  X = data.copy()\n",
        "\n",
        "  print('Converting words into vectors...')\n",
        "  for row in range(len(X['Sentence'])):\n",
        "    if not isinstance(X['Sentence'][row],float):\n",
        "      line = X['Sentence'][row].split()\n",
        "    if len(line) > 5:\n",
        "      line = line[:5]\n",
        "    big_vector = line2vec(line, max_length)\n",
        "    X['Sentence'][row] = numpy.asarray(big_vector).astype('float32')\n",
        "\n",
        "  print('Done!')\n",
        "  #print(X.head(5))\n",
        "\n",
        "  print('Perparing inputs for prediction model...')\n",
        "  input = X['Sentence'].to_numpy().tolist()\n",
        "  for row in range (len(input)):\n",
        "    input[row] = input[row].tolist()\n",
        "\n",
        "  #print(input)\n",
        "  #return data, input\n",
        "  \n",
        "  print('Predicting...')\n",
        "  predictions = (HC_model.predict_classes(input)).T[0]\n",
        "\n",
        "  # Pick up missed traits\n",
        "  predictions = HC_error_adjustment(data, input, predictions)\n",
        "  \n",
        "  print('Placing predictions into csv')\n",
        "  output = pandas.DataFrame(columns=['Sentence'])\n",
        "  # For every predictions[i] = 1, place the sentence onto the \n",
        "  for i in range (len(predictions)):\n",
        "    if (predictions[i]):\n",
        "      output.at[i, 'Sentence'] = data.at[i, 'Sentence']\n",
        "  \n",
        "  output.to_csv(output_file, index=True)\n",
        "  print(output)\n",
        "\n",
        "  return output_file\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tGITT2iPs13"
      },
      "source": [
        "# One_hot encoding\n",
        "# Input: the classified output needed to be encode\n",
        "# Output: One hot encoded output in numpy array\n",
        "def one_hot_encoding(y):\n",
        "  one_hot_y = np_utils.to_categorical(y)\n",
        "  return one_hot_y.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpDAz2oSPji5"
      },
      "source": [
        "# Read in the output_train.csv and output the vectorized+one hotted pandas\n",
        "# Input: The csv file with all the label classes marked\n",
        "# Output: The X, Y of trainig and testing set; maxlength=the length of longest word\n",
        "def data_spliting_label_classification(csv_file):\n",
        "  data = pandas.read_csv(csv_file)\n",
        "  # data = pandas.read_csv (csv_file, sep=\"\\n\")\n",
        "  data = data.drop(data.columns[0], axis=1)\n",
        "  print('Retrieved csv file into pandas dataframe')\n",
        "  max_length = 300*5\n",
        "  print('Converting sentence words into vectors...')\n",
        "  for row in range(len(data['Sentence'])):\n",
        "    line = data['Sentence'][row].split()\n",
        "    if len(line) > 5:\n",
        "      line = line[:5]\n",
        "    big_vector = line2vec(line, max_length)\n",
        "    data['Sentence'][row] = numpy.asarray(big_vector).astype('float32')\n",
        "\n",
        "  # encoder is the LabelEncoder for transforming back to labels later on\n",
        "  print('Converting labels into one_hot variables')\n",
        "  data['Label'] = one_hot_encoding(data['Label'])\n",
        "\n",
        "  # Note: we can change random_state to set seed\n",
        "  print('Spliting data into training and testing set...')\n",
        "  X_train, X_test, y_train, y_test = train_test_split(data['Sentence'], data['Label'], test_size=0.1)\n",
        "  return X_train, X_test, y_train, y_test, max_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWwJDoshZstH"
      },
      "source": [
        "# Take in training set with labels, outputing model\n",
        "# Input: The training set with labels (in csv file path)\n",
        "# Output: The LC_model\n",
        "def label_classifier(data):\n",
        "  print('Spliting given csv...')\n",
        "  X_train, X_test, y_train, y_test, max_length = data_spliting_label_classification(data)\n",
        "  #X_train, y_train, max_length = data_spliting_label_classification(data)\n",
        "  X = X_train.to_numpy().tolist()\n",
        "  for row in range (len(X)):\n",
        "    X[row] = X[row].tolist()\n",
        "  X = numpy.asarray(X)\n",
        "  Y = numpy.asarray(y_train.to_numpy().tolist())\n",
        "\n",
        "  def LC_model():\n",
        "    max_length = 1500\n",
        "    # create model\n",
        "    print('Compiling LC_model...')\n",
        "    model = Sequential()\n",
        "    # Try 'relu' for now. We can also try 'softmax', or all 'sigmoid'\n",
        "    model.add(Dense(int(2 * max_length/3), input_dim=max_length, activation='relu'))\n",
        "    model.add(Dense(6, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "  # compile, fit, and save forward sequence network\n",
        "  print('Fitting LC_model...')\n",
        "  model = KerasClassifier(build_fn=LC_model, epochs=100, batch_size=20, verbose=2)\n",
        "\n",
        "  model.fit(X, Y)\n",
        "\n",
        "  #model.save\n",
        "  model.model.save('/content/drive/MyDrive/ECS 193A ResumeData/Model & Rev_Model/LC_model.h5')\n",
        "  \n",
        "  # Testing the model\n",
        "  # Also, remember to concat the prediction of the classifier to the original csv\n",
        "  #header_classifier_testing(model, X_test, y_test)\n",
        "\n",
        "  return model.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm9ObPS9VX2x"
      },
      "source": [
        "def isPhoneRow(text):\n",
        "  import re, string\n",
        "  pattern = re.compile(\"[\" + re.escape(string.punctuation) + ' ' + \"]\")\n",
        "\n",
        "  if isinstance(text, float) or '/' in text:\n",
        "    return False\n",
        "\n",
        "  sentence = pattern.sub('', text)\n",
        "\n",
        "  if len(sentence) > 9 and sentence.isdecimal():\n",
        "    return True\n",
        "\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZCQUPv4VYBq"
      },
      "source": [
        "def isWebsiteRow(text):\n",
        "  if isinstance(text, float):\n",
        "    return False\n",
        "\n",
        "  email_list = ['com', 'gmail']\n",
        "  \n",
        "  sentence = text.lower().split(' ')\n",
        "  if len(sentence) > 8:\n",
        "    return False\n",
        "\n",
        "  for item in email_list:\n",
        "    if item in sentence:\n",
        "      return True\n",
        "        \n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yRHse40VYIN"
      },
      "source": [
        "def isNameRow(text):\n",
        "  if extract_name_new(text) != None:\n",
        "    return True\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VYrxIws0o1M"
      },
      "source": [
        "# SpecialStar\n",
        "# error adjustment\n",
        "# Input: data=pandas dataset of all the sentences; input=X; predictions=yhat\n",
        "# Output: error corrected predictions\n",
        "def LC_error_adjustment(data, input, predictions):\n",
        "  label1_list = ['academic', 'eductio', 'award', 'awards', 'education', 'coursework', 'courseworks', \n",
        "                 'qualification', 'qualifications', 'courses', 'course', 'university', 'certification']\n",
        "  label2_list = ['professional', 'work', 'working', 'employment', 'internship', 'business', 'activities', 'project', 'intern', 'projects', \n",
        "                 'milestones', 'volunteer', 'service', 'services', 'experience', 'career', 'summary', 'objective', 'me', 'organization'\n",
        "                 , 'organizations']\n",
        "  label3_list = ['languages', 'skills', 'skill', 'fluent', 'interest', 'expertise', 'hobbies', 'traits', 'proficiency', 'expert']\n",
        "  label4_list = ['profile', 'personal', 'email', 'phone', 'linkedin', 'twitter', 'contact', 'address', 'tel']\n",
        "  label5_list = ['reference']\n",
        "\n",
        "  for row in range(len(input)):\n",
        "    if (row == 0 \n",
        "      or data.at[row-1, 'Sentence'] == '--------------------------------------------------'):\n",
        "      predictions[row] = 4\n",
        "      continue\n",
        "\n",
        "    string = data.at[row, 'Sentence'].replace(':', '')\n",
        "    sentence = string.lower().split(' ')\n",
        "    for item in label2_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 2\n",
        "        break\n",
        "    \n",
        "    for item in label3_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 3\n",
        "        break\n",
        "    \n",
        "    for item in label1_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 1\n",
        "        break\n",
        "    \n",
        "    for item in label4_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 4\n",
        "        break\n",
        "    \n",
        "    #if isWebsiteRow(string) or isPhoneRow(string):\n",
        "    #  print('@ LC_error_adjustment website, or phone detected at line:', row)\n",
        "    #  predictions[row] = 4\n",
        "\n",
        "    #if isNameRow(string) or isWebsiteRow(string) or isPhoneRow(string):\n",
        "    #  print('@ LC_error_adjustment name, website, or phone detected at line:', row)\n",
        "    #  predictions[row] = 4\n",
        "\n",
        "    for item in label5_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 5\n",
        "        break\n",
        "  \n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt6e1D3tnhfM"
      },
      "source": [
        "# SpecialStar\n",
        "# Note: labels with 5 could be ignore when labeling whole resumes\n",
        "# Input: csv_file of the with Sentence only is ok\n",
        "def get_label(csv_file, model=None, get_accuracy=False):\n",
        "  LC_model = 0\n",
        "  if (model==None):\n",
        "    LC_model = load_model('/content/drive/MyDrive/ECS 193A ResumeData/Model & Rev_Model/LC_model.h5')\n",
        "  else:\n",
        "    LC_model = load_model(model)\n",
        "  \n",
        "  max_length = 300*5\n",
        "  outputfile = '/content/LC_output.csv'\n",
        "\n",
        "  print('Loading csv into dataframe...')\n",
        "  # Load csv to pandas\n",
        "  data = pandas.read_csv(csv_file)\n",
        "  data.rename(columns={data.columns[0]: \"Init_index\"}, inplace=True)\n",
        "  X = data.copy()\n",
        "\n",
        "  print('Converting words into vectors...')\n",
        "  for row in range(len(X['Sentence'])):\n",
        "    line = X['Sentence'][row].split()\n",
        "    if len(line) > 5:\n",
        "      line = line[:5]\n",
        "    big_vector = line2vec(line, max_length)\n",
        "    X['Sentence'][row] = numpy.asarray(big_vector).astype('float32')\n",
        "\n",
        "  print('Done!')\n",
        "\n",
        "  print('Perparing inputs for prediction model...')\n",
        "  input = X['Sentence'].to_numpy().tolist()\n",
        "  for row in range (len(input)):\n",
        "    input[row] = input[row].tolist()\n",
        "  input = numpy.asarray(input)\n",
        "  \n",
        "  print('Predicting...')\n",
        "  predictions = (LC_model.predict_classes(input))\n",
        "\n",
        "  # Pick up missed traits\n",
        "  predictions = LC_error_adjustment(data, input, predictions)\n",
        "\n",
        "  # Prepare output as copy of data\n",
        "  output = data.copy()\n",
        "\n",
        "  # Append predictions onto output\n",
        "  print('Placing predictions into csv')\n",
        "  output['pred_Label'] = predictions\n",
        "  \n",
        "  # Check accuracy of this prediction\n",
        "  if (get_accuracy == True):\n",
        "    accuracy = 0\n",
        "    correction_check = []\n",
        "    for i in range (len(predictions)):\n",
        "      if (predictions[i] == output.at[i, 'Label']):\n",
        "        accuracy+=1\n",
        "        correction_check.append(1)\n",
        "      else:\n",
        "        correction_check.append(0)\n",
        "    \n",
        "    output['Correction_check'] = correction_check\n",
        "    print('Prediction accuracy: ', str(accuracy/len(predictions)*100))\n",
        "    \n",
        "  output.to_csv(outputfile, index=True)\n",
        "\n",
        "  return outputfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lABOSLDiKIo"
      },
      "source": [
        "# SpecialStar\n",
        "# Input: string\n",
        "# Output: boolean\n",
        "\n",
        "# isSchoolName:\n",
        "#   Parse line into array of strings\n",
        "#   is name if length >= 3 and it have the word:\n",
        "#     'university', 'college', 'programme', 'school' , 'institute', 'chartered' and 'financial' and 'analyst', 'exchange' and 'program', 'polytechnic'\n",
        "\n",
        "def isQualification(words):\n",
        "  #words = sentence.lower().split(' ')\n",
        "  qualifications = ['level', 'levels', 'chartered', 'certified']\n",
        "  if len(words) >= 2:\n",
        "    for item in qualifications:\n",
        "      if item in words:\n",
        "        return True\n",
        "  \n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU0Rdvye4tpu"
      },
      "source": [
        "# SpecialStar\n",
        "# Input: string\n",
        "# Output: boolean\n",
        "\n",
        "# isSchoolName:\n",
        "#   Parse line into array of strings\n",
        "#   is name if length >= 3 and it have the word:\n",
        "#     'university', 'college', 'programme', 'school' , 'institute', 'chartered' and 'financial' and 'analyst', 'exchange' and 'program', 'polytechnic'\n",
        "\n",
        "def isSchoolName(words):\n",
        "  #words = sentence.lower().split(' ')\n",
        "  schoolNames = ['university', 'college', 'school' , 'institute', 'polytechnic', 'cpa', \n",
        "                 'universiti', 'program', 'université']\n",
        "  namePhrase = [['chartered', 'financial', 'analyst']]\n",
        "  schoolNamesBlackList = ['award', 'awarded', 'awards', 'president', 'leader', 'leaders', 'prize', \n",
        "                          'manager', 'middle', 'high']\n",
        "  if len(words) >= 2:\n",
        "    for item in schoolNamesBlackList:\n",
        "      if item in words:\n",
        "        return False\n",
        "\n",
        "    for item in schoolNames:\n",
        "      if item in words:\n",
        "        return True\n",
        "\n",
        "    # Check if each namePhrase is contained in sentence by\n",
        "    #   ensure all strings in phrase also in array words\n",
        "    #   if all strings are in array words, return True\n",
        "    #   else, check the next phrase\n",
        "    #   if exchange all phrases, return false\n",
        "    for item in namePhrase:\n",
        "      flag = True\n",
        "      for subitem in item:\n",
        "        if subitem not in words:\n",
        "          flag = False\n",
        "          break\n",
        "      if flag:\n",
        "        return flag\n",
        "  \n",
        "  return False\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIdEszBd5IH8"
      },
      "source": [
        "# SpecialStar\n",
        "# Input: string\n",
        "# Output: boolean\n",
        "\n",
        "# isSchoolDegree:\n",
        "#   Parse line into array of strings\n",
        "#   is name if length >= 3 and it have the word:\n",
        "#     'bachelor', 'master', 'minor', 'major', 'foundations' and 'program', 'institute', 'ba', 'ms', 'bs', 'bsc', 'chartered', 'b', 'gce', 'level[s]', 'diploma', 'business', 'degree'\n",
        "\n",
        "def isSchoolDegree(words):\n",
        "  #words = sentence.lower().split(' ')\n",
        "  schoolDegrees = ['bachelor', 'master', 'minor', 'major', 'institute', \n",
        "                   'ba', 'ms', 'bs', 'bsc', 'chartered', 'b', 'gce', 'level', 'levels', \n",
        "                   'diploma', 'business', 'degree', 'programme', 'msc', 'bba', 'hd', 'mba',\n",
        "                   'discipline', 'accounting', 'finance', 'course', 'coursework', 'studies', \n",
        "                   'science', 'computer', 'engineering']\n",
        "  degreesphrase = [['foundations', 'program'], ['exchange', 'student']]\n",
        "  schoolDegreesBlackList = ['award', 'awarded', 'awards', 'member', 'academic', 'academics', \n",
        "                            'prize', 'ranked', 'developed', 'prepared', 'technical', 'it', 'office',\n",
        "                            'manager', 'relevant', 'gpa']\n",
        "  if len(words) >= 2:\n",
        "    for item in schoolDegreesBlackList:\n",
        "      if item in words:\n",
        "        return False\n",
        "\n",
        "    for item in schoolDegrees:\n",
        "      if item in words:\n",
        "        return True\n",
        "\n",
        "    # Check if each degreesphrase is contained in sentence by\n",
        "    #   ensure all strings in phrase also in array words\n",
        "    #   if all strings are in array words, return True\n",
        "    #   else, check the next phrase\n",
        "    #   if exchange all phrases, return false\n",
        "    for item in degreesphrase:\n",
        "      flag = True\n",
        "      for subitem in item:\n",
        "        if subitem not in words:\n",
        "          flag = False\n",
        "          break\n",
        "      if flag:\n",
        "        return flag\n",
        "  \n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkjWQYLm5ITs"
      },
      "source": [
        "# SpecialStar\n",
        "# Input: string\n",
        "# Output: boolean\n",
        "\n",
        "# isSchoolDate:\n",
        "#   Parse line into array of strings\n",
        "#   is school date if the number of words are <= 5 and there are numbers in the line (we can check to have at least 2 numbers)\n",
        "\n",
        "def isSchoolDate(words):\n",
        "    schoolDateBlackList = ['gpa', 'scholarship', 'award', 'awarded', 'awards', 'out', 'dean', 'manager']\n",
        "\n",
        "    if len(words) <= 4:\n",
        "      for item in schoolDateBlackList:\n",
        "        if item in words:\n",
        "          return False\n",
        "\n",
        "      for w in words:\n",
        "        if w.isdigit():\n",
        "          if len(w) == 1:\n",
        "            return False\n",
        "          else:\n",
        "            return True\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUvftDtH0X5O"
      },
      "source": [
        "# Input: Path of label 1 csv with lineNum and Sentence\n",
        "# Output: None -- csv file with data picked out and grouped up\n",
        "\n",
        "def extract_education(csv_file_path):\n",
        "  import pandas as pd\n",
        "\n",
        "  # i = 0 --> indexing for storage array\n",
        "  i = 0\n",
        "\n",
        "  startLine = []\n",
        "  qualification, schoolName, schoolDegree, schoolDate = [[]], [[]], [[]], [[]]\n",
        "\n",
        "  # Load csv Label1 into pandas\n",
        "  # data = pandas.read_csv (csv_file_path, sep=\"\\n\")\n",
        "  data = pd.read_csv(csv_file_path)\n",
        "\n",
        "  # Initalize beginning\n",
        "  startLine.append(0)\n",
        "  qualification.append([])\n",
        "  schoolName.append([])\n",
        "  schoolDegree.append([])\n",
        "  schoolDate.append([])\n",
        "\n",
        "  # For each rows in 'Sentence', place them into classified bins\n",
        "  for row in range(len(data['Sentence'])):\n",
        "    strings = data['Sentence'][row].split(' ')\n",
        "    if data['Sentence'][row].split().count() >= 10 :\n",
        "      continue\n",
        "    # Parse sentence into subsentences\n",
        "    subsentence = []\n",
        "    subsentence.append([])\n",
        "    j = 0\n",
        "    for word in strings:\n",
        "      if word == '':\n",
        "        subsentence.append([])\n",
        "        j+=1\n",
        "      else:\n",
        "        subsentence[j].append(word)\n",
        "    for sentence in subsentence:\n",
        "      # Skip empty cells\n",
        "      if not sentence:\n",
        "        continue\n",
        "      if sentence[0] == \"--------------------------------------------------\":\n",
        "        startLine.append(data[data.columns[0]][row])\n",
        "        if row+1 != len(data['Sentence']):\n",
        "          qualification.append([])\n",
        "          schoolName.append([])\n",
        "          schoolDegree.append([])\n",
        "          schoolDate.append([])\n",
        "        i+=1\n",
        "      lower_sentence = [x.lower() for x in sentence]\n",
        "      if isQualification(lower_sentence):\n",
        "        qualification[i].append(' '.join(sentence))\n",
        "      elif isSchoolName(lower_sentence):\n",
        "        schoolName[i].append(' '.join(sentence))\n",
        "      elif isSchoolDegree(lower_sentence):\n",
        "        schoolDegree[i].append(' '.join(sentence))\n",
        "        print('School Degree Length:', data['Sentence'][row].split().count())\n",
        "      elif isSchoolDate(lower_sentence):\n",
        "        schoolDate[i].append(' '.join(sentence))\n",
        "\n",
        "  #print(len(startLine), len(schoolName), len(schoolDegree), len(schoolDate), len(qualification))\n",
        "\n",
        "  # Make data into directionary\n",
        "  data = {'Init_index':startLine, \n",
        "          'SchoolName':schoolName,\n",
        "          'SchoolDegree':schoolDegree,\n",
        "          'SchoolDate':schoolDate,\n",
        "          'Qualifications':qualification}\n",
        "  \n",
        "  # Convert data into dataFrame\n",
        "  EducationList = pd.DataFrame(data)\n",
        "\n",
        "  print(EducationList)\n",
        "\n",
        "  # Put dataFrame into csv\n",
        "  EducationList.to_csv(\"/content/EducationList.csv\", index=True)\n",
        "\n",
        "# Note: to deal with problems like 1731, we present the data via \"chaining\"\n",
        "#       if a line of degree or school have date, we skip the date\n",
        "#       Can consider pushing schoolDate with only one year out\n",
        "#       Junior schools pairs with certificate usually\n",
        "#       Capture locations too"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCWgsMdv96x3"
      },
      "source": [
        "# SpecialStar\n",
        "# Load all the existing models\n",
        "\n",
        "\n",
        "print('Loading word embedders')\n",
        "# Models for word embedding\n",
        "# If adding another path to load model, please add it to None, and comment out the existing load_model line\n",
        "model = None\n",
        "model = load_model('/content/drive/MyDrive/ECS 193A ResumeData/Model & Rev_Model/WE_model.h5')\n",
        "rev_model = None\n",
        "rev_model = load_model('/content/drive/MyDrive/ECS 193A ResumeData/Model & Rev_Model/WE_rev_model.h5')\n",
        "\n",
        "# Saved tokenizer and maximum length of word\n",
        "tokenizer_filepath = '/content/drive/MyDrive/ECS 193A ResumeData/Model & Rev_Model/tokenizer.pkl'\n",
        "with open(tokenizer_filepath, 'rb') as f:\n",
        "    tokenizer = pickle.load(f)  \n",
        "max_length_filepath = '/content/drive/MyDrive/ECS 193A ResumeData/Model & Rev_Model/max_length.pkl'\n",
        "with open(max_length_filepath, 'rb') as f:\n",
        "    max_length = pickle.load(f)\n",
        "\n",
        "#load spacy GloVe Model\n",
        "#Can load en_core_web_lg for more unique vectors\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "print('Complete')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LYsKZ6AGW5K"
      },
      "source": [
        "# Input: file path of clean text and unclean text\n",
        "# Output: clean_unclean_text dataframe\n",
        "\n",
        "def combine_text(outputtext,outputtext_org):\n",
        "  clean = pandas.read_csv (outputtext, skip_blank_lines=False)\n",
        "  unclean = pandas.read_csv (outputtext_org, sep=\"\\n\")\n",
        "\n",
        "  clean_sentences = clean['Sentence']\n",
        "\n",
        "  unclean.insert(1,\"clean_Sentence\",clean_sentences, True)\n",
        "  \n",
        "  return unclean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEF1Us-KKZDE"
      },
      "source": [
        "# Input: file path of LC_output, clean_unclean_text dataframe\n",
        "# Output: labeled_text df\n",
        "# Labelize the combined text and save it as labeled_text.csv\n",
        "\n",
        "def labelizingfulltext(lc_filepath, placeholding):\n",
        "  #read LC_output.csv and placeholding.csv as datafram format\n",
        "  lc_output = pandas.read_csv(lc_filepath)\n",
        "\n",
        "  lcdf = lc_output.iloc[:,[1,3]]\n",
        "  count_row = lcdf.shape[0]\n",
        "\n",
        "  #replace Label 5 with the previous label in lc_output\n",
        "  for i in range (1,count_row):\n",
        "    if lcdf.iloc[i,1] == 5:\n",
        "      lcdf.iloc[i,1] = lcdf.iloc[i-1,1]\n",
        "\n",
        "  #get each line's label and save them into label list\n",
        "  label = []\n",
        "  for i in range(1,count_row):\n",
        "    for j in range(0,lcdf.iloc[i,0]-lcdf.iloc[i-1,0]):   # lcdf.iloc[i-1,0] + j = currect row\n",
        "      label = label + [lcdf.iloc[i-1,1]]\n",
        "\n",
        "  label = label + [0]\n",
        "\n",
        "  #Add label list to placeholding dataframe as a column\n",
        "  placeholding.insert(2,\"Label\",label,True)\n",
        "  placeholding = placeholding.rename({'Sentence': 'unclean_Sentence'}, axis=1)\n",
        "\n",
        "  #Change all the lines that are website link or phone number to 4\n",
        "  for i in range(1, placeholding.shape[0]):\n",
        "    webstring = placeholding.at[i, 'clean_Sentence']\n",
        "    phonestring = placeholding.at[i, 'unclean_Sentence']\n",
        "    if isPhoneRow(phonestring) or isWebsiteRow(webstring):\n",
        "      placeholding.at[i, \"Label\"] = 4\n",
        "\n",
        "  #Save dataframe to Placeholding.csv\n",
        "  placeholding.to_csv('labeled_text.csv')\n",
        "  placeholding.insert(0,\"index\",numpy.arange(len(placeholding)))\n",
        "\n",
        "\n",
        "  return placeholding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2Tm4R-vi2Wi"
      },
      "source": [
        "def extract_title_corenlp(text):\n",
        "  document = client.annotate(text)\n",
        "  title_set = []\n",
        "  for sent in document.sentence:\n",
        "          for m in sent.mentions:\n",
        "            if m.entityType == \"TITLE\" and text.count(\" \") < 9:\n",
        "              title_set.append(m.entityMentionText)\n",
        "  return title_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ9ys89pjELe"
      },
      "source": [
        "def extract_organization_corenlp(text):\n",
        "  document = client.annotate(text)\n",
        "  org_set = []\n",
        "  for sent in document.sentence:\n",
        "          for m in sent.mentions:\n",
        "            if m.entityType == \"ORGANIZATION\" and text.count(\" \") < 9:\n",
        "              org_set.append(m.entityMentionText)\n",
        "  return org_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2-gV69Rn4Ns"
      },
      "source": [
        "def extract_email(text):\n",
        "    '''\n",
        "    Helper function to extract email id from text\n",
        "    :param text: plain text extracted from resume file\n",
        "    '''\n",
        "    email = re.findall(r\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", text)\n",
        "    if email:\n",
        "        try:\n",
        "            return email[0].split()[0].strip(';')\n",
        "        except IndexError:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHZcw2puppGG"
      },
      "source": [
        "def extract_mobile_number(text):\n",
        "\n",
        "    mob_num_regex = r'''(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\n",
        "                        [-\\.\\s]*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})'''\n",
        "    phone = re.findall(re.compile(mob_num_regex), text)\n",
        "    \n",
        "    if phone:\n",
        "        number = ''.join(phone[0])\n",
        "        return number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvEqXY2eprmD"
      },
      "source": [
        "def extract_skills(nlp_text, noun_chunks, skills_file=None):\n",
        "    '''\n",
        "    Helper function to extract skills from spacy nlp text\n",
        "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
        "    :param noun_chunks: noun chunks extracted from nlp text\n",
        "    :return: list of skills extracted\n",
        "    '''\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    if not skills_file:\n",
        "        data = pd.read_csv(\n",
        "            '/content/drive/MyDrive/ECS 193A ResumeData/skills.csv'\n",
        "        )\n",
        "    else:\n",
        "        data = pd.read_csv(skills_file)\n",
        "    skills = list(data.columns.values)\n",
        "    skillset = []\n",
        "    # check for one-grams\n",
        "    for token in tokens:\n",
        "        if token.lower() in skills:\n",
        "            skillset.append(token)\n",
        "\n",
        "    # check for bi-grams and tri-grams\n",
        "    for token in noun_chunks:\n",
        "        token = token.text.lower().strip()\n",
        "        if token in skills:\n",
        "            skillset.append(token)\n",
        "    return [i.capitalize() for i in set([i.lower() for i in skillset])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf5CbNDEnFAv"
      },
      "source": [
        "def extract_name_new(text):\n",
        "  classified_text = st.tag(text.split())\n",
        "  named_entities = get_continuous_chunks(classified_text)\n",
        "  named_entities_str_tag = [(\" \".join([token for token, tag in ne]), ne[0][1]) for ne in named_entities]\n",
        " \n",
        "  for x in named_entities_str_tag:\n",
        "    if x[1] == \"PERSON\":\n",
        "      return x[0]\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2reoTtLEp6_K"
      },
      "source": [
        "def get_continuous_chunks(tagged_sent):\n",
        "    continuous_chunk = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for token, tag in tagged_sent:\n",
        "        if tag != \"O\":\n",
        "            current_chunk.append((token, tag))\n",
        "        else:\n",
        "            if current_chunk: # if the current chunk is not empty\n",
        "                continuous_chunk.append(current_chunk)\n",
        "                current_chunk = []\n",
        "    # Flush the final current_chunk into the continuous_chunk, if any.\n",
        "    if current_chunk:\n",
        "        continuous_chunk.append(current_chunk)\n",
        "    return continuous_chunk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni46-J_NS7Gy"
      },
      "source": [
        "#Input: Path of laved clean text and unclean text file\n",
        "#Output: None \n",
        "\n",
        "def extract_non_context(fulltext):\n",
        "    import pandas as pd\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    i = 0\n",
        "\n",
        "    startLine = []\n",
        "    name, email, skill, phone, org, title = [[]], [[]], [[]], [[]], [[]], [[]]\n",
        "    qualification, schoolName, schoolDegree, schoolDate = [[]], [[]], [[]], [[]]\n",
        "\n",
        "    #get label4 and label1 sentences\n",
        "    unclean_sentences = fulltext['unclean_Sentence']\n",
        "    labels = fulltext['Label']\n",
        "    clean_sentences = fulltext['clean_Sentence']\n",
        "    info_sentences = []\n",
        "    skill_sentences = []\n",
        "    org_sentences = []\n",
        "    education_sentence = []\n",
        "    indx = []\n",
        "\n",
        "    for row in range(len(unclean_sentences)):\n",
        "     if labels[row] == 4 or labels[row] == 0:\n",
        "       info_sentences.append(unclean_sentences[row])\n",
        "       indx.append(fulltext.iloc[row][0])\n",
        "\n",
        "    data = {'Init_index': indx,\n",
        "            'unclean_Sentence': info_sentences}\n",
        "    personalInfo = pd.DataFrame(data)\n",
        "\n",
        "    indx = []\n",
        "    for row in range(len(unclean_sentences)):\n",
        "     if labels[row] == 3 or labels[row] == 0:\n",
        "       skill_sentences.append(unclean_sentences[row])\n",
        "       indx.append(fulltext.iloc[row][0])\n",
        "\n",
        "    data = {'Init_index': indx,\n",
        "            'unclean_Sentence': skill_sentences}\n",
        "    skillInfo = pd.DataFrame(data)\n",
        "\n",
        "    indx = []\n",
        "    for row in range(len(unclean_sentences)):\n",
        "     if labels[row] == 2 or labels[row] == 0:\n",
        "       org_sentences.append(unclean_sentences[row])\n",
        "       indx.append(fulltext.iloc[row][0])\n",
        "\n",
        "    data = {'Init_index': indx,\n",
        "            'unclean_Sentence': org_sentences}\n",
        "    orgInfo = pd.DataFrame(data)\n",
        "\n",
        "    indx = []\n",
        "    for row in range(len(clean_sentences)):\n",
        "     if labels[row] == 1 or labels[row] == 0:\n",
        "       education_sentence.append(clean_sentences[row])\n",
        "       indx.append(fulltext.iloc[row][0])\n",
        "    \n",
        "    data = {'Init_index': indx,\n",
        "            'clean_Sentence': education_sentence}\n",
        "    educationInfo = pd.DataFrame(data)\n",
        "\n",
        "    #init beginning\n",
        "    startLine.append(0)\n",
        "    name.append([])\n",
        "    org.append([])\n",
        "    title.append([])\n",
        "    email.append([])\n",
        "    phone.append([])\n",
        "    skill.append([])\n",
        "    qualification.append([])\n",
        "    schoolName.append([])\n",
        "    schoolDegree.append([])\n",
        "    schoolDate.append([])\n",
        "\n",
        "    #extract org and title\n",
        "    print(len(orgInfo['unclean_Sentence']))\n",
        "    for row in range(len(orgInfo['unclean_Sentence'])):\n",
        "        string1 = orgInfo['unclean_Sentence'][row]\n",
        "        if string1 == \"--------------------------------------------------\":\n",
        "            startLine.append(orgInfo.iloc[row][1])\n",
        "            if row+1 != len(orgInfo['unclean_Sentence']):\n",
        "              org.append([])\n",
        "              title.append([])\n",
        "            i += 1\n",
        "            # print(i)\n",
        "\n",
        "\n",
        "        orgtemp = extract_organization_corenlp(str(string1))\n",
        "        titletemp = extract_title_corenlp(str(string1))\n",
        "\n",
        "        if len(orgtemp) != 0:\n",
        "            for oneorg in orgtemp:\n",
        "                org[i].append(oneorg)\n",
        "        if len(titletemp) != 0:\n",
        "            for onetitle in titletemp:\n",
        "                title[i].append(onetitle)\n",
        "\n",
        "        # if i > 50:\n",
        "        #   startLine.append(0)\n",
        "        #   break\n",
        "\n",
        "    # print(len(startLine), len(org), len(title))\n",
        "\n",
        "    #extrac personal information\n",
        "    i = 0\n",
        "    startLine = []\n",
        "    startLine.append(0)\n",
        "    firstLine = personalInfo['unclean_Sentence'][0]\n",
        "    for row in range(len(personalInfo['unclean_Sentence'])):\n",
        "        string1 = personalInfo['unclean_Sentence'][row]\n",
        "        if string1 == \"--------------------------------------------------\":\n",
        "            if len(name[i]) < 1:\n",
        "              name[i].append(firstLine)\n",
        "            startLine.append(personalInfo.iloc[row][1])\n",
        "            if row+1 != len(personalInfo['unclean_Sentence']):\n",
        "              firstLine = personalInfo['unclean_Sentence'][row + 1]\n",
        "              name.append([])\n",
        "              email.append([])\n",
        "              phone.append([])\n",
        "            i += 1\n",
        "            # print(i)\n",
        "\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        nlp = nlp(string1)\n",
        "        skills_file = None\n",
        "        noun_chunks = list(nlp.noun_chunks)\n",
        "\n",
        "        if(len(name[i]) < 1):\n",
        "          nametemp = extract_name_new(str(string1))\n",
        "        emailtemp = extract_email(str(string1))\n",
        "        phonetemp = extract_mobile_number(str(string1))\n",
        "\n",
        "        if emailtemp != None:\n",
        "            email[i].append(emailtemp)\n",
        "        elif nametemp != None and len(name[i]) < 1:\n",
        "            name[i].append(nametemp)\n",
        "        elif phonetemp != None and len(phone[i]) < 1:\n",
        "            phone[i].append(phonetemp)\n",
        "        # if i > 50:\n",
        "        #   startLine.append(0)\n",
        "        #   break\n",
        "    # print(len(startLine), len(name), len(email), len(phone))\n",
        "\n",
        "    #extract skill\n",
        "    i = 0\n",
        "    startLine = []\n",
        "    startLine.append(0)\n",
        "    for row in range(len(skillInfo['unclean_Sentence'])):\n",
        "        string1 = skillInfo['unclean_Sentence'][row]\n",
        "        if string1 == \"--------------------------------------------------\":\n",
        "            startLine.append(skillInfo.iloc[row][1])\n",
        "            if row+1 != len(skillInfo['unclean_Sentence']):\n",
        "              skill.append([])\n",
        "            i += 1\n",
        "           # print(i)\n",
        "\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        nlp = nlp(string1)\n",
        "        skills_file = None\n",
        "        noun_chunks = list(nlp.noun_chunks)\n",
        "\n",
        "        skilltemp = extract_skills(nlp, noun_chunks, skills_file)\n",
        "\n",
        "        if len(skilltemp) != 0:\n",
        "            for oneskill in skilltemp:\n",
        "                skill[i].append(oneskill)\n",
        "        # if i > 50:\n",
        "        #   startLine.append(0)\n",
        "        #   break\n",
        "\n",
        "    #print(len(startLine), len(skill))\n",
        "\n",
        "    #extract education information\n",
        "    i = 0\n",
        "    startLine = []\n",
        "    startLine.append(0)\n",
        "\n",
        "    for row in range(len(educationInfo['clean_Sentence'])):\n",
        "      strings = educationInfo['clean_Sentence'][row].split(' ')\n",
        "      #if orgInfo['unclean_Sentence'][row].count(\" \") >= 10:\n",
        "      #  continue\n",
        "      # Parse sentence into subsentences\n",
        "      subsentence = []\n",
        "      subsentence.append([])\n",
        "      j = 0\n",
        "      for word in strings:\n",
        "        if word == '':\n",
        "          subsentence.append([])\n",
        "          j+=1\n",
        "        else:\n",
        "          subsentence[j].append(word)\n",
        "      for sentence in subsentence:\n",
        "        # Skip empty cells\n",
        "        if not sentence:\n",
        "          continue\n",
        "        if sentence[0] == \"--------------------------------------------------\":\n",
        "          startLine.append(educationInfo[educationInfo.columns[0]][row])\n",
        "          if row+1 != len(educationInfo['clean_Sentence']):\n",
        "            qualification.append([])\n",
        "            schoolName.append([])\n",
        "            schoolDegree.append([])\n",
        "            schoolDate.append([])\n",
        "          i+=1\n",
        "        lower_sentence = [x.lower() for x in sentence]\n",
        "        if isQualification(lower_sentence):\n",
        "          qualification[i].append(' '.join(sentence))\n",
        "        elif isSchoolName(lower_sentence):\n",
        "          schoolName[i].append(' '.join(sentence))\n",
        "        elif isSchoolDegree(lower_sentence):\n",
        "          schoolDegree[i].append(' '.join(sentence))\n",
        "        elif isSchoolDate(lower_sentence):\n",
        "          schoolDate[i].append(' '.join(sentence))\n",
        "      # if i > 50:\n",
        "      #   startLine.append(0)\n",
        "      #   break\n",
        "\n",
        "    #print(len(startLine), len(qualification), len(schoolName), len(schoolDegree), len(schoolDate))\n",
        "\n",
        "    # Make data into directionary\n",
        "    data = {'Init_index': startLine,\n",
        "            'name': name,\n",
        "            'organization': org,\n",
        "            'position': title,\n",
        "            'email': email,\n",
        "            'phone': phone,\n",
        "            'skill': skill,\n",
        "            'SchoolName':schoolName,\n",
        "            'SchoolDegree':schoolDegree,\n",
        "            'SchoolDate':schoolDate,\n",
        "            'Qualifications':qualification}\n",
        "\n",
        "    # Convert data into dataFrame\n",
        "    noncontextualinfo = pd.DataFrame(data)\n",
        "    noncontextualinfo = noncontextualinfo[:-1]\n",
        "\n",
        "    # Put dataFrame into csv\n",
        "    noncontextualinfo.to_csv(\"/content/Noncontutalinfo.csv\", index=True)\n",
        "\n",
        "    return noncontextualinfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjTsWzYiu1I9"
      },
      "source": [
        "# Note: might want to save nlp vectors\n",
        "# Resume scanner (in Alpha)\n",
        "# Input: the Directory with resume of interest (The directory must only contain pdf files or JPG files)\n",
        "# Output: the csv file with labels marked for each resumes\n",
        "def ResumeScanner(directory):\n",
        "  directory = processFileFormat(directory)\n",
        "\n",
        "  print('Converting resumes into text...')\n",
        "  # Convert all resumes in directory to text file\n",
        "  outputtext, outputtext_org = preprocess_resume(directory)\n",
        "  print('Finished Converting')\n",
        "\n",
        "  print('Picking out headers...')\n",
        "  # Find the headers and output into csv\n",
        "  HC_model_path = None\n",
        "  HC_outputfile = get_headers(outputtext_org, HC_model_path)\n",
        "  print('Finished Picking out headers')\n",
        "\n",
        "  print('Picking out labels...')\n",
        "  # Label each headers and output into csv\n",
        "  LC_model_path = None\n",
        "  LC_outputfile = get_label(HC_outputfile,LC_model_path)\n",
        "  print('Finished Picking out labels')\n",
        "\n",
        "  print('Combining text data...')\n",
        "  combine_text_df = combine_text(outputtext, outputtext_org)\n",
        "  print('Finished Combining text data')\n",
        "\n",
        "  print('Lablizing text data...')\n",
        "  labeled_text_df = labelizingfulltext(LC_outputfile,combine_text_df)\n",
        "  print('Finished Lablizing text data...')\n",
        "\n",
        "  print('Extracting information...')\n",
        "  info_df = extract_non_context(labeled_text_df)\n",
        "  info = pandas.read_csv('/content/Noncontutalinfo.csv')\n",
        "  sendCSVFile(info)\n",
        "  print('Finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTk3ZEyc9DCT"
      },
      "source": [
        "ResumeScanner('/content/drive/MyDrive/ECS 193A ResumeData/Resume_Google')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}