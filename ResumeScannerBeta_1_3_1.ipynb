{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResumeScannerBeta 1.3.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rwfchan/ResumeScanner/blob/main/ResumeScannerBeta_1_3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTk3ZEyc9DCT"
      },
      "source": [
        "# Input: the resumes folder path\n",
        "ResumeScanner('File path to resume')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB2cNrD1fmRm"
      },
      "source": [
        "# Connect personal google drive to Google Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gin_W7SFYl4W"
      },
      "source": [
        "pip install --upgrade google-cloud-vision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-vuxQzrF4Dk"
      },
      "source": [
        "def sendCSVFile(csv_dataframe, dir):\n",
        "    mydb = mysql.connector.connect(host=\"Enter Host Address\", user=\"Enter Username\", passwd=\"Enter Password\", database=\"Enter Database Name\")\n",
        "    mycursor = mydb.cursor()\n",
        "\n",
        "    csv_dataframe = csv_dataframe.drop(['Init_index'], axis=1)\n",
        "\n",
        "    # get the jpg list\n",
        "    jpgFiles = os.listdir(dir)\n",
        "    i = 0\n",
        "    \n",
        "    # need to change the database name and table name\n",
        "    sql_create_table = '''\n",
        "                      CREATE TABLE csv_output.resume_scanner_beta\n",
        "                      (Name TEXT(1000),Organization TEXT(1000),Position TEXT(1000),\n",
        "                      Email TEXT(1000),Phone TEXT(1000),Skill TEXT(1000),\n",
        "                      SchoolName TEXT(1000),SchoolDegree TEXT(1000),SchoolDate TEXT(1000),\n",
        "                      Qualification TEXT(1000), Image LONGBLOB)\n",
        "                      '''\n",
        "    # need to change the database name and table name\n",
        "    mycursor.execute(\"DROP TABLE IF EXISTS csv_output.resume_scanner_beta\")\n",
        "    mycursor.execute(sql_create_table)\n",
        "    \n",
        "    for row, column in csv_dataframe.iterrows():\n",
        "      b = convertToBinaryData(os.path.join(dir, jpgFiles[i]))\n",
        "      tup = (column.Name,\n",
        "                 column.organization,\n",
        "                 column.position,\n",
        "                 column.email,\n",
        "                 column.phone,\n",
        "                 column.skill,\n",
        "                 column.SchoolName,\n",
        "                 column.SchoolDegree,\n",
        "                 column.SchoolDate,\n",
        "                 column.Qualifications,\n",
        "                 b)\n",
        "      # need to change the database name and table name\n",
        "      mycursor.execute('''\n",
        "                INSERT INTO csv_output.resume_scanner_beta \n",
        "                (Name, Organization, Position, Email, Phone, \n",
        "                Skill, SchoolName, SchoolDegree, \n",
        "                SchoolDate, Qualification, Image)\n",
        "                VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
        "                ''',\n",
        "                tup)\n",
        "      i += 1\n",
        "    \n",
        "    mydb.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QmGCgV8x55C"
      },
      "source": [
        "def detect_text_refine(path):\n",
        "  \"\"\"Detects text in the file.\"\"\"\n",
        "  from google.cloud import vision\n",
        "  import io\n",
        "  import os\n",
        "  import pandas\n",
        "\n",
        "  from enum import Enum\n",
        "  \n",
        "  class FeatureType(Enum):\n",
        "      PAGE = 1\n",
        "      BLOCK = 2\n",
        "      PARA = 3\n",
        "      WORD = 4\n",
        "      SYMBOL = 5\n",
        "\n",
        "  os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"Upload Credential Here\"\n",
        "  \n",
        "  client = vision.ImageAnnotatorClient()\n",
        "\n",
        "  with io.open(path, 'rb') as image_file:\n",
        "      content = image_file.read()\n",
        "\n",
        "  image = vision.Image(content=content)\n",
        "\n",
        "  response = client.document_text_detection(image=image)\n",
        "  texts = response.full_text_annotation\n",
        "\n",
        "  resume_blocks = []\n",
        "  id = 1\n",
        "  \n",
        "  for page in texts.pages:\n",
        "    for block in page.blocks:\n",
        "      resume_block = {}\n",
        "      resume_block['id'] = id\n",
        "      resume_block['vertices'] = block.bounding_box.vertices\n",
        "      resume_block['paragraphs'] = block.paragraphs\n",
        "      id += 1\n",
        "      if len(block.paragraphs) == 1 and len(block.paragraphs[0].words) == 1 and len(block.paragraphs[0].words[0].symbols) <= 2:\n",
        "        continue\n",
        "      else:\n",
        "        resume_blocks.append(resume_block)\n",
        "  \n",
        "  width = get_resume_width(resume_blocks)\n",
        "  \n",
        "  #get the most left x-axis and the most top y-axis\n",
        "  lx = 99999\n",
        "  ty = 99999\n",
        "  for block in resume_blocks:\n",
        "    x = block['vertices'][0].x\n",
        "    y = block['vertices'][0].y\n",
        "    if lx > x:\n",
        "      lx = x\n",
        "    if ty > y:\n",
        "      ty = y\n",
        "  \n",
        "  columns = []\n",
        "\n",
        "  for block in resume_blocks:\n",
        "    find = False\n",
        "    for column in columns:\n",
        "      if block['vertices'][0].x >= column[0]['vertices'][0].x  - 100 and block['vertices'][0].x  <= column[0]['vertices'][0].x + 100:\n",
        "        column.append(block)\n",
        "        find = True\n",
        "        break\n",
        "      \n",
        "    if not find:\n",
        "      column_blocks = []\n",
        "      column_blocks.append(block)\n",
        "      columns.append(column_blocks)\n",
        "  \n",
        "  #rearrange column blocks from left to right\n",
        "  columns_ = []\n",
        "\n",
        "  while columns:\n",
        "    index = 0\n",
        "    lt_x = columns[0][0]['vertices'][0].x\n",
        "    for i in range(len(columns)):\n",
        "      if lt_x > columns[i][0]['vertices'][0].x:\n",
        "        index = i\n",
        "        lt_x = columns[i][0]['vertices'][0].x\n",
        "    columns_.append(columns[index])\n",
        "    columns.pop(index)\n",
        "\n",
        "  columns = columns_\n",
        "\n",
        "  #pick the column which has the most blocks\n",
        "  col_index = 0\n",
        "  num_blocks = 0\n",
        "  \n",
        "  for i in range(len(columns)):\n",
        "    if num_blocks < len(columns[i]):\n",
        "      col_index = i\n",
        "      num_blocks = len(columns[i])\n",
        "  \n",
        "  col_blocks = columns[col_index]\n",
        "\n",
        "  #check if the reusme is one-column case or two-column case\n",
        "  istwo = True\n",
        "\n",
        "  num_realcolumns = 0\n",
        "  for column in columns:\n",
        "    if len(column) > 2:\n",
        "      num_realcolumns += 1\n",
        "  \n",
        "  if num_realcolumns == 1:\n",
        "    istwo = False\n",
        "\n",
        "  if col_index == 0 and width < get_resume_width(col_blocks) + 30:\n",
        "    istwo = False\n",
        "    if len(columns) > 1:\n",
        "      if get_density(col_blocks) > 0.35 and get_density(columns[col_index+1]) > 0.38 and get_blocks_len(col_blocks)/get_blocks_len(columns[col_index+1]) < 3:\n",
        "        istwo = True\n",
        "  \n",
        "  if col_index != 0:\n",
        "    if get_density(columns[col_index-1])<0.35 and len(columns[col_index-1]) >= 3:\n",
        "      istwo = False\n",
        "\n",
        "  print('is two column case?', istwo)\n",
        "\n",
        "  if istwo:\n",
        "    #check if the blocks in col_blocks are nested.\n",
        "    #if not, then will insert blocks into gaps\n",
        "    col_blocks_ = []\n",
        "\n",
        "    if col_index != len(columns) - 1:\n",
        "      if get_density(col_blocks) < 0.35:\n",
        "        next_col_index = col_index+1\n",
        "        next_col_blocks = columns[next_col_index]\n",
        "        for i in range(len(col_blocks)-1):\n",
        "          col_blocks_.append(col_blocks[i])\n",
        "          del_idx = []\n",
        "          for q in range(len(next_col_blocks)):\n",
        "            if next_col_blocks[q]['vertices'][3].y <= col_blocks[i+1]['vertices'][1].y and next_col_blocks[q]['vertices'][0].y >= col_blocks[i]['vertices'][1].y-20:\n",
        "              col_blocks_.append(next_col_blocks[q])\n",
        "              del_idx.insert(0,q)\n",
        "          for idx in del_idx:\n",
        "            next_col_blocks.pop(idx)\n",
        "\n",
        "        col_blocks_.append(col_blocks[-1])\n",
        "\n",
        "        if next_col_blocks:\n",
        "          for next_col_block in next_col_blocks:\n",
        "            col_blocks_.append(next_col_block)\n",
        "        \n",
        "        col_blocks = col_blocks_\n",
        "\n",
        "    #reorder blocks\n",
        "    #1. delete col_blocks from resume blocks\n",
        "    for col_block in col_blocks:\n",
        "      id = col_block['id']\n",
        "      for block in resume_blocks:\n",
        "        if block['id'] == id:\n",
        "          resume_blocks.remove(block)\n",
        "\n",
        "    #2. combine col_blocks and resume_blocks and put in correct order\n",
        "    if col_blocks[0]['vertices'][0].y < ty + 50 :\n",
        "      if col_blocks[0]['vertices'][0].x < lx + 200:\n",
        "        for block in resume_blocks:\n",
        "          col_blocks.append(block)\n",
        "        resume_blocks = col_blocks\n",
        "      else:\n",
        "        for block in col_blocks:\n",
        "          resume_blocks.append(block)\n",
        "    else:\n",
        "      if col_blocks[0]['vertices'][0].x < lx + 100:\n",
        "        for block in col_blocks:\n",
        "          resume_blocks.append(block)\n",
        "      else:\n",
        "        last_blcoks = []\n",
        "        for block in resume_blocks:\n",
        "          if block['vertices'][0].x > col_blocks[0]['vertices'][0].x:\n",
        "            last_blcoks.append(block)\n",
        "            resume_blocks.remove(block)\n",
        "        for block in col_blocks:\n",
        "          resume_blocks.append(block)\n",
        "        for block in last_blcoks:\n",
        "          resume_blocks.append(block)\n",
        "  \n",
        "  #3. convert resume blcoks to output files\n",
        "  i = 0   # Counting number of blocks\n",
        "  x1, x2, y1, y2 = 9999, 0, 9999, 0   # the sentence's coordinates\n",
        "  breaks = vision.TextAnnotation.DetectedBreak.BreakType\n",
        "  bounds = [[[]]]\n",
        "  sentence_location = []\n",
        "\n",
        "  # Initialization\n",
        "  bounds.append([])\n",
        "  bounds[0].append([])\n",
        "\n",
        "  for block in resume_blocks:\n",
        "      s = 0   # Counting number of sentence\n",
        "      for paragraph in block['paragraphs']:\n",
        "          for word in paragraph.words:\n",
        "              for symbol in word.symbols: # Beginning of word\n",
        "                  pending_x1 = symbol.bounding_box.vertices[0].x\n",
        "                  pending_y1 = symbol.bounding_box.vertices[0].y\n",
        "                  pending_x2 = symbol.bounding_box.vertices[3].x\n",
        "                  pending_y2 = symbol.bounding_box.vertices[3].y\n",
        "\n",
        "                  x1 = pending_x1 if x1 > pending_x1 else x1\n",
        "                  y1 = pending_y1 if y1 > pending_y1 else y1\n",
        "                  x2 = pending_x2 if x2 < pending_x2 else x2\n",
        "                  y2 = pending_y2 if y2 < pending_y2 else y2\n",
        "\n",
        "                  if symbol.text == '•':\n",
        "                    bounds[i][s] = \"\".join(bounds[i][s]) \n",
        "                    if (bounds[i][s] != '.'):\n",
        "                        sentence_location.append((x1, y1, x2, y2))\n",
        "                    bounds[i].append([])  # append new sentence\n",
        "                    s+=1\n",
        "                    x1, x2, y1, y2 = 9999, 0, 9999, 0   # reset the coordinates\n",
        "                    continue\n",
        "                  else: \n",
        "                    bounds[i][s]+=symbol.text\n",
        "                  \n",
        "                  if symbol.property.detected_break.type_ == breaks.SPACE:\n",
        "                    bounds[i][s]+=' '  # End of a word\n",
        "                  elif symbol.property.detected_break.type_ == breaks.EOL_SURE_SPACE or symbol.property.detected_break.type_ == breaks.LINE_BREAK:\n",
        "                    bounds[i][s] = \"\".join(bounds[i][s]) \n",
        "                    if (bounds[i][s] != '.'):\n",
        "                        sentence_location.append((x1, y1, x2, y2))\n",
        "                    bounds[i].append([])  # append new sentence\n",
        "                    s+=1\n",
        "                    x1, x2, y1, y2 = 9999, 0, 9999, 0   # reset the coordinates\n",
        "          bounds[i].append([])  # append new sentence\n",
        "      i+=1\n",
        "      bounds.append([])\n",
        "      bounds[i].append([]) # For each sentence, not each paragraphs\n",
        "\n",
        "\n",
        "  data = pandas.DataFrame(columns=['Sentence', 'Location'])\n",
        "\n",
        "  tidy_text = \"\"\n",
        "  s = 0\n",
        "  for bound in bounds:\n",
        "      for sentence in bound:\n",
        "          if not sentence or sentence == '.':\n",
        "            continue\n",
        "          if (isinstance(sentence,list) == True):\n",
        "            tidy_text = tidy_text + \"\".join(sentence) + '\\n'\n",
        "            sentence_location.insert(s, (-1,-1,-1,-1))    # If there are any out of handle line, simply insert invalid\n",
        "            data = data.append({'Sentence' : \"\".join(sentence), 'Location' : (-1,-1,-1,-1)}, ignore_index=True)\n",
        "          else:\n",
        "            tidy_text = tidy_text + sentence + '\\n'\n",
        "            data = data.append({'Sentence' : sentence, 'Location' : sentence_location[s]}, ignore_index=True)\n",
        "          s+=1\n",
        "  sentence_location.append((0,0,0,0))   # Appending for end of resume indicator\n",
        "  data = data.append({'Sentence' : \"--------------------------------------------------\\n\", 'Location' : (0,0,0,0)}, ignore_index=True)\n",
        "  \n",
        "  return tidy_text, sentence_location, data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z0exEKNhLdj"
      },
      "source": [
        "pip install pdf2jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idZg4VKxdeJJ"
      },
      "source": [
        "pip install --upgrade spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUuughx9dW8F"
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zx9CoyRmB3j"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5qmxmQ-FszC"
      },
      "source": [
        "pip install mysql-connector-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF1JDXtKiptZ"
      },
      "source": [
        "# Install stanza; note that the prefix \"!\" is not needed if you are running in a terminal\n",
        "!pip install stanza\n",
        "\n",
        "# Import stanza\n",
        "import stanza\n",
        "corenlp_dir = './corenlp'\n",
        "stanza.install_corenlp(dir=corenlp_dir)\n",
        "\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir\n",
        "from stanza.server import CoreNLPClient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUSmzCeKnuB_"
      },
      "source": [
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "!wget 'https://nlp.stanford.edu/software/stanford-ner-2018-10-16.zip'\n",
        "!unzip stanford-ner-2018-10-16.zip\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "st = StanfordNERTagger('/content/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
        "                       '/content/stanford-ner-2018-10-16/stanford-ner.jar',\n",
        "                       encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQmV3i01ZZ-8"
      },
      "source": [
        "import spacy\n",
        "from spacy.vocab import Vocab\n",
        "import numpy\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import np_utils\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import GlobalAveragePooling1D\n",
        "from keras.models import load_model\n",
        "import pickle\n",
        "from spacy.matcher import Matcher\n",
        "import pandas as pd\n",
        "import re\n",
        "import mysql.connector\n",
        "from sqlalchemy import create_engine\n",
        "import io\n",
        "import os\n",
        "import shutil\n",
        "from pdf2jpg import pdf2jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9VWqbx1is0D",
        "outputId": "92427f09-c429-4aa9-9bf7-8fa24186a25c"
      },
      "source": [
        "# Setup standford could\n",
        "client = CoreNLPClient(annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner'], \n",
        "                    memory='4G', endpoint='http://localhost:9003', be_quiet=True)\n",
        "import time; time.sleep(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-20 02:27:03 INFO: Writing properties to tmp file: corenlp_server-dab190ec56534389.props\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnY8sOy2oEtL"
      },
      "source": [
        "import mysql.connector\n",
        "from mysql.connector import Error\n",
        "\n",
        "def convertToBinaryData(filename):\n",
        "    # Convert digital data to binary format\n",
        "    import io\n",
        "    from PIL import Image\n",
        "    with open(filename, 'rb') as file:\n",
        "        binaryData = file.read()\n",
        "    return binaryData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwz-_1pIvdbc"
      },
      "source": [
        "def write_file(data, filename):\n",
        "    # Convert binary data to proper format and write it on Hard Disk\n",
        "    with open(filename, 'wb') as file:\n",
        "        file.write(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdv6gc0-zKHM"
      },
      "source": [
        "def convertPDF2JEG(directory):\n",
        "  import os, shutil\n",
        "  outputpathpre = \"/content/drive/MyDrive/Resume Scanner Folder/ResumeFirstDir\"\n",
        "  outputpathpost = \"/content/drive/MyDrive/Resume Scanner Folder/testingJPG\"\n",
        "\n",
        "  containPDF = False\n",
        "\n",
        "  dir = outputpathpre\n",
        "  for files in os.listdir(dir):\n",
        "    path = os.path.join(dir, files)\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "    except OSError:\n",
        "        os.remove(path)\n",
        "  \n",
        "  dir = outputpathpost\n",
        "  for files in os.listdir(dir):\n",
        "    path = os.path.join(dir, files)\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "    except OSError:\n",
        "        os.remove(path)\n",
        "\n",
        "  # Convert pdf to jpg and place them into separate directories each\n",
        "  for filename in os.listdir(directory):\n",
        "    if  filename.endswith(\".pdf\"):\n",
        "      containPDF = True\n",
        "      pdf2jpg.convert_pdf2jpg(os.path.join(directory, filename), outputpathpre, dpi=300, pages=\"ALL\")\n",
        "\n",
        "  # Place jpg from subdirectories into one singular directory\n",
        "  for subdirectory in os.listdir(outputpathpre):\n",
        "    for filename in os.listdir(os.path.join(outputpathpre, subdirectory)):\n",
        "      shutil.move(os.path.join(outputpathpre, subdirectory, filename), outputpathpost)\n",
        "  \n",
        "  return containPDF\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FTdN0mdHYEG"
      },
      "source": [
        "def processFileFormat(directory):\n",
        "  import os, shutil\n",
        "  from pdf2jpg import pdf2jpg\n",
        "\n",
        "  # Convert pdf to jpg and place them into the same directory\n",
        "  for filename in os.listdir(directory):\n",
        "    if  filename.endswith(\".pdf\"):\n",
        "      pdf2jpg.convert_pdf2jpg(os.path.join(directory, filename), directory, dpi=300, pages=\"ALL\")\n",
        "      foldername = filename + '_dir'\n",
        "      newpath = os.path.join(directory, foldername)\n",
        "\n",
        "      for filename_ in os.listdir(newpath):\n",
        "        newnamelist = filename_.split('.')\n",
        "        newnamelist.remove('pdf')\n",
        "        newname = '.'.join(newnamelist)\n",
        "        os.rename(os.path.join(newpath,filename_),os.path.join(newpath,newname))\n",
        "        shutil.move(os.path.join(newpath,newname),os.path.join(directory,newname))\n",
        "\n",
        "      os.rmdir(newpath)\n",
        "      os.remove(os.path.join(directory,filename))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb4_fNjyc7cp"
      },
      "source": [
        "# Process the resume into relevant text informations\n",
        "# Input: Directory of resumes\n",
        "# Output: the name of the text files\n",
        "def preprocess_resume(directory, debug=False):\n",
        "    import os\n",
        "    import re, string\n",
        "    import copy\n",
        "\n",
        "    print(\"Processing \" + directory)\n",
        "\n",
        "    outputtxt = \"/content/BulkTxtOutput.txt\"\n",
        "    outputtxt_org = \"/content/BulkTxtOutput_org.txt\"\n",
        "\n",
        "    # Set up file clean up\n",
        "    newline = re.compile(\"[\" + \"\\n\" + \"]\")\n",
        "    pattern = re.compile(\"[\" + re.escape(string.punctuation) + \"]\")\n",
        "    afterPattern = re.compile(\"•\")\n",
        "    text_orgPattern = re.compile(\"\\\"\")\n",
        "\n",
        "    text_file = open(outputtxt, \"w\")\n",
        "    text_file_org = open(outputtxt_org, \"w\")\n",
        "    data = pandas.DataFrame(columns=['Sentence', 'Location'])\n",
        "\n",
        "    # Start by writing the column name into textfile\n",
        "    text_file.write('Sentence' + '\\n')\n",
        "    text_file_org.write('Sentence' + '\\n')\n",
        "\n",
        "    # Counter for the first x resume we want to process\n",
        "    i = 0\n",
        "    x = 19\n",
        "\n",
        "    #save process file name to csv\n",
        "    filenameList = [[]]\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "      if debug==True and i > x:\n",
        "        break\n",
        "\n",
        "      if not filename.endswith(\".pdf\"):\n",
        "        print(\"\\n\" + \"Currently processing: \" + filename)\n",
        "        filenameList += [filename]\n",
        "        texts, sentence_location_perResume, df = detect_text_refine(os.path.join(directory, filename))\n",
        "        df['Cleaned Sentence'] = df['Sentence']\n",
        "\n",
        "        for row in range(len(df['Cleaned Sentence'])):\n",
        "          if df['Cleaned Sentence'][row] != \"--------------------------------------------------\\n\" :\n",
        "            df['Cleaned Sentence'][row] = pattern.sub(' ', df['Cleaned Sentence'][row])\n",
        "        for row in range(len(df['Sentence'])):\n",
        "          df['Sentence'][row] = text_orgPattern.sub(' ', df['Sentence'][row])\n",
        "\n",
        "        clean_sentence = '\\n'.join(df['Cleaned Sentence'].tolist())\n",
        "        text_file.write(clean_sentence)\n",
        "\n",
        "        sentence = '\\n'.join(df['Sentence'].tolist())\n",
        "        text_file_org.write(sentence)\n",
        "\n",
        "        data = data.append(df, ignore_index=True)\n",
        "\n",
        "        print(\"Finished Resume number: \" + str(i) + \"--------------------------------------------------\" + \"\\n\")\n",
        "        i+=1\n",
        "\n",
        "    text_file.close()\n",
        "    text_file_org.close()\n",
        "\n",
        "    data = {'filename':filenameList}\n",
        "    filenameDF = pd.DataFrame(data)\n",
        "    filenameDF.to_csv('/content/fileorder.csv')\n",
        "\n",
        "    return outputtxt, outputtxt_org"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgXdqhpjaz4d"
      },
      "source": [
        "def get_column_length(blocks):\n",
        "  length = 0\n",
        "\n",
        "  for block in blocks:\n",
        "    l = block['vertices'][1].x - block['vertices'][0].x\n",
        "    if l > length:\n",
        "      length = l\n",
        "\n",
        "  return length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJDiqNXylc9v"
      },
      "source": [
        "def get_density(blocks):\n",
        "  cl = blocks[-1]['vertices'][3].y - blocks[0]['vertices'][0].y\n",
        "  bl = 0\n",
        "  for block in blocks:\n",
        "    bl += block['vertices'][3].y - block['vertices'][0].y\n",
        "  \n",
        "  if not cl:\n",
        "    return 1\n",
        "    \n",
        "  return bl/cl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBVoMcOzELbi"
      },
      "source": [
        "def get_resume_width(blocks):\n",
        "  lx = 9999\n",
        "  rx = 0\n",
        "  for block in blocks:\n",
        "    lx_ = block['vertices'][0].x\n",
        "    rx_ = block['vertices'][1].x\n",
        "    if lx_ < lx:\n",
        "      lx = lx_\n",
        "    if rx_ > rx:\n",
        "      rx = rx_\n",
        "  \n",
        "  return rx - lx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A-5peXPLBBs"
      },
      "source": [
        "def get_blocks_len(blocks):\n",
        "  bl = 0\n",
        "  for block in blocks:\n",
        "    bl += block['vertices'][3].y - block['vertices'][0].y\n",
        "  \n",
        "  return bl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5AEcohXZb0H"
      },
      "source": [
        "# Running Vision API for parsing resume image into text file\n",
        "# Input: path of the resume image\n",
        "# Output: the converted text string\n",
        "def detect_text(path):\n",
        "    \"\"\"Detects text in the file.\"\"\"\n",
        "    from google.cloud import vision\n",
        "    import io\n",
        "    import os\n",
        "\n",
        "    from enum import Enum\n",
        "    from PIL import Image\n",
        "    im = Image.open(path)\n",
        "    width, height = im.size\n",
        "    # print(width)\n",
        "    \n",
        "\n",
        "    \n",
        "    class FeatureType(Enum):\n",
        "        PAGE = 1\n",
        "        BLOCK = 2\n",
        "        PARA = 3\n",
        "        WORD = 4\n",
        "        SYMBOL = 5\n",
        "\n",
        "    # TODO: Modfiy this address\n",
        "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"Enter Credentials Here\"\n",
        "    client = vision.ImageAnnotatorClient()\n",
        "\n",
        "    with io.open(path, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    response = client.document_text_detection(image=image)\n",
        "    texts = response.full_text_annotation\n",
        "\n",
        "    first_line_x = 0\n",
        "\n",
        "    for page in texts.pages:\n",
        "        for block in page.blocks:\n",
        "            if block.bounding_box.vertices[0].x < width/2:\n",
        "                first_line_x = block.bounding_box.vertices[0].x\n",
        "                break\n",
        "\n",
        "    i = 0   # Counting number of blocks\n",
        "    x1, x2, y1, y2 = 9999, 0, 9999, 0   # the sentence's coordinates\n",
        "    breaks = vision.TextAnnotation.DetectedBreak.BreakType\n",
        "    bounds = [[[]]]\n",
        "    sentence_location = []\n",
        "\n",
        "    # Initialization\n",
        "    bounds.append([])\n",
        "    bounds[0].append([])\n",
        "\n",
        "    num_sentence = 0\n",
        "\n",
        "    for page in texts.pages:\n",
        "        for block in page.blocks:\n",
        "            s = 0   # Counting number of sentence\n",
        "            if abs(block.bounding_box.vertices[0].x - first_line_x) < width/4:\n",
        "                  for paragraph in block.paragraphs:\n",
        "                      for word in paragraph.words:\n",
        "                          for symbol in word.symbols: # Beginning of word\n",
        "                              pending_x1 = symbol.bounding_box.vertices[0].x\n",
        "                              pending_y1 = symbol.bounding_box.vertices[0].y\n",
        "                              pending_x2 = symbol.bounding_box.vertices[3].x\n",
        "                              pending_y2 = symbol.bounding_box.vertices[3].y\n",
        "\n",
        "                              x1 = pending_x1 if x1 > pending_x1 else x1\n",
        "                              y1 = pending_y1 if y1 > pending_y1 else y1\n",
        "                              x2 = pending_x2 if x2 < pending_x2 else x2\n",
        "                              y2 = pending_y2 if y2 < pending_y2 else y2\n",
        "\n",
        "                              if symbol.text == '•':\n",
        "                                bounds[i][s] = \"\".join(bounds[i][s]) \n",
        "                                if (bounds[i][s] != '.'):\n",
        "                                    sentence_location.append((x1, y1, x2, y2))\n",
        "                                bounds[i].append([])  # append new sentence\n",
        "                                s+=1\n",
        "                                x1, x2, y1, y2 = 9999, 0, 9999, 0   # reset the coordinates\n",
        "                                continue\n",
        "                              else: \n",
        "                                bounds[i][s]+=symbol.text\n",
        "                              \n",
        "                              if symbol.property.detected_break.type_ == breaks.SPACE:\n",
        "                                bounds[i][s]+=' '  # End of a word\n",
        "                              elif symbol.property.detected_break.type_ == breaks.EOL_SURE_SPACE or symbol.property.detected_break.type_ == breaks.LINE_BREAK:\n",
        "                                bounds[i][s] = \"\".join(bounds[i][s]) \n",
        "                                if (bounds[i][s] != '.'):\n",
        "                                    sentence_location.append((x1, y1, x2, y2))\n",
        "                                bounds[i].append([])  # append new sentence\n",
        "                                s+=1\n",
        "                                x1, x2, y1, y2 = 9999, 0, 9999, 0   # reset the coordinates\n",
        "                      bounds[i].append([])  # append new sentence\n",
        "                  i+=1\n",
        "                  bounds.append([])\n",
        "                  bounds[i].append([]) # For each sentence, not each paragraphs\n",
        "    \n",
        "    for page in texts.pages:\n",
        "        for block in page.blocks:\n",
        "            s = 0   # Counting number of sentence\n",
        "            if abs(block.bounding_box.vertices[0].x - first_line_x) >= width/4:\n",
        "                  for paragraph in block.paragraphs:\n",
        "                      for word in paragraph.words:\n",
        "                          for symbol in word.symbols: # Beginning of word\n",
        "                              pending_x1 = symbol.bounding_box.vertices[0].x\n",
        "                              pending_y1 = symbol.bounding_box.vertices[0].y\n",
        "                              pending_x2 = symbol.bounding_box.vertices[3].x\n",
        "                              pending_y2 = symbol.bounding_box.vertices[3].y\n",
        "\n",
        "                              x1 = pending_x1 if x1 > pending_x1 else x1\n",
        "                              y1 = pending_y1 if y1 > pending_y1 else y1\n",
        "                              x2 = pending_x2 if x2 < pending_x2 else x2\n",
        "                              y2 = pending_y2 if y2 < pending_y2 else y2\n",
        "\n",
        "                              if symbol.text == '•':\n",
        "                                bounds[i][s] = \"\".join(bounds[i][s]) \n",
        "                                if (bounds[i][s] != '.'):\n",
        "                                    sentence_location.append((x1, y1, x2, y2))\n",
        "                                bounds[i].append([])  # append new sentence\n",
        "                                s+=1\n",
        "                                x1, x2, y1, y2 = 9999, 0, 9999, 0   # reset the coordinates\n",
        "                                continue\n",
        "                              else: \n",
        "                                bounds[i][s]+=symbol.text\n",
        "                              \n",
        "                              if symbol.property.detected_break.type_ == breaks.SPACE:\n",
        "                                bounds[i][s]+=' '  # End of a word\n",
        "                              elif symbol.property.detected_break.type_ == breaks.EOL_SURE_SPACE or symbol.property.detected_break.type_ == breaks.LINE_BREAK:\n",
        "                                bounds[i][s] = \"\".join(bounds[i][s]) \n",
        "                                if (bounds[i][s] != '.'):\n",
        "                                    sentence_location.append((x1, y1, x2, y2))\n",
        "                                bounds[i].append([])  # append new sentence\n",
        "                                s+=1\n",
        "                                x1, x2, y1, y2 = 9999, 0, 9999, 0   # reset the coordinates\n",
        "                      bounds[i].append([])  # append new sentence\n",
        "                  i+=1\n",
        "                  bounds.append([])\n",
        "                  bounds[i].append([]) # For each sentence, not each paragraphs\n",
        "\n",
        "\n",
        "    data = pandas.DataFrame(columns=['Sentence', 'Location'])\n",
        "\n",
        "    tidy_text = \"\"\n",
        "    s = 0\n",
        "    for bound in bounds:\n",
        "        for sentence in bound:\n",
        "            if not sentence or sentence == '.':\n",
        "              continue\n",
        "            if (isinstance(sentence,list) == True):\n",
        "              tidy_text = tidy_text + \"\".join(sentence) + '\\n'\n",
        "              sentence_location.insert(s, (-1,-1,-1,-1))    # If there are any out of handle line, simply insert invalid\n",
        "              data = data.append({'Sentence' : \"\".join(sentence), 'Location' : (-1,-1,-1,-1)}, ignore_index=True)\n",
        "            else:\n",
        "              tidy_text = tidy_text + sentence + '\\n'\n",
        "              data = data.append({'Sentence' : sentence, 'Location' : sentence_location[s]}, ignore_index=True)\n",
        "            s+=1\n",
        "    sentence_location.append((0,0,0,0))   # Appending for end of resume indicator\n",
        "    data = data.append({'Sentence' : \"--------------------------------------------------\\n\", 'Location' : (0,0,0,0)}, ignore_index=True)\n",
        "    \n",
        "    return tidy_text, sentence_location, data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJm7myQTUlS8"
      },
      "source": [
        "# Reference from https://github.com/shabeelkandi/Handling-Out-of-Vocabulary-Words-in-Natural-Language-Processing-using-Language-Modelling/blob/master/RNN%20LSTM%20Model.ipynb\n",
        "\n",
        "# generate a sequence using a language model\n",
        "# Input: the model being used (rev or forward), tokenizer file, max_length of word, and seed of the text\n",
        "# Output: list of predicted words\n",
        "def generate_seq(model, tokenizer, max_length, seed_text):\n",
        "    if seed_text == \"\":\n",
        "        return \"\"\n",
        "    else:\n",
        "        in_text = seed_text\n",
        "        n_words = 1\n",
        "        n_preds = 5\n",
        "        pred_words = \"\"\n",
        "        # generate a fixed number of words\n",
        "        for _ in range(n_words):\n",
        "            # encode the text as integer\n",
        "            encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "            # pre-pad sequences to a fixed length\n",
        "            encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "            # predict probabilities for each word\n",
        "            proba = model.predict(encoded, verbose=0).flatten()\n",
        "            #take the n_preds highest probability classes \n",
        "            yhat = numpy.argsort(-proba)[:n_preds] \n",
        "            # map predicted words index to word\n",
        "            out_word = ''\n",
        "\n",
        "            for _ in range(n_preds):\n",
        "                for word, index in tokenizer.word_index.items():\n",
        "                    if index == yhat[_]:\n",
        "                        out_word = word\n",
        "                        pred_words += ' ' + out_word\n",
        "                        break\n",
        "\n",
        "        return pred_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fb8wRi8aqFX"
      },
      "source": [
        "# Find and set embeddings for OOV words\n",
        "# Input: the interested nlp string\n",
        "# Output: None -- function set OOV to the global nlp vector\n",
        "def set_embedding_for_oov(doc):\n",
        "    #checking for oov words and adding embedding\n",
        "    #Can improve accuracy performance by using the weight from embedding layer. \n",
        "    # However, still doesn't mean it will update the embedding layer\n",
        "    for token in doc:\n",
        "        if token.is_oov == True:\n",
        "            before_text = doc[:token.i].text\n",
        "            after_text = str(array(doc)[:token.i:-1]).replace('[','').replace(']','')\n",
        "\n",
        "            pred_before = generate_seq(model, tokenizer, max_length-1, before_text).split()\n",
        "            pred_after = generate_seq(rev_model, tokenizer, max_length-1, after_text).split()\n",
        "            \n",
        "            # Change embedding array size\n",
        "            # spacy en_core_web_md uses 300 dimension. \n",
        "            # Even if the sentence max length becomes 40. We believe the max length of \n",
        "            embedding = numpy.zeros((300,))\n",
        "\n",
        "            i=len(before_text)\n",
        "            for word in pred_before:\n",
        "                embedding += i*nlp.vocab.get_vector(word)\n",
        "                i= i*.5\n",
        "            i=len(after_text)\n",
        "            for word in pred_after:\n",
        "                embedding += i*nlp.vocab.get_vector(word)\n",
        "                i= i*.25\n",
        "            nlp.vocab.set_vector(token.text, embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtQPhWGTbnTr"
      },
      "source": [
        "#function to find most similar words\n",
        "def most_similar(word):\n",
        "    by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)\n",
        "    return [w.orth_ for w in by_similarity[:10]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiRFKtqKQnqA"
      },
      "source": [
        "# Convert a line of words into array of vectors\n",
        "# Input: the string of text\n",
        "# Output: vectorized text string (size=300)\n",
        "def line2vec(line, maxlength):\n",
        "  import itertools\n",
        "\n",
        "  separator = ' '\n",
        "  # Make sure the embedding learn all the oov\n",
        "  set_embedding_for_oov(nlp(separator.join(line)))\n",
        "  # convert words into vectors\n",
        "  vectors = [nlp.vocab.get_vector(word) for word in line]\n",
        "  # join all vectors into one vector\n",
        "  big_vector = itertools.chain(vectors)\n",
        "  big_vector = pad_sequences(list(big_vector), maxlen=maxlength, dtype='float32', padding='post')\n",
        "\n",
        "  return big_vector[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izFitQrgMvWF"
      },
      "source": [
        "# Error adjustment\n",
        "# Input: data=pandas dataset of all the sentences; input=X; predictions=yhat\n",
        "# Output: error corrected predictions\n",
        "def HC_error_adjustment(data, input, predictions):\n",
        "  header_list = ['academic', 'academics', 'experience', 'education', 'awards', 'award', 'courses', 'voluteer', 'volunteer', 'qualification', 'qualifications',\n",
        "                 'professional', 'work', 'working', 'employment', 'internship', 'business', 'activities', 'project', 'projects', 'research', 'publication', \n",
        "                 'intern', 'languages', 'skills', 'skill', 'fluent', 'summary', 'objective', 'eductio', 'personal', 'profile',\n",
        "                 'interest', 'personal', 'email', 'phone', 'milestones', 'linkedin', 'twitter', 'expertise', 'contact', \n",
        "                 'service', 'services', 'career', 'hobbies', 'coursework', 'traits', 'address', 'certificates', 'about', 'tel',\n",
        "                 'university', 'organizations', 'organization', 'expert', 'certifications']\n",
        "  header_black_list = ['leadership', 'speak', 'communities', 'enterprisse', 'administration', 'director', 'design', 'manager', \n",
        "                       'teacher', 'marketing', 'ethic', 'screenwriting', 'cooperative', 'excellence', 'kuo', 'writing', 'association',\n",
        "                       'practices', 'university']\n",
        "\n",
        "  for row in range(len(input)):\n",
        "    if isinstance(data.at[row, 'Sentence'],float):\n",
        "      continue\n",
        "    \n",
        "    string = data.at[row, 'Sentence'].replace(':', '')\n",
        "    sentence = string.lower().split(' ')\n",
        "    for item in header_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 1\n",
        "        break\n",
        "    \n",
        "    for item in header_black_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 0\n",
        "        break\n",
        "\n",
        "    if sentence[0] != '' and string.split(' ')[0][0].islower():\n",
        "      predictions[row] = 0\n",
        "    \n",
        "    if (len(sentence) > 5):\n",
        "      predictions[row] = 0\n",
        "    if (row == 0 \n",
        "      or data.at[row-1, 'Sentence'] == '--------------------------------------------------'):\n",
        "      predictions[row] = 1\n",
        "\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycIWzCVSjH3w"
      },
      "source": [
        "# Perpare for pulling all headers out and able to train on all papers (on BulkTxtOutput)\n",
        "#   Program can pull all headers out, and put them into another csv\n",
        "# Input: The resume in text file\n",
        "# Output: file path of the csv file with headers marked\n",
        "def get_headers(BulkTxtOutput, model=None):\n",
        "  HC_model = 0\n",
        "  if (model==None):\n",
        "    HC_model = load_model('/content/drive/MyDrive/ECS 193A ResumeData/Model & Rev_Model/HC_model.h5')\n",
        "  else:\n",
        "    HC_model = load_model(model)\n",
        "\n",
        "  csv_file = '/content/placeholding.csv'\n",
        "  max_length = 300*5\n",
        "  output_file = '/content/HC_output.csv'\n",
        "\n",
        "  print('Loading BulkTxtFile...')\n",
        "  # First, load text file to csv\n",
        "  #read_file = pandas.read_csv (BulkTxtOutput)\n",
        "  read_file = pandas.read_csv (BulkTxtOutput, sep=\"\\n\")\n",
        "  read_file.to_csv (csv_file)\n",
        "\n",
        "  print('Loading csv into dataframe...')\n",
        "  # Now, load csv to pandas (because this is the way I know how to do this)\n",
        "  data = pandas.read_csv(csv_file)\n",
        "  data = data.drop(data.columns[0], axis=1)\n",
        "  X = data.copy()\n",
        "\n",
        "  print('Converting words into vectors...')\n",
        "  for row in range(len(X['Sentence'])):\n",
        "    if not isinstance(X['Sentence'][row],float):\n",
        "      line = X['Sentence'][row].split()\n",
        "    if len(line) > 5:\n",
        "      line = line[:5]\n",
        "    big_vector = line2vec(line, max_length)\n",
        "    X['Sentence'][row] = numpy.asarray(big_vector).astype('float32')\n",
        "\n",
        "  print('Done!')\n",
        "\n",
        "  print('Perparing inputs for prediction model...')\n",
        "  input = X['Sentence'].to_numpy().tolist()\n",
        "  for row in range (len(input)):\n",
        "    input[row] = input[row].tolist()\n",
        "  \n",
        "  print('Predicting...')\n",
        "  predictions = (HC_model.predict_classes(input)).T[0]\n",
        "\n",
        "  # Pick up missed traits\n",
        "  predictions = HC_error_adjustment(data, input, predictions)\n",
        "  \n",
        "  print('Placing predictions into csv')\n",
        "  output = pandas.DataFrame(columns=['Sentence'])\n",
        "  # For every predictions[i] = 1, place the sentence onto the \n",
        "  for i in range (len(predictions)):\n",
        "    if (predictions[i]):\n",
        "      output.at[i, 'Sentence'] = data.at[i, 'Sentence']\n",
        "  \n",
        "  output.to_csv(output_file, index=True)\n",
        "\n",
        "  return output_file\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tGITT2iPs13"
      },
      "source": [
        "# One_hot encoding\n",
        "# Input: the classified output needed to be encode\n",
        "# Output: One hot encoded output in numpy array\n",
        "def one_hot_encoding(y):\n",
        "  one_hot_y = np_utils.to_categorical(y)\n",
        "  return one_hot_y.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm9ObPS9VX2x"
      },
      "source": [
        "def isPhoneRow(text):\n",
        "  import re, string\n",
        "  pattern = re.compile(\"[\" + re.escape(string.punctuation) + ' ' + \"]\")\n",
        "\n",
        "  if isinstance(text, float) or '/' in text:\n",
        "    return False\n",
        "\n",
        "  sentence = pattern.sub('', text)\n",
        "\n",
        "  if len(sentence) > 9 and sentence.isdecimal():\n",
        "    return True\n",
        "\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZCQUPv4VYBq"
      },
      "source": [
        "def isWebsiteRow(text):\n",
        "  if isinstance(text, float):\n",
        "    return False\n",
        "\n",
        "  email_list = ['com', 'gmail']\n",
        "  \n",
        "  sentence = text.lower().split(' ')\n",
        "  if len(sentence) > 8:\n",
        "    return False\n",
        "\n",
        "  for item in email_list:\n",
        "    if item in sentence:\n",
        "      return True\n",
        "        \n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yRHse40VYIN"
      },
      "source": [
        "def isNameRow(text):\n",
        "  if extract_name_new(text) != None:\n",
        "    return True\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VYrxIws0o1M"
      },
      "source": [
        "# error adjustment\n",
        "# Input: data=pandas dataset of all the sentences; input=X; predictions=yhat\n",
        "# Output: error corrected predictions\n",
        "def LC_error_adjustment(data, input, predictions):\n",
        "  label1_list = ['academic', 'academics', 'eductio', 'award', 'awards', 'education', 'coursework', 'courseworks', \n",
        "                 'qualification', 'qualifications', 'courses', 'course', 'university', 'certification']\n",
        "  label2_list = ['professional', 'work', 'working', 'employment', 'internship', 'business', 'activities', 'project', 'intern', 'projects', \n",
        "                 'milestones', 'volunteer', 'service', 'services', 'experience', 'career', 'summary', 'objective', 'me', 'organization'\n",
        "                 , 'organizations', 'research', 'publication']\n",
        "  label3_list = ['languages', 'skills', 'skill', 'fluent', 'interest', 'expertise', 'hobbies', 'traits', 'proficiency', 'expert']\n",
        "  label4_list = ['profile', 'personal', 'email', 'phone', 'linkedin', 'twitter', 'contact', 'address', 'tel']\n",
        "  label5_list = ['reference']\n",
        "\n",
        "  for row in range(len(input)):\n",
        "    if (row == 0 \n",
        "      or data.at[row-1, 'Sentence'] == '--------------------------------------------------'):\n",
        "      predictions[row] = 4\n",
        "      continue\n",
        "\n",
        "    string = data.at[row, 'Sentence'].replace(':', '')\n",
        "    sentence = string.lower().split(' ')\n",
        "    for item in label2_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 2\n",
        "        break\n",
        "    \n",
        "    for item in label3_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 3\n",
        "        break\n",
        "    \n",
        "    for item in label1_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 1\n",
        "        break\n",
        "    \n",
        "    for item in label4_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 4\n",
        "        break\n",
        "    \n",
        "    #if isWebsiteRow(string) or isPhoneRow(string):\n",
        "    #  print('@ LC_error_adjustment website, or phone detected at line:', row)\n",
        "    #  predictions[row] = 4\n",
        "\n",
        "    #if isNameRow(string) or isWebsiteRow(string) or isPhoneRow(string):\n",
        "    #  print('@ LC_error_adjustment name, website, or phone detected at line:', row)\n",
        "    #  predictions[row] = 4\n",
        "\n",
        "    for item in label5_list:\n",
        "      if item in sentence:\n",
        "        predictions[row] = 5\n",
        "        break\n",
        "  \n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt6e1D3tnhfM"
      },
      "source": [
        "# Note: labels with 5 could be ignore when labeling whole resumes\n",
        "# Input: csv_file of the with Sentence only is ok\n",
        "def get_label(csv_file, model=None, get_accuracy=False):\n",
        "  LC_model = 0\n",
        "  if (model==None):\n",
        "    LC_model = load_model('/content/drive/MyDrive/ECS 193A ResumeData/Model & Rev_Model/LC_model.h5')\n",
        "  else:\n",
        "    LC_model = load_model(model)\n",
        "  \n",
        "  max_length = 300*5\n",
        "  outputfile = '/content/LC_output.csv'\n",
        "\n",
        "  print('Loading csv into dataframe...')\n",
        "  # Load csv to pandas\n",
        "  data = pandas.read_csv(csv_file)\n",
        "  data.rename(columns={data.columns[0]: \"Init_index\"}, inplace=True)\n",
        "  X = data.copy()\n",
        "\n",
        "  print('Converting words into vectors...')\n",
        "  for row in range(len(X['Sentence'])):\n",
        "    line = X['Sentence'][row].split()\n",
        "    if len(line) > 5:\n",
        "      line = line[:5]\n",
        "    big_vector = line2vec(line, max_length)\n",
        "    X['Sentence'][row] = numpy.asarray(big_vector).astype('float32')\n",
        "\n",
        "  print('Done!')\n",
        "\n",
        "  print('Perparing inputs for prediction model...')\n",
        "  input = X['Sentence'].to_numpy().tolist()\n",
        "  for row in range (len(input)):\n",
        "    input[row] = input[row].tolist()\n",
        "  input = numpy.asarray(input)\n",
        "  \n",
        "  print('Predicting...')\n",
        "  predictions = (LC_model.predict_classes(input))\n",
        "\n",
        "  # Pick up missed traits\n",
        "  predictions = LC_error_adjustment(data, input, predictions)\n",
        "\n",
        "  # Prepare output as copy of data\n",
        "  output = data.copy()\n",
        "\n",
        "  # Append predictions onto output\n",
        "  print('Placing predictions into csv')\n",
        "  output['pred_Label'] = predictions\n",
        "  \n",
        "  # Check accuracy of this prediction\n",
        "  if (get_accuracy == True):\n",
        "    accuracy = 0\n",
        "    correction_check = []\n",
        "    for i in range (len(predictions)):\n",
        "      if (predictions[i] == output.at[i, 'Label']):\n",
        "        accuracy+=1\n",
        "        correction_check.append(1)\n",
        "      else:\n",
        "        correction_check.append(0)\n",
        "    \n",
        "    output['Correction_check'] = correction_check\n",
        "    print('Prediction accuracy: ', str(accuracy/len(predictions)*100))\n",
        "    \n",
        "  output.to_csv(outputfile, index=True)\n",
        "\n",
        "  return outputfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lABOSLDiKIo"
      },
      "source": [
        "# Input: string\n",
        "# Output: boolean\n",
        "\n",
        "def isQualification(words):\n",
        "  #words = sentence.lower().split(' ')\n",
        "  qualifications = ['level', 'levels', 'chartered', 'certified']\n",
        "  if len(words) >= 2:\n",
        "    for item in qualifications:\n",
        "      if item in words:\n",
        "        return True\n",
        "  \n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU0Rdvye4tpu"
      },
      "source": [
        "# Input: string\n",
        "# Output: boolean\n",
        "\n",
        "def isSchoolName(words):\n",
        "  #words = sentence.lower().split(' ')\n",
        "  schoolNames = ['university', 'college', 'school' , 'institute', 'polytechnic', 'cpa', \n",
        "                 'universiti', 'program', 'université', 'uc']\n",
        "  specificNames = ['berkeley', 'san', 'irvine', 'los', 'davis', 'santa', 'riverside']\n",
        "  namePhrase = [['chartered', 'financial', 'analyst']]\n",
        "  schoolNamesBlackList = ['award', 'awarded', 'awards', 'president', 'leader', 'leaders', 'prize', \n",
        "                          'manager', 'middle', 'high', 'name', 'how']\n",
        "  \n",
        "  if len(words) == 1:\n",
        "    for item in specificNames:\n",
        "      if item in words:\n",
        "        return True\n",
        "\n",
        "  if len(words) >= 2:\n",
        "    for item in schoolNamesBlackList:\n",
        "      if item in words:\n",
        "        return False\n",
        "\n",
        "    for item in schoolNames:\n",
        "      if item in words:\n",
        "        return True\n",
        "\n",
        "    # Check if each namePhrase is contained in sentence by\n",
        "    #   ensure all strings in phrase also in array words\n",
        "    #   if all strings are in array words, return True\n",
        "    #   else, check the next phrase\n",
        "    #   if exchange all phrases, return false\n",
        "    for item in namePhrase:\n",
        "      flag = True\n",
        "      for subitem in item:\n",
        "        if subitem not in words:\n",
        "          flag = False\n",
        "          break\n",
        "      if flag:\n",
        "        return flag\n",
        "  \n",
        "  return False\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIdEszBd5IH8"
      },
      "source": [
        "# Input: string\n",
        "# Output: boolean\n",
        "\n",
        "def isSchoolDegree(words):\n",
        "  schoolDegrees = ['bachelor', 'master', 'masters', 'minor', 'major', 'institute', \n",
        "                   'ba', 'ms', 'bs', 'bsc', 'b', 'msc', 'bba', 'hd', 'mba', 'ma', 'm',\n",
        "                   'gce', 'level', 'levels', \n",
        "                   'diploma', 'business', 'degree', 'programme', 'chartered', \n",
        "                   'discipline', 'accounting', 'finance', 'course', 'coursework',\n",
        "                   'science', 'computer', 'engineering', 'marketing', 'art', 'design', 'english']\n",
        "  degreesphrase = [['foundations', 'program'], ['exchange', 'student']]\n",
        "  schoolDegreesBlackList = ['award', 'awarded', 'awards', 'member', 'academic', 'academics', \n",
        "                            'prize', 'ranked', 'developed', 'prepared', 'technical', 'it', 'office',\n",
        "                            'manager', 'relevant', 'gpa', 'i', 'name', 'hours', 'related', 'programmer']\n",
        "\n",
        "  if len(words) >= 2:\n",
        "    for item in schoolDegreesBlackList:\n",
        "      if item in words:\n",
        "        return False\n",
        "\n",
        "    for item in schoolDegrees:\n",
        "      if item in words:\n",
        "        return True\n",
        "\n",
        "    # Check if each degreesphrase is contained in sentence by\n",
        "    #   ensure all strings in phrase also in array words\n",
        "    #   if all strings are in array words, return True\n",
        "    #   else, check the next phrase\n",
        "    #   if exchange all phrases, return false\n",
        "    for item in degreesphrase:\n",
        "      flag = True\n",
        "      for subitem in item:\n",
        "        if subitem not in words:\n",
        "          flag = False\n",
        "          break\n",
        "      if flag:\n",
        "        return flag\n",
        "  \n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkjWQYLm5ITs"
      },
      "source": [
        "# Input: string\n",
        "# Output: boolean\n",
        "\n",
        "def isSchoolDate(words):\n",
        "    schoolDateBlackList = ['gpa', 'scholarship', 'award', 'awarded', 'awards', 'out', 'dean', 'manager']\n",
        "\n",
        "    if len(words) <= 4:\n",
        "      for item in schoolDateBlackList:\n",
        "        if item in words:\n",
        "          return False\n",
        "\n",
        "      for w in words:\n",
        "        if w.isdigit():\n",
        "          if len(w) == 1:\n",
        "            return False\n",
        "          else:\n",
        "            return True\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUvftDtH0X5O"
      },
      "source": [
        "# Input: Path of label 1 csv with lineNum and Sentence\n",
        "# Output: None -- csv file with data picked out and grouped up\n",
        "\n",
        "def extract_education(csv_file_path):\n",
        "  import pandas as pd\n",
        "\n",
        "  i = 0 # --> indexing for storage array\n",
        "\n",
        "  startLine = []\n",
        "  qualification, schoolName, schoolDegree, schoolDate = [[]], [[]], [[]], [[]]\n",
        "\n",
        "  # Load csv Label1 into pandas\n",
        "  data = pd.read_csv(csv_file_path)\n",
        "\n",
        "  # Initalize beginning\n",
        "  startLine.append(0)\n",
        "  qualification.append([])\n",
        "  schoolName.append([])\n",
        "  schoolDegree.append([])\n",
        "  schoolDate.append([])\n",
        "\n",
        "  # For each rows in 'Sentence', place them into classified bins\n",
        "  for row in range(len(data['Sentence'])):\n",
        "    strings = data['Sentence'][row].split(' ')\n",
        "    if data['Sentence'][row].split().count() >= 10 :\n",
        "      continue\n",
        "    # Parse sentence into subsentences\n",
        "    subsentence = []\n",
        "    subsentence.append([])\n",
        "    j = 0\n",
        "    for word in strings:\n",
        "      if word == '':\n",
        "        subsentence.append([])\n",
        "        j+=1\n",
        "      else:\n",
        "        subsentence[j].append(word)\n",
        "    for sentence in subsentence:\n",
        "      # Skip empty cells\n",
        "      if not sentence:\n",
        "        continue\n",
        "      if sentence[0] == \"--------------------------------------------------\":\n",
        "        startLine.append(data[data.columns[0]][row])\n",
        "        if row+1 != len(data['Sentence']):\n",
        "          qualification.append([])\n",
        "          schoolName.append([])\n",
        "          schoolDegree.append([])\n",
        "          schoolDate.append([])\n",
        "        i+=1\n",
        "      lower_sentence = [x.lower() for x in sentence]\n",
        "      if isQualification(lower_sentence):\n",
        "        qualification[i].append(' '.join(lower_sentence))\n",
        "      elif isSchoolName(lower_sentence):\n",
        "        schoolName[i].append(' '.join(lower_sentence))\n",
        "      elif isSchoolDegree(lower_sentence):\n",
        "        schoolDegree[i].append(' '.join(lower_sentence))\n",
        "        print('School Degree Length:', data['Sentence'][row].split().count())\n",
        "      elif isSchoolDate(lower_sentence):\n",
        "        schoolDate[i].append(' '.join(lower_sentence))\n",
        "\n",
        "  # Make data into directionary\n",
        "  data = {'Init_index':startLine, \n",
        "          'SchoolName':schoolName,\n",
        "          'SchoolDegree':schoolDegree,\n",
        "          'SchoolDate':schoolDate,\n",
        "          'Qualifications':qualification}\n",
        "  \n",
        "  # Convert data into dataFrame\n",
        "  EducationList = pd.DataFrame(data)\n",
        "\n",
        "  print(EducationList)\n",
        "\n",
        "  # Put dataFrame into csv\n",
        "  EducationList.to_csv(\"/content/EducationList.csv\", index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCWgsMdv96x3",
        "outputId": "dae83f5f-c702-4db4-df6b-cd5053116078"
      },
      "source": [
        "# Load all the existing models\n",
        "\n",
        "\n",
        "print('Loading word embedders')\n",
        "# Models for word embedding\n",
        "# If adding another path to load model, please add it to None, and comment out the existing load_model line\n",
        "model = None\n",
        "model = load_model('/content/drive/MyDrive/Resume Scanner Folder/WE_model.h5')\n",
        "rev_model = None\n",
        "rev_model = load_model('/content/drive/MyDrive/Resume Scanner Folder/WE_rev_model.h5')\n",
        "\n",
        "# Saved tokenizer and maximum length of word\n",
        "tokenizer_filepath = '/content/drive/MyDrive/Resume Scanner Folder/tokenizer.pkl'\n",
        "with open(tokenizer_filepath, 'rb') as f:\n",
        "    tokenizer = pickle.load(f)  \n",
        "max_length_filepath = '/content/drive/MyDrive/Resume Scanner Folder/max_length.pkl'\n",
        "with open(max_length_filepath, 'rb') as f:\n",
        "    max_length = pickle.load(f)\n",
        "\n",
        "#load spacy GloVe Model\n",
        "#Can load en_core_web_lg for more unique vectors\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "print('Complete')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading word embedders\n",
            "Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LYsKZ6AGW5K"
      },
      "source": [
        "# Input: file path of clean text and unclean text\n",
        "# Output: clean_unclean_text dataframe\n",
        "\n",
        "def combine_text(outputtext,outputtext_org):\n",
        "  clean = pandas.read_csv (outputtext, skip_blank_lines=False)\n",
        "  unclean = pandas.read_csv (outputtext_org, sep=\"\\n\")\n",
        "\n",
        "  clean_sentences = clean['Sentence']\n",
        "\n",
        "  unclean.insert(1,\"clean_Sentence\",clean_sentences, True)\n",
        "  \n",
        "  return unclean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEF1Us-KKZDE"
      },
      "source": [
        "# Input: file path of LC_output, clean_unclean_text dataframe\n",
        "# Output: labeled_text df\n",
        "# Labelize the combined text and save it as labeled_text.csv\n",
        "\n",
        "def labelizingfulltext(lc_filepath, placeholding):\n",
        "  #read LC_output.csv and placeholding.csv as datafram format\n",
        "  lc_output = pandas.read_csv(lc_filepath)\n",
        "\n",
        "  lcdf = lc_output.iloc[:,[1,3]]\n",
        "  count_row = lcdf.shape[0]\n",
        "\n",
        "  #replace Label 5 with the previous label in lc_output\n",
        "  for i in range (1,count_row):\n",
        "    if lcdf.iloc[i,1] == 5:\n",
        "      lcdf.iloc[i,1] = lcdf.iloc[i-1,1]\n",
        "\n",
        "  #get each line's label and save them into label list\n",
        "  label = []\n",
        "  for i in range(1,count_row):\n",
        "    for j in range(0,lcdf.iloc[i,0]-lcdf.iloc[i-1,0]):   # lcdf.iloc[i-1,0] + j = currect row\n",
        "      label = label + [lcdf.iloc[i-1,1]]\n",
        "\n",
        "  label = label + [0]\n",
        "\n",
        "  #Add label list to placeholding dataframe as a column\n",
        "  placeholding.insert(2,\"Label\",label,True)\n",
        "  placeholding = placeholding.rename({'Sentence': 'unclean_Sentence'}, axis=1)\n",
        "\n",
        "  #Change all the lines that are website link or phone number to 4\n",
        "  for i in range(1, placeholding.shape[0]):\n",
        "    webstring = placeholding.at[i, 'clean_Sentence']\n",
        "    phonestring = placeholding.at[i, 'unclean_Sentence']\n",
        "    if isPhoneRow(phonestring) or isWebsiteRow(webstring):\n",
        "      placeholding.at[i, \"Label\"] = 4\n",
        "\n",
        "  #Save dataframe to Placeholding.csv\n",
        "  placeholding.to_csv('labeled_text.csv')\n",
        "  placeholding.insert(0,\"index\",numpy.arange(len(placeholding)))\n",
        "\n",
        "\n",
        "  return placeholding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2Tm4R-vi2Wi"
      },
      "source": [
        "def extract_title_corenlp(text):\n",
        "  document = client.annotate(text)\n",
        "  title_set = []\n",
        "  for sent in document.sentence:\n",
        "          for m in sent.mentions:\n",
        "            if m.entityType == \"TITLE\" and text.count(\" \") < 15:\n",
        "              #title_set.append(m.entityMentionText)\n",
        "              title_set.append(m.entityMentionText.lower())\n",
        "  return title_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ9ys89pjELe"
      },
      "source": [
        "def extract_organization_corenlp(text):\n",
        "  document = client.annotate(text)\n",
        "  org_set = []\n",
        "  for sent in document.sentence:\n",
        "          for m in sent.mentions:\n",
        "            if m.entityType == \"ORGANIZATION\" and text.count(\" \") < 15:\n",
        "              #org_set.append(m.entityMentionText)\n",
        "              org_set.append(m.entityMentionText.lower())\n",
        "  return org_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2-gV69Rn4Ns"
      },
      "source": [
        "def extract_email(text):\n",
        "    '''\n",
        "    Helper function to extract email id from text\n",
        "    :param text: plain text extracted from resume file\n",
        "    '''\n",
        "    email = re.findall(r\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", text)\n",
        "    if email:\n",
        "        try:\n",
        "            return email[0].split()[0].strip(';')\n",
        "        except IndexError:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHZcw2puppGG"
      },
      "source": [
        "def extract_mobile_number(text):\n",
        "\n",
        "    mob_num_regex = r\"\\(?(\\d{3})?\\)?[\\s\\.-]{0,2}?(\\d{3})[\\s\\.-]{0,2}(\\d{4})\"\n",
        "    phone = re.findall(re.compile(mob_num_regex), text)\n",
        "    \n",
        "    if phone:\n",
        "        number = ''.join(phone[0])\n",
        "        return number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr1kDG6kp6s-"
      },
      "source": [
        "def extract_name_revise(text):\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc2 = nlp(text)\n",
        "\n",
        "  # Identify the persons\n",
        "  persons = [ent.text for ent in doc2.ents if ent.label_ == 'PERSON']\n",
        "\n",
        "  # Return persons\n",
        "  if persons:\n",
        "    persons = text\n",
        "\n",
        "  return persons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvEqXY2eprmD"
      },
      "source": [
        "def extract_skills(nlp_text, noun_chunks, skills_file=None):\n",
        "    '''\n",
        "    Helper function to extract skills from spacy nlp text\n",
        "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
        "    :param noun_chunks: noun chunks extracted from nlp text\n",
        "    :return: list of skills extracted\n",
        "    '''\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    if not skills_file:\n",
        "        data = pd.read_csv(\n",
        "            '/content/drive/MyDrive/Resume Scanner Folder/skills.csv'\n",
        "        )\n",
        "    else:\n",
        "        data = pd.read_csv(skills_file)\n",
        "    skills = list(data.columns.values)\n",
        "    skillset = []\n",
        "    # check for one-grams\n",
        "    for token in tokens:\n",
        "        if token.lower() in skills:\n",
        "            #skillset.append(token)\n",
        "            skillset.append(token.lower())\n",
        "\n",
        "    # check for bi-grams and tri-grams\n",
        "    for token in noun_chunks:\n",
        "        token = token.text.lower().strip()\n",
        "        if token in skills:\n",
        "            skillset.append(token)\n",
        "    #return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
        "    return [i.lower() for i in skillset]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf5CbNDEnFAv"
      },
      "source": [
        "def extract_name_new(text):\n",
        "  classified_text = st.tag(text.split())\n",
        "  named_entities = get_continuous_chunks(classified_text)\n",
        "  named_entities_str_tag = [(\" \".join([token for token, tag in ne]), ne[0][1]) for ne in named_entities]\n",
        " \n",
        "  for x in named_entities_str_tag:\n",
        "    if x[1] == \"PERSON\":\n",
        "      return x[0]\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2reoTtLEp6_K"
      },
      "source": [
        "def get_continuous_chunks(tagged_sent):\n",
        "    continuous_chunk = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for token, tag in tagged_sent:\n",
        "        if tag != \"O\":\n",
        "            current_chunk.append((token, tag))\n",
        "        else:\n",
        "            if current_chunk: # if the current chunk is not empty\n",
        "                continuous_chunk.append(current_chunk)\n",
        "                current_chunk = []\n",
        "    # Flush the final current_chunk into the continuous_chunk, if any.\n",
        "    if current_chunk:\n",
        "        continuous_chunk.append(current_chunk)\n",
        "    return continuous_chunk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni46-J_NS7Gy"
      },
      "source": [
        "#Input: Path of laved clean text and unclean text file\n",
        "#Output: None \n",
        "\n",
        "def extract_non_context(fulltext):\n",
        "    import pandas as pd\n",
        "    import timeit\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    i = 0\n",
        "\n",
        "    startLine = []\n",
        "    name, email, skill, phone, org, title = [[]], [[]], [[]], [[]], [[]], [[]]\n",
        "    qualification, schoolName, schoolDegree, schoolDate = [[]], [[]], [[]], [[]]\n",
        "\n",
        "    #get label4 and label1 sentences\n",
        "    unclean_sentences = fulltext['unclean_Sentence']\n",
        "    labels = fulltext['Label']\n",
        "    clean_sentences = fulltext['clean_Sentence']\n",
        "    info_sentences = []\n",
        "    skill_sentences = []\n",
        "    org_sentences = []\n",
        "    education_sentence = []\n",
        "    indx = []\n",
        "\n",
        "    for row in range(len(unclean_sentences)):\n",
        "     if labels[row] == 4 or labels[row] == 0:\n",
        "       info_sentences.append(unclean_sentences[row])\n",
        "       indx.append(fulltext.iloc[row][0])\n",
        "\n",
        "    data = {'Init_index': indx,\n",
        "            'unclean_Sentence': info_sentences}\n",
        "    personalInfo = pd.DataFrame(data)\n",
        "\n",
        "    indx = []\n",
        "    for row in range(len(unclean_sentences)):\n",
        "     if labels[row] == 3 or labels[row] == 0:\n",
        "       skill_sentences.append(unclean_sentences[row])\n",
        "       indx.append(fulltext.iloc[row][0])\n",
        "\n",
        "    data = {'Init_index': indx,\n",
        "            'unclean_Sentence': skill_sentences}\n",
        "    skillInfo = pd.DataFrame(data)\n",
        "\n",
        "    indx = []\n",
        "    for row in range(len(unclean_sentences)):\n",
        "     if labels[row] == 2 or labels[row] == 0:\n",
        "       org_sentences.append(unclean_sentences[row])\n",
        "       indx.append(fulltext.iloc[row][0])\n",
        "\n",
        "    data = {'Init_index': indx,\n",
        "            'unclean_Sentence': org_sentences}\n",
        "    orgInfo = pd.DataFrame(data)\n",
        "\n",
        "    indx = []\n",
        "    for row in range(len(clean_sentences)):\n",
        "     if labels[row] == 1 or labels[row] == 0:\n",
        "       education_sentence.append(clean_sentences[row])\n",
        "       indx.append(fulltext.iloc[row][0])\n",
        "    \n",
        "    data = {'Init_index': indx,\n",
        "            'clean_Sentence': education_sentence}\n",
        "    educationInfo = pd.DataFrame(data)\n",
        "\n",
        "    #init beginning\n",
        "    startLine.append(0)\n",
        "    name.append([])\n",
        "    org.append([])\n",
        "    title.append([])\n",
        "    email.append([])\n",
        "    phone.append([])\n",
        "    skill.append([])\n",
        "    qualification.append([])\n",
        "    schoolName.append([])\n",
        "    schoolDegree.append([])\n",
        "    schoolDate.append([])\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "\n",
        "    #extract org and title\n",
        "    print(len(orgInfo['unclean_Sentence']))\n",
        "    for row in range(len(orgInfo['unclean_Sentence'])):\n",
        "        string1 = orgInfo['unclean_Sentence'][row]\n",
        "        if string1 == \"--------------------------------------------------\":\n",
        "            startLine.append(orgInfo.iloc[row][1])\n",
        "            if row+1 != len(orgInfo['unclean_Sentence']):\n",
        "              org.append([])\n",
        "              title.append([])\n",
        "            i += 1\n",
        "\n",
        "        orgtemp = extract_organization_corenlp(str(string1))\n",
        "        titletemp = extract_title_corenlp(str(string1))\n",
        "\n",
        "        if len(orgtemp) != 0:\n",
        "            for oneorg in orgtemp:\n",
        "                org[i].append(oneorg)\n",
        "        if len(titletemp) != 0:\n",
        "            for onetitle in titletemp:\n",
        "                title[i].append(onetitle)\n",
        "    \n",
        "    end1 = timeit.default_timer()\n",
        "    print('extract org and title Time:',end1 - start)\n",
        "\n",
        "    #extract personal information\n",
        "    i = 0\n",
        "    startLine = []\n",
        "    startLine.append(0)\n",
        "    firstLine = personalInfo['unclean_Sentence'][0]\n",
        "    perinfo_text = ''\n",
        "    for row in range(len(personalInfo['unclean_Sentence'])):\n",
        "        string1 = personalInfo['unclean_Sentence'][row]\n",
        "        if string1 == \"--------------------------------------------------\":\n",
        "            nametemp = extract_name_new(str(perinfo_text))\n",
        "            perinfo_text = ''\n",
        "            if nametemp and len(nametemp.split(' ')) <= 3:\n",
        "              name[i].append(nametemp)\n",
        "            if len(name[i]) < 1:\n",
        "              name[i].append(firstLine)\n",
        "            startLine.append(personalInfo.iloc[row][1])\n",
        "            if row+1 != len(personalInfo['unclean_Sentence']):\n",
        "              firstLine = personalInfo['unclean_Sentence'][row + 1]\n",
        "              name.append([])\n",
        "              email.append([])\n",
        "              phone.append([])\n",
        "            i += 1\n",
        "        else:\n",
        "            perinfo_text += ' ' + string1\n",
        "        \n",
        "        if len(email[i]) < 1:\n",
        "          emailtemp = extract_email(str(string1))\n",
        "          if emailtemp:\n",
        "            email[i].append(emailtemp)\n",
        "        \n",
        "        if len(phone[i]) < 1:\n",
        "          phonetemp = extract_mobile_number(str(string1))\n",
        "          if phonetemp:\n",
        "            phone[i].append(phonetemp)\n",
        "    \n",
        "    end2 = timeit.default_timer()\n",
        "    print('extract personal information Time:',end2 - end1)\n",
        "\n",
        "    #extract skill\n",
        "    i = 0\n",
        "    startLine = []\n",
        "    startLine.append(0)\n",
        "\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    skill_text = ''\n",
        "\n",
        "    for row in range(len(skillInfo['unclean_Sentence'])):\n",
        "        string1 = skillInfo['unclean_Sentence'][row]\n",
        "        \n",
        "        if string1 == \"--------------------------------------------------\":\n",
        "            tktxt = nlp(skill_text)\n",
        "            skills_file = None\n",
        "            noun_chunks = list(tktxt.noun_chunks)\n",
        "\n",
        "            skilltemp = extract_skills(tktxt, noun_chunks, skills_file)\n",
        "\n",
        "            if len(skilltemp) != 0:\n",
        "                for oneskill in skilltemp:\n",
        "                    skill[i].append(oneskill)\n",
        "            \n",
        "            skill_text = ''\n",
        "\n",
        "            startLine.append(skillInfo.iloc[row][1])\n",
        "            if row+1 != len(skillInfo['unclean_Sentence']):\n",
        "              skill.append([])\n",
        "            i += 1\n",
        "        else:\n",
        "            skill_text += ' ' + string1\n",
        "    \n",
        "    end3 = timeit.default_timer()\n",
        "    print('extract skill Time:',end3 - end2)\n",
        "\n",
        "    #extract education information\n",
        "    i = 0\n",
        "    startLine = []\n",
        "    startLine.append(0)\n",
        "\n",
        "    for row in range(len(educationInfo['clean_Sentence'])):\n",
        "      strings = educationInfo['clean_Sentence'][row].split(' ')\n",
        "      #  continue\n",
        "      if educationInfo['clean_Sentence'][row].split().count(\" \") > 9 :\n",
        "        continue\n",
        "      # Parse sentence into subsentences\n",
        "      subsentence = []\n",
        "      subsentence.append([])\n",
        "      j = 0\n",
        "      for word in strings:\n",
        "        if word == '':\n",
        "          subsentence.append([])\n",
        "          j+=1\n",
        "        else:\n",
        "          subsentence[j].append(word)\n",
        "      for sentence in subsentence:\n",
        "        # Skip empty cells\n",
        "        if not sentence:\n",
        "          continue\n",
        "        if sentence[0] == \"--------------------------------------------------\":\n",
        "          startLine.append(educationInfo[educationInfo.columns[0]][row])\n",
        "          if row+1 != len(educationInfo['clean_Sentence']):\n",
        "            qualification.append([])\n",
        "            schoolName.append([])\n",
        "            schoolDegree.append([])\n",
        "            schoolDate.append([])\n",
        "          i+=1\n",
        "        lower_sentence = [x.lower() for x in sentence]\n",
        "        if len(lower_sentence) > 9:\n",
        "          continue\n",
        "        if isQualification(lower_sentence):\n",
        "          qualification[i].append(' '.join(lower_sentence))\n",
        "        elif isSchoolName(lower_sentence):\n",
        "          schoolName[i].append(' '.join(lower_sentence))\n",
        "        elif isSchoolDegree(lower_sentence):\n",
        "          schoolDegree[i].append(' '.join(lower_sentence))\n",
        "        elif isSchoolDate(lower_sentence):\n",
        "          schoolDate[i].append(' '.join(lower_sentence))\n",
        "    \n",
        "    end4 = timeit.default_timer()\n",
        "    print('extract education information Time:',end4 - end3)\n",
        "\n",
        "    # Make data into directionary\n",
        "    data = {'Init_index': startLine,\n",
        "            'Name': name,\n",
        "            'organization': org,\n",
        "            'position': title,\n",
        "            'email': email,\n",
        "            'phone': phone,\n",
        "            'skill': skill,\n",
        "            'SchoolName':schoolName,\n",
        "            'SchoolDegree':schoolDegree,\n",
        "            'SchoolDate':schoolDate,\n",
        "            'Qualifications':qualification}\n",
        "\n",
        "    # Convert data into dataFrame\n",
        "    noncontextualinfo = pd.DataFrame(data)\n",
        "    noncontextualinfo = noncontextualinfo[:-1]\n",
        "\n",
        "    # Put dataFrame into csv\n",
        "    noncontextualinfo.to_csv(\"/content/Noncontutalinfo.csv\", index=True)\n",
        "\n",
        "    return noncontextualinfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjTsWzYiu1I9"
      },
      "source": [
        "# Note: might want to save nlp vectors\n",
        "# Resume scanner (in Beta)\n",
        "# Input: the Directory with resume of interest (The directory must only contain pdf files or JPG files)\n",
        "# Output: the csv file with labels marked for each resumes\n",
        "def ResumeScanner(directory):\n",
        "  processFileFormat(directory)\n",
        "\n",
        "  print('Converting resumes into text...')\n",
        "  # Convert all resumes in directory to text fileR\n",
        "  outputtext, outputtext_org = preprocess_resume(directory)\n",
        "  print('Finished Converting')\n",
        "\n",
        "  print('Picking out headers...')\n",
        "  # Find the headers and output into csv\n",
        "  #HC_model_path = None\n",
        "  HC_model_path = '/content/drive/MyDrive/Resume Scanner Folder/HC_model.h5'\n",
        "  HC_outputfile = get_headers(outputtext_org, HC_model_path)\n",
        "  print('Finished Picking out headers')\n",
        "\n",
        "  print('Picking out labels...')\n",
        "  # Label each headers and output into csv\n",
        "  #LC_model_path = None\n",
        "  LC_model_path = '/content/drive/MyDrive/Resume Scanner Folder/LC_model.h5'\n",
        "  LC_outputfile = get_label(HC_outputfile,LC_model_path)\n",
        "  print('Finished Picking out labels')\n",
        "\n",
        "  print('Combining text data...')\n",
        "  combine_text_df = combine_text(outputtext, outputtext_org)\n",
        "  print('Finished Combining text data')\n",
        "\n",
        "  print('Lablizing text data...')\n",
        "  labeled_text_df = labelizingfulltext(LC_outputfile,combine_text_df)\n",
        "  print('Finished Lablizing text data...')\n",
        "\n",
        "  print('Extracting information...')\n",
        "  info_df = extract_non_context(labeled_text_df)\n",
        "  info = pandas.read_csv('/content/Noncontutalinfo.csv')\n",
        "  sendCSVFile(info, directory)\n",
        "  print('Finished')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}